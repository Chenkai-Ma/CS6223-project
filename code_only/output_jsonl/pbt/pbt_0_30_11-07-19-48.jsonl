{"function_name": "numpy.resize", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import resize\n\n@given(st.lists(st.integers(), min_size=1).map(np.array), \n       st.lists(st.integers(min_value=0, max_value=100), min_size=1))\ndef test_output_shape_property(arr, new_shape):\n    result = resize(arr, new_shape)\n    assert result.shape == tuple(new_shape)\n# End program\n\n@given(st.lists(st.integers(), min_size=1).map(np.array))\ndef test_empty_input_property(arr):\n    result = resize(arr, (0,))\n    assert result.size == 0\n    assert result.shape == (0,)\n# End program\n\n@given(st.lists(st.integers(), min_size=1).map(np.array), \n       st.lists(st.integers(max_value=-1), min_size=1))\ndef test_negative_dimension_property(arr, negative_shape):\n    with pytest.raises(ValueError):\n        resize(arr, negative_shape)\n# End program\n\n@given(st.lists(st.integers(), min_size=1).map(np.array), \n       st.lists(st.integers(), min_size=1).map(np.array))\ndef test_repeated_elements_property(arr, new_shape):\n    result = resize(arr, new_shape)\n    expected_size = np.prod(new_shape)\n    assert len(result) == expected_size\n# End program\n\n@given(st.lists(st.integers(), min_size=1).map(np.array), \n       st.lists(st.integers(min_value=1, max_value=100), min_size=1))\ndef test_total_elements_property(arr, new_shape):\n    result = resize(arr, new_shape)\n    assert result.size == np.prod(new_shape)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import resize\n\n@given(st.lists(st.integers(min_value=0), min_size=1).map(tuple), st.integers(min_value=0))\ndef test_output_shape_property(new_shape, a_length):\n    a = np.arange(a_length)\n    output = resize(a, new_shape)\n    assert output.shape == new_shape\n\n@given(st.integers(min_value=0))\ndef test_empty_input_property(a_length):\n    a = np.array([])\n    new_shape = (a_length,)\n    output = resize(a, new_shape)\n    assert output.shape == new_shape\n    assert output.size == 0\n\n@given(st.lists(st.integers(min_value=-1, max_value=10), min_size=1).map(tuple))\ndef test_negative_dimension_property(new_shape):\n    a = np.arange(10)\n    try:\n        resize(a, new_shape)\n        assert all(dim >= 0 for dim in new_shape)  # Should not reach here\n    except ValueError:\n        pass  # Expected behavior\n\n@given(st.lists(st.integers(), min_size=1).map(tuple), st.integers(min_value=0))\ndef test_elements_repetition_property(new_shape, a_length):\n    a = np.arange(a_length)\n    output = resize(a, new_shape)\n    expected_size = np.prod(new_shape)\n    assert output.size == expected_size\n    assert np.all(output[:a.size] == a)  # Check repeated elements match the original array\n\n@given(st.lists(st.integers(min_value=1, max_value=10), min_size=1).map(tuple))\ndef test_total_elements_property(new_shape):\n    a = np.arange(np.prod(new_shape))\n    output = resize(a, new_shape)\n    assert output.size == np.prod(new_shape)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nimport sys\n\n@given(st.lists(st.integers(), min_size=1), st.lists(st.integers(min_value=0), max_size=5))\ndef test_output_shape_property(input_list, new_shape):\n    a = np.array(input_list)\n    resized = np.resize(a, new_shape)\n    assert resized.shape == tuple(new_shape)\n# End program\n\n@given(st.lists(st.integers()))\ndef test_empty_input_property(input_list):\n    a = np.array(input_list)\n    resized = np.resize(a, (0,))\n    assert resized.size == 0\n    assert resized.shape == (0,)\n# End program\n\n@given(st.lists(st.integers()), st.lists(st.integers(min_value=-1, max_value=5), min_size=1))\ndef test_negative_dimension_property(input_list, new_shape):\n    a = np.array(input_list)\n    try:\n        np.resize(a, new_shape)\n        assert all(dim >= 0 for dim in new_shape)\n    except ValueError:\n        assert any(dim < 0 for dim in new_shape)\n# End program\n\n@given(st.lists(st.integers()), st.lists(st.integers(min_value=1, max_value=5)))\ndef test_element_repetition_property(input_list, new_shape):\n    a = np.array(input_list)\n    resized = np.resize(a, new_shape)\n    expected_size = np.prod(new_shape)\n    original_size = a.size\n    repeats = -(-expected_size // original_size) if original_size > 0 else 0\n    expected_elements = np.concatenate((a,) * repeats)[:expected_size]\n    assert np.array_equal(resized, expected_elements)\n# End program\n\n@given(st.lists(st.integers()), st.lists(st.integers(min_value=1, max_value=sys.maxsize), min_size=1))\ndef test_total_elements_property(input_list, new_shape):\n    a = np.array(input_list)\n    resized = np.resize(a, new_shape)\n    assert resized.size == np.prod(new_shape)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import resize\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=0, max_value=100), \n                                                        st.integers(min_value=0, max_value=100), \n                                                        st.integers(min_value=0, max_value=100)))\ndef test_output_shape_property(input_array, new_shape):\n    result = resize(input_array, new_shape)\n    assert result.shape == new_shape\n# End program\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_empty_input_property(input_array):\n    if len(input_array) == 0:\n        result = resize(input_array, (0,))\n        assert result.shape == (0,)\n# End program\n\n@given(st.lists(st.integers(), min_size=1), st.lists(st.integers(min_value=-1, max_value=100), min_size=1))\ndef test_negative_dimension_property(input_array, new_shape):\n    for dim in new_shape:\n        if dim < 0:\n            try:\n                resize(input_array, new_shape)\n                assert False, \"Expected ValueError for negative dimension\"\n            except ValueError:\n                pass\n# End program\n\n@given(st.lists(st.integers(), min_size=1), st.integers(min_value=1, max_value=100))\ndef test_elements_repeated_property(input_array, new_size):\n    new_shape = (new_size,)\n    result = resize(input_array, new_shape)\n    expected_size = len(input_array) * (new_size // len(input_array) + (new_size % len(input_array) > 0))\n    assert len(result) == expected_size\n\n@given(st.lists(st.integers(), min_size=1), st.lists(st.integers(min_value=0, max_value=100), min_size=1))\ndef test_total_elements_property(input_array, new_shape):\n    result = resize(input_array, new_shape)\n    expected_size = np.prod(new_shape)\n    assert len(result) == expected_size\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import resize\n\n@given(st.lists(st.integers(), min_size=1, max_size=10), st.lists(st.integers(min_value=0), max_size=3))\ndef test_output_shape_property(original_array, new_shape):\n    resized_array = resize(original_array, new_shape)\n    assert resized_array.shape == tuple(new_shape)\n\n@given(st.lists(st.integers(), min_size=1, max_size=10))\ndef test_empty_input_property(original_array):\n    empty_array = []\n    resized_empty_array = resize(empty_array, (3, 4))\n    assert resized_empty_array.size == 0\n\n@given(st.lists(st.integers(), min_size=1, max_size=10), st.lists(st.integers(min_value=-1, max_value=10), max_size=3))\ndef test_negative_dimension_property(original_array, new_shape):\n    if any(dim < 0 for dim in new_shape):\n        try:\n            resize(original_array, new_shape)\n            assert False, \"Expected ValueError for negative dimension\"\n        except ValueError:\n            pass\n\n@given(st.lists(st.integers(), min_size=1, max_size=10), st.lists(st.integers(min_value=1, max_value=10), max_size=3))\ndef test_elements_filled_property(original_array, new_shape):\n    resized_array = resize(original_array, new_shape)\n    total_size = np.prod(new_shape)\n    repeated_array = (original_array * (total_size // len(original_array) + 1))[:total_size]\n    assert np.array_equal(resized_array, repeated_array)\n\n@given(st.lists(st.integers(), min_size=1, max_size=10), st.lists(st.integers(min_value=1, max_value=10), max_size=3))\ndef test_output_size_property(original_array, new_shape):\n    resized_array = resize(original_array, new_shape)\n    assert resized_array.size == np.prod(new_shape)\n# End program\n```"], "properties": ["1. The output shape must match the specified `new_shape` argument, ensuring that the dimensions of the resized array are as intended.\n\n2. If the input array is empty, the output must also be an empty array with the specified `new_shape`, as resizing an empty array should still yield an empty result.\n\n3. If `new_shape` contains negative dimensions, a `ValueError` must be raised, ensuring that all dimensions are non-negative.\n\n4. The elements of the output array must be filled with the original elements of the input array, repeated as necessary to fit the new size, ensuring that the resizing behavior is consistent with the input data.\n\n5. The total number of elements in the output array must equal the product of the dimensions in `new_shape`, confirming that the output size corresponds accurately to the requested shape."], "api_doc": "numpy.resize\nnumpy.resize(a, new_shape)[source]\nReturn a new array with the specified shape.\n\nIf the new array is larger than the original array, then the new array is filled with repeated copies of a. Note that this behavior is different from a.resize(new_shape) which fills with zeros instead of repeated copies of a.\n\nParameters\n:\na\narray_like\nArray to be resized.\n\nnew_shape\nint or tuple of int\nShape of resized array.\n\nReturns\n:\nreshaped_array\nndarray\nThe new array is formed from the data in the old array, repeated if necessary to fill out the required number of elements. The data are repeated iterating over the array in C-order.\n\nSee also\n\nnumpy.reshape\nReshape an array without changing the total size.\n\nnumpy.pad\nEnlarge and pad an array.\n\nnumpy.repeat\nRepeat elements of an array.\n\nndarray.resize\nresize an array in-place.\n\nNotes\n\nWhen the total size of the array does not change reshape should be used. In most other cases either indexing (to reduce the size) or padding (to increase the size) may be a more appropriate solution.\n\nWarning: This functionality does not consider axes separately, i.e. it does not apply interpolation/extrapolation. It fills the return array with the required number of elements, iterating over a in C-order, disregarding axes (and cycling back from the start if the new shape is larger). This functionality is therefore not suitable to resize images, or data where each axis represents a separate and distinct entity.\n\nExamples\n\nimport numpy as np\na = np.array([[0,1],[2,3]])\nnp.resize(a,(2,3))\narray([[0, 1, 2],\n       [3, 0, 1]])\nnp.resize(a,(1,4))\narray([[0, 1, 2, 3]])\nnp.resize(a,(2,4))\narray([[0, 1, 2, 3],\n       [0, 1, 2, 3]])", "api_code": "@array_function_dispatch(_resize_dispatcher)\ndef resize(a, new_shape):\n    if isinstance(new_shape, (int, nt.integer)):\n        new_shape = (new_shape,)\n\n    a = ravel(a)\n\n    new_size = 1\n    for dim_length in new_shape:\n        new_size *= dim_length\n        if dim_length < 0:\n            raise ValueError(\n                'all elements of `new_shape` must be non-negative'\n            )\n\n    if a.size == 0 or new_size == 0:\n        # First case must zero fill. The second would have repeats == 0.\n        return np.zeros_like(a, shape=new_shape)\n\n    repeats = -(-new_size // a.size)  # ceil division\n    a = concatenate((a,) * repeats)[:new_size]\n\n    return reshape(a, new_shape)"}
{"function_name": "networkx.algorithms.clique.find_cliques", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_nodes_are_in_input_graph_property(data):\n    G = data.draw(st.one_of(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=0, max_size=100)),\n                             st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=0, max_size=100))))\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    for clique in cliques:\n        for node in clique:\n            assert node in G.nodes\n\n@given(st.data())\ndef test_output_cliques_are_complete_subgraphs_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=0, max_size=100)))\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    for clique in cliques:\n        for i in range(len(clique)):\n            for j in range(i + 1, len(clique)):\n                assert G.has_edge(clique[i], clique[j])\n\n@given(st.data())\ndef test_output_contains_all_maximal_cliques_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=0, max_size=100)))\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    # This is a placeholder for the logic to check for maximality.\n    # A full implementation would require additional logic to verify maximality.\n    assert True  # Replace with actual maximality checks.\n\n@given(st.data())\ndef test_empty_graph_yields_no_cliques_property(data):\n    G = nx.Graph()  # Empty graph\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    assert cliques == []\n\n@given(st.data())\ndef test_given_nodes_form_clique_or_raise_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=0, max_size=100)))\n    nodes = data.draw(st.lists(st.integers(), min_size=1, max_size=5))\n    if all(G.has_edge(u, v) for u in nodes for v in nodes if u != v):\n        cliques = list(nx.algorithms.clique.find_cliques(G, nodes=nodes))\n        assert any(set(nodes).issubset(set(clique)) for clique in cliques)\n    else:\n        try:\n            nx.algorithms.clique.find_cliques(G, nodes=nodes)\n            assert False  # Should not reach here; an exception should be raised\n        except ValueError:\n            pass  # Expected behavior\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.clique import find_cliques\n\n@given(st.data())\ndef test_output_nodes_are_subsets_property(data):\n    graph = data.draw(st.builds(nx.Graph, nodes=st.sets(st.integers(), min_size=1, max_size=100)))\n    cliques = list(find_cliques(graph))\n    for clique in cliques:\n        assert set(clique).issubset(graph.nodes)\n\n@given(st.data())\ndef test_cliques_are_complete_subgraphs_property(data):\n    graph = data.draw(st.builds(nx.Graph, nodes=st.sets(st.integers(), min_size=1, max_size=100)))\n    cliques = list(find_cliques(graph))\n    for clique in cliques:\n        for i in range(len(clique)):\n            for j in range(i + 1, len(clique)):\n                assert graph.has_edge(clique[i], clique[j])\n\n@given(st.data())\ndef test_cliques_are_maximal_property(data):\n    graph = data.draw(st.builds(nx.Graph, nodes=st.sets(st.integers(), min_size=1, max_size=100)))\n    cliques = list(find_cliques(graph))\n    for clique in cliques:\n        # Check that no nodes can be added to the clique without breaking its completeness\n        for node in graph.nodes:\n            if node not in clique and graph.has_edges_from((node, n) for n in clique):\n                assert False  # A node can be added to the clique\n\n@given(st.data())\ndef test_empty_graph_yields_no_cliques_property(data):\n    graph = nx.Graph()\n    cliques = list(find_cliques(graph))\n    assert cliques == []\n\n@given(st.data())\ndef test_specific_nodes_must_form_clique_property(data):\n    nodes = data.draw(st.sets(st.integers(), min_size=2, max_size=10))\n    graph = nx.complete_graph(nodes)  # Create a complete graph\n    cliques = list(find_cliques(graph, nodes=list(nodes)))\n    assert len(cliques) > 0  # There must be at least one clique, the nodes themselves\n    for clique in cliques:\n        assert set(clique) == set(nodes)  # Must yield the specified clique\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.clique import find_cliques\n\n@given(st.data())\ndef test_output_nodes_are_in_input_graph_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=20), \n                             p=st.floats(min_value=0, max_value=1)))\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        assert all(node in G for node in clique)\n\n@given(st.data())\ndef test_cliques_are_complete_subgraphs_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=20), \n                             p=st.floats(min_value=0, max_value=1)))\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        for i in range(len(clique)):\n            for j in range(i + 1, len(clique)):\n                assert G.has_edge(clique[i], clique[j])\n\n@given(st.data())\ndef test_all_maximal_cliques_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=20), \n                             p=st.floats(min_value=0, max_value=1)))\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        for node in G.nodes():\n            if node not in clique and all(G.has_edge(node, other) for other in clique):\n                assert False  # A node that can extend the clique should not exist\n\n@given(st.data())\ndef test_empty_graph_yields_no_cliques_property(data):\n    G = nx.Graph()\n    cliques = list(find_cliques(G))\n    assert cliques == []\n\n@given(st.data())\ndef test_given_nodes_form_clique_or_raise_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=20), \n                             p=st.floats(min_value=0, max_value=1)))\n    nodes = data.draw(st.lists(st.sampled_from(G.nodes()), min_size=1, max_size=5))\n    if not all(G.has_edge(nodes[i], nodes[j]) for i in range(len(nodes)) for j in range(i + 1, len(nodes))):\n        with pytest.raises(ValueError):\n            list(find_cliques(G, nodes))\n    else:\n        cliques = list(find_cliques(G, nodes))\n        assert any(set(nodes).issubset(set(clique)) for clique in cliques)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_nodes_are_in_input_graph_property(data):\n    # Generate a graph and nodes for testing\n    G = nx.generators.random_graphs.gnp_random_graph(10, 0.5)\n    nodes = list(G.nodes())\n    \n    # Call the find_cliques function\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    \n    # Check that all nodes in the cliques are in the original graph\n    for clique in cliques:\n        assert all(node in G.nodes() for node in clique)\n\n@given(st.data())\ndef test_cliques_are_complete_subgraphs_property(data):\n    # Generate a graph for testing\n    G = nx.generators.random_graphs.gnp_random_graph(10, 0.5)\n    \n    # Call the find_cliques function\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    \n    # Check that each clique is a complete subgraph\n    for clique in cliques:\n        for i in range(len(clique)):\n            for j in range(i + 1, len(clique)):\n                assert G.has_edge(clique[i], clique[j])\n\n@given(st.data())\ndef test_yields_all_maximal_cliques_property(data):\n    # Generate a graph for testing\n    G = nx.generators.random_graphs.gnp_random_graph(10, 0.5)\n    \n    # Call the find_cliques function\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    \n    # Check that no additional nodes can be added to the cliques\n    for clique in cliques:\n        for node in G.nodes():\n            if node not in clique:\n                assert not all(G.has_edge(node, other) for other in clique)\n\n@given(st.data())\ndef test_empty_graph_yields_no_cliques_property(data):\n    # Create an empty graph\n    G = nx.Graph()\n    \n    # Call the find_cliques function\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    \n    # Check that no cliques are yielded\n    assert cliques == []\n\n@given(st.data())\ndef test_specific_nodes_form_clique_or_raise_property(data):\n    # Generate a random graph and select nodes\n    G = nx.generators.random_graphs.gnp_random_graph(10, 0.5)\n    nodes = list(G.nodes())\n    \n    # Test with a subset of nodes\n    if len(nodes) > 1:\n        sub_nodes = nodes[:2]\n        if all(G.has_edge(sub_nodes[i], sub_nodes[j]) for i in range(len(sub_nodes)) for j in range(i + 1, len(sub_nodes))):\n            # Should not raise an error\n            try:\n                cliques = list(nx.algorithms.clique.find_cliques(G, nodes=sub_nodes))\n                assert any(set(sub_nodes) == set(clique) for clique in cliques)\n            except ValueError:\n                assert False  # Should not raise\n        else:\n            # Should raise an error\n            with pytest.raises(ValueError):\n                list(nx.algorithms.clique.find_cliques(G, nodes=sub_nodes))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_nodes_subset_property(data):\n    @st.composite\n    def graphs(draw):\n        nodes = draw(st.sets(st.integers(), min_size=1, max_size=100))\n        edges = draw(st.sets(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes)), min_size=0, max_size=1000))\n        G = nx.Graph()\n        G.add_nodes_from(nodes)\n        G.add_edges_from(edges)\n        return G, nodes\n\n    G, nodes = graphs(data)\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    \n    for clique in cliques:\n        assert all(node in G.nodes for node in clique)\n\n@given(st.data())\ndef test_cliques_complete_subgraph_property(data):\n    @st.composite\n    def graphs(draw):\n        nodes = draw(st.sets(st.integers(), min_size=1, max_size=20))\n        edges = draw(st.sets(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes)), min_size=0, max_size=100))\n        G = nx.Graph()\n        G.add_nodes_from(nodes)\n        G.add_edges_from(edges)\n        return G\n\n    G = graphs(data)\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n\n    for clique in cliques:\n        for i in range(len(clique)):\n            for j in range(i + 1, len(clique)):\n                assert G.has_edge(clique[i], clique[j])\n\n@given(st.data())\ndef test_maximal_cliques_property(data):\n    @st.composite\n    def graphs(draw):\n        nodes = draw(st.sets(st.integers(), min_size=1, max_size=20))\n        edges = draw(st.sets(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes)), min_size=0, max_size=100))\n        G = nx.Graph()\n        G.add_nodes_from(nodes)\n        G.add_edges_from(edges)\n        return G\n\n    G = graphs(data)\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    \n    for clique in cliques:\n        for node in G.nodes:\n            if node not in clique and all(G.has_edge(node, c) for c in clique):\n                assert False  # A node can be added to the clique, so it's not maximal\n\n@given(st.data())\ndef test_empty_graph_yields_no_cliques_property(data):\n    G = nx.Graph()  # Explicitly create an empty graph\n    cliques = list(nx.algorithms.clique.find_cliques(G))\n    assert cliques == []  # Expecting empty output\n\n@given(st.data())\ndef test_specific_nodes_must_form_clique_property(data):\n    @st.composite\n    def graphs(draw):\n        nodes = draw(st.sets(st.integers(), min_size=3, max_size=10))\n        edges = draw(st.sets(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes)), min_size=0, max_size=50))\n        G = nx.Graph()\n        G.add_nodes_from(nodes)\n        G.add_edges_from(edges)\n        return G, nodes\n\n    G, nodes = graphs(data)\n    if all(G.has_edge(n1, n2) for n1 in nodes for n2 in nodes if n1 != n2):\n        cliques = list(nx.algorithms.clique.find_cliques(G))\n        assert any(set(nodes).issubset(set(clique)) for clique in cliques)\n    else:\n        try:\n            list(nx.algorithms.clique.find_cliques(G, nodes=nodes))\n            assert False  # Should raise ValueError\n        except ValueError:\n            pass  # Expected behavior\n# End program\n```"], "properties": ["1. The output cliques must be subsets of the input graph's nodes, ensuring that all nodes in each returned clique are present in the original graph.\n\n2. Each returned clique must be a complete subgraph, meaning that every pair of nodes within the clique must be directly connected by an edge in the graph.\n\n3. The function should yield all maximal cliques, meaning no additional nodes can be added to any returned clique without breaking its completeness.\n\n4. If the input graph is empty, the function should yield no cliques, returning an empty output.\n\n5. If specific nodes are provided as input, the function should only yield cliques that include those nodes, and it should raise an error if the provided nodes do not form a clique."], "api_doc": "find_cliques\nfind_cliques(G, nodes=None)[source]\nReturns all maximal cliques in an undirected graph.\n\nFor each node n, a maximal clique for n is a largest complete subgraph containing n. The largest maximal clique is sometimes called the maximum clique.\n\nThis function returns an iterator over cliques, each of which is a list of nodes. It is an iterative implementation, so should not suffer from recursion depth issues.\n\nThis function accepts a list of nodes and only the maximal cliques containing all of these nodes are returned. It can considerably speed up the running time if some specific cliques are desired.\n\nParameters\n:\nG\nNetworkX graph\nAn undirected graph.\n\nnodes\nlist, optional (default=None)\nIf provided, only yield maximal cliques containing all nodes in nodes. If nodes isn\u2019t a clique itself, a ValueError is raised.\n\nReturns\n:\niterator\nAn iterator over maximal cliques, each of which is a list of nodes in G. If nodes is provided, only the maximal cliques containing all the nodes in nodes are returned. The order of cliques is arbitrary.\n\nRaises\n:\nValueError\nIf nodes is not a clique.\n\nSee also\n\nfind_cliques_recursive\nA recursive version of the same algorithm.\n\nNotes\n\nTo obtain a list of all maximal cliques, use list(find_cliques(G)). However, be aware that in the worst-case, the length of this list can be exponential in the number of nodes in the graph. This function avoids storing all cliques in memory by only keeping current candidate node lists in memory during its search.\n\nThis implementation is based on the algorithm published by Bron and Kerbosch (1973) [1], as adapted by Tomita, Tanaka and Takahashi (2006) [2] and discussed in Cazals and Karande (2008) [3]. It essentially unrolls the recursion used in the references to avoid issues of recursion stack depth (for a recursive implementation, see find_cliques_recursive()).\n\nThis algorithm ignores self-loops and parallel edges, since cliques are not conventionally defined with such edges.\n\nReferences\n\n[1]\nBron, C. and Kerbosch, J. \u201cAlgorithm 457: finding all cliques of an undirected graph\u201d. Communications of the ACM 16, 9 (Sep. 1973), 575\u2013577. <http://portal.acm.org/citation.cfm?doid=362342.362367>\n\n[2]\nEtsuji Tomita, Akira Tanaka, Haruhisa Takahashi, \u201cThe worst-case time complexity for generating all maximal cliques and computational experiments\u201d, Theoretical Computer Science, Volume 363, Issue 1, Computing and Combinatorics, 10th Annual International Conference on Computing and Combinatorics (COCOON 2004), 25 October 2006, Pages 28\u201342 <https://doi.org/10.1016/j.tcs.2006.06.015>\n\n[3]\nF. Cazals, C. Karande, \u201cA note on the problem of reporting maximal cliques\u201d, Theoretical Computer Science, Volume 407, Issues 1\u20133, 6 November 2008, Pages 564\u2013568, <https://doi.org/10.1016/j.tcs.2008.05.010>\n\nExamples\n\nfrom pprint import pprint  # For nice dict formatting\nG = nx.karate_club_graph()\nsum(1 for c in nx.find_cliques(G))  # The number of maximal cliques in G\n36\nmax(nx.find_cliques(G), key=len)  # The largest maximal clique in G\n[0, 1, 2, 3, 13]\nThe size of the largest maximal clique is known as the clique number of the graph, which can be found directly with:\n\nmax(len(c) for c in nx.find_cliques(G))\n5\nOne can also compute the number of maximal cliques in G that contain a given node. The following produces a dictionary keyed by node whose values are the number of maximal cliques in G that contain the node:\n\npprint({n: sum(1 for c in nx.find_cliques(G) if n in c) for n in G})\n{0: 13,\n 1: 6,\n 2: 7,\n 3: 3,\n 4: 2,\n 5: 3,\n 6: 3,\n 7: 1,\n 8: 3,\n 9: 2,\n 10: 2,\n 11: 1,\n 12: 1,\n 13: 2,\n 14: 1,\n 15: 1,\n 16: 1,\n 17: 1,\n 18: 1,\n 19: 2,\n 20: 1,\n 21: 1,\n 22: 1,\n 23: 3,\n 24: 2,\n 25: 2,\n 26: 1,\n 27: 3,\n 28: 2,\n 29: 2,\n 30: 2,\n 31: 4,\n 32: 9,\n 33: 14}\nOr, similarly, the maximal cliques in G that contain a given node. For example, the 4 maximal cliques that contain node 31:\n\n[c for c in nx.find_cliques(G) if 31 in c]\n[[0, 31], [33, 32, 31], [33, 28, 31], [24, 25, 31]]", "api_code": "@not_implemented_for(\"directed\")\n@nx._dispatchable\ndef find_cliques(G, nodes=None):\n    if len(G) == 0:\n        return\n\n    adj = {u: {v for v in G[u] if v != u} for u in G}\n\n    # Initialize Q with the given nodes and subg, cand with their nbrs\n    Q = nodes[:] if nodes is not None else []\n    cand = set(G)\n    for node in Q:\n        if node not in cand:\n            raise ValueError(f\"The given `nodes` {nodes} do not form a clique\")\n        cand &= adj[node]\n\n    if not cand:\n        yield Q[:]\n        return\n\n    subg = cand.copy()\n    stack = []\n    Q.append(None)\n\n    u = max(subg, key=lambda u: len(cand & adj[u]))\n    ext_u = cand - adj[u]\n\n    try:\n        while True:\n            if ext_u:\n                q = ext_u.pop()\n                cand.remove(q)\n                Q[-1] = q\n                adj_q = adj[q]\n                subg_q = subg & adj_q\n                if not subg_q:\n                    yield Q[:]\n                else:\n                    cand_q = cand & adj_q\n                    if cand_q:\n                        stack.append((subg, cand, ext_u))\n                        Q.append(None)\n                        subg = subg_q\n                        cand = cand_q\n                        u = max(subg, key=lambda u: len(cand & adj[u]))\n                        ext_u = cand - adj[u]\n            else:\n                Q.pop()\n                subg, cand, ext_u = stack.pop()\n    except IndexError:\n        pass"}
{"function_name": "statistics.correlation", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2), \n           st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_correlation_coefficient_range_property(x, y):\n    \"\"\"The correlation coefficient should always be between -1 and 1.\"\"\"\n    result = statistics.correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_identical_lists_property(x):\n    \"\"\"If both input lists are identical, the correlation should be 1.\"\"\"\n    result = statistics.correlation(x, x)\n    assert result == 1\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_constant_input_property(x):\n    \"\"\"If one of the input lists is constant, it should raise StatisticsError.\"\"\"\n    constant = [1.0] * len(x)\n    try:\n        statistics.correlation(x, constant)\n    except statistics.StatisticsError:\n        pass  # Expected behavior\n    else:\n        assert False, \"Expected StatisticsError not raised\"\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2),\n           st.floats())\ndef test_linear_transformation_property(x, c):\n    \"\"\"For two input lists where the second list is a linear transformation of the first, the correlation should remain 1 or -1.\"\"\"\n    transformed = [xi * c for xi in x]\n    result = statistics.correlation(x, transformed)\n    assert result == 1 if c > 0 else -1\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2),\n           st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_permutation_invariance_property(x, y):\n    \"\"\"The correlation should be invariant under permutations of the input lists.\"\"\"\n    from random import shuffle\n    shuffled_x = x[:]\n    shuffled_y = y[:]\n    shuffle(shuffled_x)\n    shuffle(shuffled_y)\n    original_result = statistics.correlation(x, y)\n    shuffled_result = statistics.correlation(shuffled_x, shuffled_y)\n    assert original_result == shuffled_result\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(), min_size=2, max_size=100), st.lists(st.floats(), min_size=2, max_size=100))\ndef test_correlation_between_neg1_and_1_property(x, y):\n    if len(x) == len(y):\n        result = statistics.correlation(x, y)\n        assert -1 <= result <= 1\n\n@given(st.lists(st.floats(), min_size=2, max_size=100))\ndef test_identical_lists_correlation_property(x):\n    result = statistics.correlation(x, x)\n    assert result == 1\n\n@given(st.lists(st.floats(), min_size=2, max_size=100))\ndef test_constant_list_correlation_property(x):\n    constant_value = x[0]\n    x_constant = [constant_value] * len(x)\n    try:\n        statistics.correlation(x_constant, x)\n        assert False  # This should not be reached\n    except statistics.StatisticsError:\n        pass\n\n@given(st.lists(st.floats(), min_size=2, max_size=100), st.floats(), st.floats())\ndef test_linear_transformation_correlation_property(x, a, b):\n    if len(x) > 0:\n        y = [a * xi + b for xi in x]\n        result = statistics.correlation(x, y)\n        assert result == 1 if a > 0 else -1\n\n@given(st.lists(st.floats(), min_size=2, max_size=100))\ndef test_permutation_invariance_property(x):\n    if len(x) > 0:\n        y = x[:]\n        shuffled_y = y.copy()\n        import random\n        random.shuffle(shuffled_y)\n        result_original = statistics.correlation(x, y)\n        result_shuffled = statistics.correlation(x, shuffled_y)\n        assert result_original == result_shuffled\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics  # Assuming the correlation function is in a module named statistics\nfrom statistics import StatisticsError\n\n@given(st.lists(st.floats()), st.lists(st.floats()))\ndef test_correlation_output_range_property(x, y):\n    if len(x) == len(y) and len(x) >= 2:\n        result = statistics.correlation(x, y)\n        assert -1 <= result <= 1\n\n@given(st.lists(st.floats()))\ndef test_correlation_identical_lists_property(x):\n    if len(x) >= 2:\n        result = statistics.correlation(x, x)\n        assert result == 1\n\n@given(st.lists(st.floats(), min_size=2))\ndef test_correlation_constant_input_property(x):\n    constant_list = [x[0]] * len(x)\n    try:\n        statistics.correlation(x, constant_list)\n        assert False  # Should not reach this line\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats()), st.floats(), st.floats())\ndef test_correlation_linear_transformation_property(x, scale, shift):\n    if len(x) >= 2:\n        y = [scale * xi + shift for xi in x]\n        result = statistics.correlation(x, y)\n        assert result == 1 if scale > 0 else -1\n\n@given(st.lists(st.floats()), st.lists(st.floats()))\ndef test_correlation_permutation_invariance_property(x, y):\n    if len(x) == len(y) and len(x) >= 2:\n        result_original = statistics.correlation(x, y)\n        result_permuted = statistics.correlation(sorted(x), sorted(y))\n        assert result_original == result_permuted\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import correlation, StatisticsError\n\n@given(st.lists(st.floats()), st.lists(st.floats()))\ndef test_correlation_outputs_between_minus_one_and_one_property(x, y):\n    if len(x) == len(y) and len(x) >= 2:\n        result = correlation(x, y)\n        assert -1 <= result <= 1\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10), min_size=2))\ndef test_identical_lists_property(x):\n    result = correlation(x, x)\n    assert result == 1\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10), min_size=2))\ndef test_constant_input_property(x):\n    constant_list = [x[0]] * len(x)\n    try:\n        correlation(x, constant_list)\n        assert False, \"Expected StatisticsError for constant input\"\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats()), st.floats(), st.floats())\ndef test_linear_transformation_property(x, a, b):\n    if len(x) >= 2:\n        y = [a * xi + b for xi in x]\n        result = correlation(x, y)\n        assert result == 1 if a > 0 else -1\n\n@given(st.lists(st.floats()), st.lists(st.floats()))\ndef test_permutation_invariance_property(x, y):\n    if len(x) == len(y) and len(x) >= 2:\n        result1 = correlation(x, y)\n        result2 = correlation(y, x)\n        assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom statistics import correlation, StatisticsError\n\n@given(st.lists(st.floats(), min_size=2), st.lists(st.floats(), min_size=2))\ndef test_correlation_output_range_property(x, y):\n    result = correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_identical_lists_property(x):\n    result = correlation(x, x)\n    assert result == 1\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_constant_input_property(x):\n    constant_list = [x[0]] * len(x)\n    with st.raises(StatisticsError):\n        correlation(x, constant_list)\n\n@given(st.lists(st.floats(), min_size=1), st.floats(), st.floats())\ndef test_linear_transformation_property(x, a, b):\n    transformed_y = [a * xi + b for xi in x]\n    result = correlation(x, transformed_y)\n    assert result == 1 if a > 0 else -1\n\n@given(st.lists(st.floats(), min_size=2))\ndef test_permutation_invariance_property(x):\n    y = list(x)  # Copy of x\n    result1 = correlation(x, y)\n    np.random.shuffle(y)  # Shuffle the copy\n    result2 = correlation(x, y)\n    assert result1 == result2\n# End program\n```"], "properties": ["1. The correlation coefficient should always be between -1 and 1, inclusive, regardless of the input values provided.\n\n2. If both input lists are identical, the correlation should be 1, indicating perfect positive correlation.\n\n3. If one of the input lists is constant (all elements are the same), the function should raise a `StatisticsError` indicating that at least one of the inputs is constant.\n\n4. For two input lists where the second list is a linear transformation of the first (e.g., scaled or shifted), the correlation should remain 1 or -1, depending on the direction of the transformation.\n\n5. The correlation should be invariant under permutations of the input lists, meaning that changing the order of elements in either input list should not affect the calculated correlation coefficient."], "api_doc": "statistics.correlation(x, y, /, *, method='linear')\nReturn the Pearson\u2019s correlation coefficient for two inputs. Pearson\u2019s correlation coefficient r takes values between -1 and +1. It measures the strength and direction of a linear relationship.\n\nIf method is \u201cranked\u201d, computes Spearman\u2019s rank correlation coefficient for two inputs. The data is replaced by ranks. Ties are averaged so that equal values receive the same rank. The resulting coefficient measures the strength of a monotonic relationship.\n\nSpearman\u2019s correlation coefficient is appropriate for ordinal data or for continuous data that doesn\u2019t meet the linear proportion requirement for Pearson\u2019s correlation coefficient.\n\nBoth inputs must be of the same length (no less than two), and need not to be constant, otherwise StatisticsError is raised.\n\nExample with Kepler\u2019s laws of planetary motion:\n\n>>>\n# Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and  Neptune\norbital_period = [88, 225, 365, 687, 4331, 10_756, 30_687, 60_190]    # days\ndist_from_sun = [58, 108, 150, 228, 778, 1_400, 2_900, 4_500] # million km\n\n# Show that a perfect monotonic relationship exists\ncorrelation(orbital_period, dist_from_sun, method='ranked')\n1.0\n\n# Observe that a linear relationship is imperfect\nround(correlation(orbital_period, dist_from_sun), 4)\n0.9882\n\n# Demonstrate Kepler's third law: There is a linear correlation\n# between the square of the orbital period and the cube of the\n# distance from the sun.\nperiod_squared = [p * p for p in orbital_period]\ndist_cubed = [d * d * d for d in dist_from_sun]\nround(correlation(period_squared, dist_cubed), 4)\n1.0\nAdded in version 3.10.\n\nChanged in version 3.12: Added support for Spearman\u2019s rank correlation coefficient.", "api_code": "def correlation(x, y, /, *, method='linear'):\n    n = len(x)\n    if len(y) != n:\n        raise StatisticsError('correlation requires that both inputs have same number of data points')\n    if n < 2:\n        raise StatisticsError('correlation requires at least two data points')\n    if method not in {'linear', 'ranked'}:\n        raise ValueError(f'Unknown method: {method!r}')\n    if method == 'ranked':\n        start = (n - 1) / -2            # Center rankings around zero\n        x = _rank(x, start=start)\n        y = _rank(y, start=start)\n    else:\n        xbar = fsum(x) / n\n        ybar = fsum(y) / n\n        x = [xi - xbar for xi in x]\n        y = [yi - ybar for yi in y]\n    sxy = sumprod(x, y)\n    sxx = sumprod(x, x)\n    syy = sumprod(y, y)\n    try:\n        return sxy / _sqrtprod(sxx, syy)\n    except ZeroDivisionError:\n        raise StatisticsError('at least one of the inputs is constant')"}
{"function_name": "networkx.algorithms.approximation.clustering_coefficient.average_clustering", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom hypothesis import settings\n\n# Setting a reasonable limit for performance and to avoid very large inputs\nsettings.max_examples = 100\n\n@given(st.data())\ndef test_average_clustering_non_negative_property(data):\n    G = data.draw(st.builds(nx.gnm_random_graph, st.integers(0, 100), st.integers(0, 500)))\n    result = nx.algorithms.approximation.clustering_coefficient.average_clustering(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_average_clustering_upper_bound_property(data):\n    G = data.draw(st.builds(nx.gnm_random_graph, st.integers(0, 100), st.integers(0, 500)))\n    result = nx.algorithms.approximation.clustering_coefficient.average_clustering(G)\n    assert result <= 1\n\n@given(st.data())\ndef test_average_clustering_empty_graph_property(data):\n    G = nx.Graph()  # Creating an empty graph\n    result = nx.algorithms.approximation.clustering_coefficient.average_clustering(G)\n    assert result == 0\n\n@given(st.data())\ndef test_average_clustering_consistency_property(data):\n    G = data.draw(st.builds(nx.gnm_random_graph, st.integers(0, 100), st.integers(0, 500)))\n    seed = 42  # Fixed seed for consistency\n    result_1 = nx.algorithms.approximation.clustering_coefficient.average_clustering(G, seed=seed)\n    result_2 = nx.algorithms.approximation.clustering_coefficient.average_clustering(G, seed=seed)\n    assert result_1 == result_2\n\n@given(st.data())\ndef test_average_clustering_invariance_property(data):\n    G = data.draw(st.builds(nx.gnm_random_graph, st.integers(0, 100), st.integers(0, 500)))\n    result_1 = nx.algorithms.approximation.clustering_coefficient.average_clustering(G)\n    G_reordered = nx.relabel_nodes(G, {n: n for n in G.nodes()})  # Relabeling nodes does not change the graph\n    result_2 = nx.algorithms.approximation.clustering_coefficient.average_clustering(G_reordered)\n    assert result_1 == result_2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation.clustering_coefficient import average_clustering\n\n@given(st.data())\ndef test_average_clustering_non_negative_property(data):\n    graph = data.draw(st.connected_graphs(min_size=0))\n    result = average_clustering(graph)\n    assert result >= 0\n\n@given(st.data())\ndef test_average_clustering_upper_bound_property(data):\n    graph = data.draw(st.connected_graphs(min_size=1))\n    result = average_clustering(graph)\n    assert result <= 1\n\n@given(st.data())\ndef test_average_clustering_empty_graph_property(data):\n    graph = nx.Graph()  # Create an empty graph\n    result = average_clustering(graph)\n    assert result == 0\n\n@given(st.data())\ndef test_average_clustering_consistency_property(data):\n    graph = data.draw(st.connected_graphs(min_size=1))\n    seed = data.draw(st.integers(min_value=0, max_value=1000))\n    result_1 = average_clustering(graph, seed=seed)\n    result_2 = average_clustering(graph, seed=seed)\n    assert result_1 == result_2\n\n@given(st.data())\ndef test_average_clustering_invariance_property(data):\n    graph = data.draw(st.connected_graphs(min_size=1))\n    result_1 = average_clustering(graph)\n    result_2 = average_clustering(nx.relabel.relabel_nodes(graph, {n: n for n in graph.nodes()}))  # Same graph with relabeled nodes\n    assert result_1 == result_2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import average_clustering\n\n@given(st.data())\ndef test_average_clustering_non_negative_property(data):\n    G = nx.gnp_random_graph(data.draw(st.integers(min_value=0, max_value=1000)),\n                             data.draw(st.floats(min_value=0.0, max_value=1.0)))\n    result = average_clustering(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_average_clustering_bounded_by_one_property(data):\n    G = nx.gnp_random_graph(data.draw(st.integers(min_value=0, max_value=1000)),\n                             data.draw(st.floats(min_value=0.0, max_value=1.0)))\n    result = average_clustering(G)\n    assert result <= 1\n\n@given(st.data())\ndef test_average_clustering_empty_graph_property(data):\n    G = nx.Graph()\n    result = average_clustering(G)\n    assert result == 0\n\n@given(st.data())\ndef test_average_clustering_consistency_property(data):\n    G = nx.gnp_random_graph(data.draw(st.integers(min_value=0, max_value=1000)),\n                             data.draw(st.floats(min_value=0.0, max_value=1.0)))\n    seed = data.draw(st.integers(min_value=0, max_value=10000))\n    result1 = average_clustering(G, seed=seed)\n    result2 = average_clustering(G, seed=seed)\n    assert result1 == result2\n\n@given(st.data())\ndef test_average_clustering_invariance_property(data):\n    G = nx.gnp_random_graph(data.draw(st.integers(min_value=0, max_value=1000)),\n                             data.draw(st.floats(min_value=0.0, max_value=1.0)))\n    result1 = average_clustering(G)\n    result2 = average_clustering(G.reverse())\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import average_clustering\n\n@given(st.data())\ndef test_average_clustering_non_negative_property(data):\n    G = data.draw(stGraphs())\n    result = average_clustering(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_average_clustering_upper_bound_property(data):\n    G = data.draw(stGraphs())\n    result = average_clustering(G)\n    assert result <= 1\n\n@given(st.data())\ndef test_average_clustering_empty_graph_property(data):\n    G = nx.Graph()  # Creating an empty graph\n    result = average_clustering(G)\n    assert result == 0\n\n@given(st.data())\ndef test_average_clustering_consistency_property(data):\n    G = data.draw(stGraphs())\n    seed = data.draw(st.integers())  # Use a random integer seed\n    result1 = average_clustering(G, seed=seed)\n    result2 = average_clustering(G, seed=seed)\n    assert result1 == result2\n\n@given(st.data())\ndef test_average_clustering_invariance_property(data):\n    G = data.draw(stGraphs())\n    result1 = average_clustering(G)\n    result2 = average_clustering(nx.relabel_nodes(G, {n: n for n in G.nodes()}))  # Relabeling should not change the result\n    assert result1 == result2\n# End program\n``` \n\nNote: The testing strategy `stGraphs()` needs to be defined or imported appropriately to generate graphs, which is not shown here. You might want to use `st.graphs()` or a similar strategy depending on your specific implementation context.", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import average_clustering\n\n@given(st.data())\ndef test_average_clustering_non_negative_property(data):\n    G = data.draw(st.graphs())\n    result = average_clustering(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_average_clustering_upper_bound_property(data):\n    G = data.draw(st.graphs())\n    result = average_clustering(G)\n    assert result <= 1\n\n@given(st.data())\ndef test_average_clustering_empty_graph_property(data):\n    G = nx.Graph()  # Create an empty graph\n    result = average_clustering(G)\n    assert result == 0\n\n@given(st.data())\ndef test_average_clustering_consistency_property(data):\n    G = data.draw(st.graphs())\n    seed = data.draw(st.integers(min_value=0, max_value=1000))\n    result1 = average_clustering(G, seed=seed)\n    result2 = average_clustering(G, seed=seed)\n    assert result1 == result2\n\n@given(st.data())\ndef test_average_clustering_invariance_property(data):\n    G = data.draw(st.graphs())\n    result1 = average_clustering(G)\n    G_reordered = nx.relabel_nodes(G, {n: n for n in G.nodes()})  # Label nodes to keep the structure\n    result2 = average_clustering(G_reordered)\n    assert result1 == result2\n# End program\n```"], "properties": ["1. The average clustering coefficient should be a non-negative value, as it represents a proportion of triangles formed among the neighbors of nodes in the graph.\n\n2. The average clustering coefficient should be less than or equal to 1, since the maximum possible clustering coefficient occurs when every pair of neighbors is connected, forming a complete subgraph.\n\n3. For an empty graph (with no nodes), the average clustering coefficient should return 0, as there are no triangles to count.\n\n4. The average clustering coefficient should be consistent across multiple trials with the same graph and seed, producing similar average values due to the random sampling process.\n\n5. The average clustering coefficient should be invariant to the order of nodes in the graph; that is, the result should remain the same regardless of how the nodes are arranged or labeled."], "api_doc": "average_clustering\naverage_clustering(G, trials=1000, seed=None)[source]\nEstimates the average clustering coefficient of G.\n\nThe local clustering of each node in G is the fraction of triangles that actually exist over all possible triangles in its neighborhood. The average clustering coefficient of a graph G is the mean of local clusterings.\n\nThis function finds an approximate average clustering coefficient for G by repeating n times (defined in trials) the following experiment: choose a node at random, choose two of its neighbors at random, and check if they are connected. The approximate coefficient is the fraction of triangles found over the number of trials [1].\n\nParameters\n:\nG\nNetworkX graph\ntrials\ninteger\nNumber of trials to perform (default 1000).\n\nseed\ninteger, random_state, or None (default)\nIndicator of random number generation state. See Randomness.\n\nReturns\n:\nc\nfloat\nApproximated average clustering coefficient.\n\nRaises\n:\nNetworkXNotImplemented\nIf G is directed.\n\nReferences\n\n[1]\nSchank, Thomas, and Dorothea Wagner. Approximating clustering coefficient and transitivity. Universit\u00e4t Karlsruhe, Fakult\u00e4t f\u00fcr Informatik, 2004. https://doi.org/10.5445/IR/1000001239\n\nExamples\n\nfrom networkx.algorithms import approximation\nG = nx.erdos_renyi_graph(10, 0.2, seed=10)\napproximation.average_clustering(G, trials=1000, seed=10)\n0.214", "api_code": "@not_implemented_for(\"directed\")\n@py_random_state(2)\n@nx._dispatchable(name=\"approximate_average_clustering\")\ndef average_clustering(G, trials=1000, seed=None):\n    n = len(G)\n    triangles = 0\n    nodes = list(G)\n    for i in [int(seed.random() * n) for i in range(trials)]:\n        nbrs = list(G[nodes[i]])\n        if len(nbrs) < 2:\n            continue\n        u, v = seed.sample(nbrs, 2)\n        if u in G[v]:\n            triangles += 1\n    return triangles / trials"}
{"function_name": "statistics.pstdev", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom statistics import pstdev, StatisticsError\nimport math\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_statistics_pstdev_non_negative_output(data):\n    result = pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)).filter(lambda x: len(x) == 0))\ndef test_statistics_pstdev_empty_data_raises_error(data):\n    with st.raises(StatisticsError):\n        pstdev(data)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)).filter(lambda x: len(x) == 1))\ndef test_statistics_pstdev_single_data_point_zero(data):\n    result = pstdev(data)\n    assert result == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_statistics_pstdev_order_independence(data):\n    result1 = pstdev(data)\n    result2 = pstdev(data[::-1])\n    assert result1 == result2\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)).filter(lambda x: len(x) > 1))\ndef test_statistics_pstdev_equivalence_to_sqrt_variance(data):\n    variance = sum((x - sum(data) / len(data)) ** 2 for x in data) / len(data)\n    result = pstdev(data)\n    assert math.isclose(result, math.sqrt(variance), rel_tol=1e-9)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import pstdev, StatisticsError\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_statistics_pstdev_non_negative_property(data):\n    result = pstdev(data)\n    assert result >= 0\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_statistics_pstdev_empty_input_property(data):\n    if len(data) == 0:\n        try:\n            pstdev(data)\n            assert False, \"Expected StatisticsError for empty input\"\n        except StatisticsError:\n            pass\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_statistics_pstdev_single_value_property(data):\n    result = pstdev([data[0]])\n    assert result == 0\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_statistics_pstdev_order_invariance_property(data):\n    result1 = pstdev(data)\n    result2 = pstdev(data[::-1])\n    assert result1 == result2\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_statistics_pstdev_variance_relationship_property(data):\n    variance = sum((x - sum(data)/len(data)) ** 2 for x in data) / len(data)\n    result = pstdev(data)\n    assert result ** 2 == variance\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import pstdev, StatisticsError\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_statistics_pstdev_non_negative_property(data):\n    result = pstdev(data)\n    assert result >= 0\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_statistics_pstdev_empty_data_property(data):\n    if not data:\n        with st.raises(StatisticsError):\n            pstdev(data)\n    else:\n        pstdev(data)  # Just to execute the function for non-empty cases\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_statistics_pstdev_single_value_property(data):\n    if len(data) == 1:\n        result = pstdev(data)\n        assert result == 0\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_statistics_pstdev_order_independence_property(data):\n    if len(data) > 1:\n        result1 = pstdev(data)\n        result2 = pstdev(list(reversed(data)))\n        assert result1 == result2\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_statistics_pstdev_variance_relationship_property(data):\n    variance = sum((x - sum(data) / len(data)) ** 2 for x in data) / len(data)\n    result = pstdev(data)\n    assert result**2 == variance\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import pstdev, StatisticsError\nimport math\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_statistics_pstdev_non_negative_property(data):\n    result = pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_statistics_pstdev_empty_input_property(data):\n    if len(data) == 0:\n        try:\n            pstdev(data)\n            assert False, \"Expected StatisticsError for empty input\"\n        except StatisticsError:\n            pass\n\n@given(st.floats())\ndef test_statistics_pstdev_single_value_property(value):\n    result = pstdev([value])\n    assert result == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_statistics_pstdev_order_independence_property(data):\n    result1 = pstdev(data)\n    result2 = pstdev(data[::-1])  # Reverse the list\n    assert result1 == result2\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_statistics_pstdev_variance_relationship_property(data):\n    variance = sum((x - sum(data) / len(data)) ** 2 for x in data) / len(data)\n    result = pstdev(data)\n    assert math.isclose(result, math.sqrt(variance))\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import pstdev, StatisticsError\nimport numpy as np\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_statistics_pstdev_non_negative_property(data):\n    result = pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(), min_size=0))\ndef test_statistics_pstdev_requires_data_property(data):\n    if len(data) == 0:\n        with pytest.raises(StatisticsError):\n            pstdev(data)\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_statistics_pstdev_single_value_property(data):\n    if len(data) == 1:\n        result = pstdev(data)\n        assert result == 0\n\n@given(st.lists(st.floats()))\ndef test_statistics_pstdev_order_invariance_property(data):\n    result1 = pstdev(data)\n    result2 = pstdev(data[::-1])\n    assert result1 == result2\n\n@given(st.lists(st.floats(), min_size=2))\ndef test_statistics_pstdev_variance_relationship_property(data):\n    variance = np.var(data, ddof=0)\n    result = pstdev(data)\n    assert np.isclose(result ** 2, variance)\n\n# End program\n```"], "properties": ["1. The output of `pstdev` should always be non-negative, as standard deviation cannot be negative.\n\n2. If the input data set is empty, `pstdev` should raise a `StatisticsError`, indicating that at least one data point is required.\n\n3. If the input data contains only one data point, the output of `pstdev` should be zero, since there is no variation in a single value.\n\n4. The output of `pstdev` should be consistent with the output of `pstdev` when the input data is the same, regardless of the order of the data points.\n\n5. The output of `pstdev` should be equal to the square root of the variance of the input data, confirming the mathematical relationship between standard deviation and variance."], "api_doc": "statistics.pstdev(data, mu=None)\nReturn the population standard deviation (the square root of the population variance). See pvariance() for arguments and other details.\n\n>>>\npstdev([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])\n0.986893273527251", "api_code": "def pstdev(data, mu=None):\n    T, ss, c, n = _ss(data, mu)\n    if n < 1:\n        raise StatisticsError('pstdev requires at least one data point')\n    mss = ss / n\n    if issubclass(T, Decimal):\n        return _decimal_sqrt_of_frac(mss.numerator, mss.denominator)\n    return _float_sqrt_of_frac(mss.numerator, mss.denominator)"}
{"function_name": "datetime.date.weekday", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_weekday_output_range_property(date):\n    result = date.weekday()\n    assert 0 <= result <= 6\n\n@given(st.dates())\ndef test_weekday_correct_day_of_week_property(date):\n    result = date.weekday()\n    # Mapping: 0 = Monday, 1 = Tuesday, ..., 6 = Sunday\n    expected_day = (date.toordinal() + 6) % 7\n    assert result == expected_day\n\n@given(st.dates())\ndef test_weekday_consistency_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n\n@given(st.dates())\ndef test_weekday_predictable_change_property(date):\n    result_today = date.weekday()\n    tomorrow = date + datetime.timedelta(days=1)\n    result_tomorrow = tomorrow.weekday()\n    assert result_tomorrow == (result_today + 1) % 7\n\n@given(st.dates())\ndef test_weekday_periodicity_property(date):\n    result_today = date.weekday()\n    one_week_later = date + datetime.timedelta(weeks=1)\n    result_one_week_later = one_week_later.weekday()\n    assert result_today == result_one_week_later\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_weekday_output_range_property(date):\n    result = date.weekday()\n    assert 0 <= result <= 6\n\n@given(st.dates())\ndef test_weekday_output_day_mapping_property(date):\n    result = date.weekday()\n    # Mapping of result to days for verification\n    day_mapping = {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thursday\", 4: \"Friday\", 5: \"Saturday\", 6: \"Sunday\"}\n    assert result in day_mapping\n\n@given(st.dates())\ndef test_weekday_consistency_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n\n@given(st.dates(), st.integers())\ndef test_weekday_predictable_output_property(date, days_increment):\n    original_weekday = date.weekday()\n    new_date = date + datetime.timedelta(days=days_increment)\n    new_weekday = new_date.weekday()\n    assert new_weekday == (original_weekday + days_increment) % 7\n\n@given(st.dates())\ndef test_weekday_periodicity_property(date):\n    result1 = date.weekday()\n    result2 = (date + datetime.timedelta(weeks=1)).weekday()\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int)\n    assert 0 <= result <= 6\n\n@given(st.dates())\ndef test_output_represents_correct_day_of_week_property(date):\n    result = date.weekday()\n    # Check mapping: 0 = Monday, 1 = Tuesday, ..., 6 = Sunday\n    expected = (date.toordinal() + 6) % 7\n    assert result == expected\n\n@given(st.dates())\ndef test_result_is_consistent_for_same_date_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n\n@given(st.dates())\ndef test_output_changes_predictably_with_input_increment_property(date):\n    next_day = date + datetime.timedelta(days=1)\n    result_today = date.weekday()\n    result_next_day = next_day.weekday()\n    assert result_next_day == (result_today + 1) % 7\n\n@given(st.dates())\ndef test_output_exhibits_periodicity_property(date):\n    result_today = date.weekday()\n    one_week_later = date + datetime.timedelta(weeks=1)\n    result_one_week_later = one_week_later.weekday()\n    assert result_today == result_one_week_later\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int) and 0 <= result <= 6\n\n@given(st.dates())\ndef test_output_represents_correct_day_of_week_property(date):\n    result = date.weekday()\n    assert result == (date.toordinal() + 6) % 7\n\n@given(st.dates())\ndef test_result_is_consistent_for_same_date_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n\n@given(st.dates())\ndef test_output_changes_predictably_with_input_property(date):\n    next_day = date + datetime.timedelta(days=1)\n    result_today = date.weekday()\n    result_tomorrow = next_day.weekday()\n    assert result_tomorrow == (result_today + 1) % 7\n\n@given(st.dates())\ndef test_output_exhibits_periodicity_property(date):\n    one_week_later = date + datetime.timedelta(weeks=1)\n    result_today = date.weekday()\n    result_next_week = one_week_later.weekday()\n    assert result_today == result_next_week\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_weekday_output_range_property(date):\n    result = date.weekday()\n    assert 0 <= result <= 6\n\n@given(st.dates())\ndef test_weekday_correct_day_property(date):\n    day_of_week = date.weekday()\n    assert day_of_week == (date.toordinal() + 6) % 7\n\n@given(st.dates())\ndef test_weekday_consistency_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n\n@given(st.dates())\ndef test_weekday_predictable_change_property(date):\n    next_day = date + datetime.timedelta(days=1)\n    assert (date.weekday() + 1) % 7 == next_day.weekday()\n\n@given(st.dates())\ndef test_weekday_periodicity_property(date):\n    one_week_later = date + datetime.timedelta(weeks=1)\n    assert date.weekday() == one_week_later.weekday()\n# End program\n```"], "properties": ["1. The output of the `weekday` function should always be an integer between 0 and 6, inclusive.\n2. The output value should represent the correct day of the week, where 0 corresponds to Monday and 6 corresponds to Sunday.\n3. The result of the `weekday` function should be consistent for the same date input, meaning that calling the function multiple times with the same date should yield the same result.\n4. The output should change predictably when the input date changes, specifically advancing by 1 for each day increment in the input.\n5. The output should exhibit periodicity, such that the `weekday` function returns the same value for dates that are exactly one week apart."], "api_doc": "date.weekday()\nReturn the day of the week as an integer, where Monday is 0 and Sunday is 6. For example, date(2002, 12, 4).weekday() == 2, a Wednesday. See also isoweekday().", "api_code": "def weekday(self):\n    return (self.toordinal() + 6) % 7"}
{"function_name": "decimal.Decimal.from_float", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport decimal\nimport math\n\n@given(st.integers())\ndef test_decimal_Decimal_from_float_integer_property(i):\n    result = decimal.Decimal.from_float(i)\n    assert isinstance(result, decimal.Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_from_float_finite_float_property(f):\n    result = decimal.Decimal.from_float(f)\n    assert result == decimal.Decimal(f)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_from_float_sign_property(f):\n    result = decimal.Decimal.from_float(f)\n    if f >= 0:\n        assert result >= 0\n    else:\n        assert result < 0\n\n@given(st.one_of(st.integers(), st.floats(allow_nan=False, allow_infinity=False)))\ndef test_decimal_Decimal_from_float_type_error_property(x):\n    if isinstance(x, float) and (math.isinf(x) or math.isnan(x)):\n        result = decimal.Decimal.from_float(x)\n        assert isinstance(result, decimal.Decimal)  # Should return Decimal for special floats\n    else:\n        with st.raises(TypeError):\n            decimal.Decimal.from_float(\"string\")  # Testing invalid input type\n\n@given(st.one_of(st.floats(allow_nan=True, allow_infinity=True)))\ndef test_decimal_Decimal_from_float_special_float_property(f):\n    result = decimal.Decimal.from_float(f)\n    if math.isinf(f):\n        assert result == decimal.Decimal(repr(f))\n    elif math.isnan(f):\n        assert result == decimal.Decimal(repr(f))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.integers())\ndef test_output_is_instance_of_Decimal_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_equals_decimal_of_float_property(f):\n    result = Decimal.from_float(f)\n    assert result == Decimal(f)\n\n@given(st.floats())\ndef test_sign_of_output_matches_input_property(f):\n    result = Decimal.from_float(f)\n    if f >= 0:\n        assert result.is_signed() == 0  # Positive sign\n    else:\n        assert result.is_signed() == 1  # Negative sign\n\n@given(st.one_of(st.integers(), st.floats(allow_nan=False, allow_infinity=False)))\ndef test_type_error_on_invalid_input_property(f):\n    if isinstance(f, (int, float)):\n        try:\n            Decimal.from_float(f)\n        except TypeError:\n            assert False, \"TypeError should not be raised for valid inputs\"\n    else:\n        try:\n            Decimal.from_float(\"invalid\")\n        except TypeError:\n            pass  # Expected behavior\n\n@given(st.one_of(st.floats(allow_nan=True, allow_infinity=True), st.just(float('inf')), st.just(float('-inf')), st.just(float('nan'))))\ndef test_special_float_output_property(f):\n    result = Decimal.from_float(f)\n    if math.isinf(f):\n        assert result == Decimal(repr(f))\n    elif math.isnan(f):\n        assert result == Decimal(repr(f))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_is_instance_of_Decimal_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_equals_decimal_of_float_property(f):\n    result = Decimal.from_float(f)\n    assert result == Decimal(f)\n\n@given(st.floats())\ndef test_output_sign_property(f):\n    result = Decimal.from_float(f)\n    if f >= 0:\n        assert result >= 0\n    else:\n        assert result < 0\n\n@given(st.one_of(st.integers(), st.floats(allow_nan=False, allow_infinity=False)))\ndef test_type_error_on_invalid_input_property(value):\n    if isinstance(value, (int, float)):\n        result = Decimal.from_float(value)  # Should not raise\n    else:\n        try:\n            Decimal.from_float(value)  # Should raise TypeError\n        except TypeError:\n            pass  # Expected behavior\n\n@given(st.sampled_from([float('inf'), float('-inf'), float('nan')]))\ndef test_special_float_input_property(f):\n    result = Decimal.from_float(f)\n    assert result == Decimal(repr(f))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport decimal\nimport math\n\n@given(st.integers())\ndef test_output_instance_of_Decimal_property(f):\n    result = decimal.Decimal.from_float(f)\n    assert isinstance(result, decimal.Decimal)\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_represents_finite_float_property(f):\n    if not (math.isinf(f) or math.isnan(f)):\n        result = decimal.Decimal.from_float(f)\n        assert result == decimal.Decimal(f)\n\n@given(st.floats())\ndef test_sign_of_output_property(f):\n    result = decimal.Decimal.from_float(f)\n    if f >= 0:\n        assert result.is_signed() == False\n    else:\n        assert result.is_signed() == True\n\n@given(st.one_of(st.integers(), st.floats()))\ndef test_type_error_on_invalid_input_property(f):\n    if not isinstance(f, (int, float)):\n        with pytest.raises(TypeError):\n            decimal.Decimal.from_float(f)\n\n@given(st.sampled_from([float('inf'), float('-inf'), float('nan')]))\ndef test_special_float_inputs_property(f):\n    result = decimal.Decimal.from_float(f)\n    assert result == repr(f)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_from_float_finite_float_representation_property(f):\n    assert Decimal.from_float(f) == Decimal(f)\n\n@given(st.floats(allow_nan=False, allow_infinity=True))\ndef test_decimal_Decimal_from_float_special_float_representation_property(f):\n    if math.isinf(f):\n        assert Decimal.from_float(f) == Decimal(repr(f))\n    elif math.isnan(f):\n        assert Decimal.from_float(f) == Decimal(repr(f))\n\n@given(st.integers())\ndef test_decimal_Decimal_from_float_integer_sign_property(i):\n    result = Decimal.from_float(i)\n    assert (result < 0) == (i < 0)\n\n@given(st.one_of(st.integers(), st.floats()))\ndef test_decimal_Decimal_from_float_type_error_property(value):\n    if isinstance(value, int) or isinstance(value, float):\n        # Should not raise an error\n        Decimal.from_float(value)\n    else:\n        with pytest.raises(TypeError):\n            Decimal.from_float(\"string\")  # Example of invalid type\n\n@given(st.floats())\ndef test_decimal_Decimal_from_float_instance_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n# End program\n```"], "properties": ["1. The output should be an instance of the `Decimal` class when the input is either an integer or a float.\n\n2. For any finite float input, the output should accurately represent the same numeric value as a `Decimal`, meaning `Decimal.from_float(f)` should equal `Decimal(f)`.\n\n3. If the input is positive, the sign of the output `Decimal` should be positive; if the input is negative, the sign of the output should be negative.\n\n4. The output should raise a `TypeError` if the input is neither an integer nor a float.\n\n5. For special float inputs like positive or negative infinity (`float('inf')` or `float('-inf')`) and NaN (`float('nan')`), the output should be the string representation of these values when converted to `Decimal`."], "api_doc": "classmethod from_float(f)\nAlternative constructor that only accepts instances of float or int.\n\nNote Decimal.from_float(0.1) is not the same as Decimal('0.1'). Since 0.1 is not exactly representable in binary floating point, the value is stored as the nearest representable value which is 0x1.999999999999ap-4. That equivalent value in decimal is 0.1000000000000000055511151231257827021181583404541015625.\n\nNote From Python 3.2 onwards, a Decimal instance can also be constructed directly from a float.\n>>>\nDecimal.from_float(0.1)\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\nDecimal.from_float(float('nan'))\nDecimal('NaN')\nDecimal.from_float(float('inf'))\nDecimal('Infinity')\nDecimal.from_float(float('-inf'))\nDecimal('-Infinity')\nAdded in version 3.1.", "api_code": "@classmethod\ndef from_float(cls, f):\n    if isinstance(f, int):                # handle integer inputs\n        sign = 0 if f >= 0 else 1\n        k = 0\n        coeff = str(abs(f))\n    elif isinstance(f, float):\n        if _math.isinf(f) or _math.isnan(f):\n            return cls(repr(f))\n        if _math.copysign(1.0, f) == 1.0:\n            sign = 0\n        else:\n            sign = 1\n        n, d = abs(f).as_integer_ratio()\n        k = d.bit_length() - 1\n        coeff = str(n*5**k)\n    else:\n        raise TypeError(\"argument must be int or float.\")\n\n    result = _dec_from_triple(sign, coeff, -k)\n    if cls is Decimal:\n        return result\n    else:\n        return cls(result)"}
{"function_name": "decimal.Decimal.shift", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.integers())\ndef test_output_is_valid_decimal_property(self, other):\n    result = self.shift(other)\n    assert isinstance(result, Decimal)\n\n@given(st.decimals(), st.just(0))\ndef test_output_equals_input_when_shifted_by_zero_property(self, self):\n    result = self.shift(0)\n    assert result == self\n\n@given(st.decimals(), st.integers())\ndef test_exponent_adjustment_property(self, self, other):\n    context = getcontext()\n    original_exp = self._exp\n    result = self.shift(other)\n    expected_exp = original_exp + int(other)\n    assert result._exp == expected_exp\n\n@given(st.decimals(), st.integers())\ndef test_significant_digits_within_precision_property(self, self, other):\n    context = getcontext()\n    result = self.shift(other)\n    assert len(result._int) <= context.prec\n\n@given(st.decimals(), st.integers())\ndef test_infinity_output_property(self, self, other):\n    if self._isinfinity():\n        result = self.shift(other)\n        assert result._isinfinity() == True\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.integers(min_value=-1000, max_value=1000))\ndef test_output_is_valid_Decimal_property(self_value, shift_value):\n    result = Decimal(self_value).shift(Decimal(shift_value))\n    assert isinstance(result, Decimal)\n\n@given(st.decimals(), st.integers())\ndef test_shifting_by_zero_property(self_value, shift_value):\n    result = Decimal(self_value).shift(Decimal(0))\n    assert result == Decimal(self_value)\n\n@given(st.decimals(), st.integers(min_value=-200, max_value=200))\ndef test_exponent_adjustment_property(self_value, shift_value):\n    decimal_self = Decimal(self_value)\n    original_exp = decimal_self._exp\n    result = decimal_self.shift(Decimal(shift_value))\n    assert result._exp == original_exp + int(shift_value)\n\n@given(st.decimals(), st.integers(min_value=-200, max_value=200))\ndef test_significant_digits_within_precision_property(self_value, shift_value):\n    context = getcontext()\n    decimal_self = Decimal(self_value)\n    result = decimal_self.shift(Decimal(shift_value))\n    assert len(result._int) <= context.prec\n\n@given(st.decimals(), st.integers(min_value=-1000, max_value=1000))\ndef test_infinity_preservation_property(self_value, shift_value):\n    decimal_self = Decimal(self_value)\n    if decimal_self._isinfinity():\n        result = decimal_self.shift(Decimal(shift_value))\n        assert result._isinfinity()\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n# Set a reasonable precision for Decimal context to avoid overflow issues\ngetcontext().prec = 50\n\n@given(st.decimals(), st.integers(min_value=-100, max_value=100))\ndef test_output_is_valid_decimal_property(d, shift):\n    result = d.shift(shift)\n    assert isinstance(result, Decimal)\n\n@given(st.decimals(), st.integers())\ndef test_shifting_by_zero_property(d, shift):\n    if shift == 0:\n        result = d.shift(shift)\n        assert result == d\n\n@given(st.decimals(), st.integers(min_value=-100, max_value=100))\ndef test_exponent_adjustment_property(d, shift):\n    original_exp = d.as_tuple().exponent\n    result = d.shift(shift)\n    result_exp = result.as_tuple().exponent\n    assert result_exp == original_exp + shift\n\n@given(st.decimals(), st.integers(min_value=-100, max_value=100))\ndef test_significant_digits_limit_property(d, shift):\n    result = d.shift(shift)\n    assert len(result.to_eng_string().replace('.', '').lstrip('0')) <= getcontext().prec\n\n@given(st.decimals(), st.integers(min_value=-100, max_value=100))\ndef test_infinity_preservation_property(d, shift):\n    if d.is_infinite():\n        result = d.shift(shift)\n        assert result.is_infinite() == True\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.integers())\ndef test_output_is_valid_decimal_property(self, other):\n    result = self.shift(other)\n    assert isinstance(result, Decimal)\n\n@given(st.decimals(), st.just(0))\ndef test_shifting_by_zero_property(self):\n    result = self.shift(0)\n    assert result == self\n\n@given(st.decimals(), st.integers())\ndef test_exponent_adjustment_property(self, other):\n    original_exponent = self._exp\n    result = self.shift(other)\n    expected_exponent = original_exponent + int(other)\n    assert result._exp == expected_exponent\n\n@given(st.decimals(), st.integers())\ndef test_precision_limit_property(self, other):\n    context = getcontext()\n    result = self.shift(other)\n    assert len(result.to_eng_string().replace('.', '').lstrip('-')) <= context.prec\n\n@given(st.decimals(), st.integers())\ndef test_infinity_preserved_property(self, other):\n    if self._isinfinity():\n        result = self.shift(other)\n        assert result._isinfinity() == True\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals())\ndef test_output_is_valid_decimal_property(decimal_value):\n    context = getcontext()\n    result = Decimal(decimal_value).shift(Decimal(1), context)\n    assert isinstance(result, Decimal)\n\n@given(st.decimals())\ndef test_shifting_by_zero_returns_same_value_property(decimal_value):\n    context = getcontext()\n    result = Decimal(decimal_value).shift(Decimal(0), context)\n    assert result == Decimal(decimal_value)\n\n@given(st.decimals(), st.integers())\ndef test_exponent_adjustment_property(decimal_value, shift_amount):\n    context = getcontext()\n    original_decimal = Decimal(decimal_value)\n    result = original_decimal.shift(Decimal(shift_amount), context)\n    expected_exponent = original_decimal._exp + int(shift_amount)\n    assert result._exp == expected_exponent\n\n@given(st.decimals(), st.integers())\ndef test_significant_digits_within_precision_property(decimal_value, shift_amount):\n    context = getcontext()\n    context.prec = 10  # Set precision for testing\n    result = Decimal(decimal_value).shift(Decimal(shift_amount), context)\n    significant_digits = str(result).replace('.', '').lstrip('0')\n    assert len(significant_digits) <= context.prec\n\n@given(st.decimals(), st.integers())\ndef test_infinity_preserved_property(decimal_value, shift_amount):\n    context = getcontext()\n    infinite_decimal = Decimal('Infinity')\n    result = infinite_decimal.shift(Decimal(shift_amount), context)\n    assert result == infinite_decimal\n# End program\n```"], "properties": ["1. The output should always be a valid `Decimal` object, maintaining the same sign as the input `self`.\n\n2. If the `other` argument is zero, the output should be identical to the input `self`, as shifting by zero does not change the value.\n\n3. The output should have an exponent that is equal to the input's exponent, adjusted by the integer value of `other`, ensuring the overall scale of the number is preserved.\n\n4. The number of significant digits in the output should not exceed the precision defined in the context, meaning it should always be truncated or padded to fit within the allowed precision.\n\n5. If the input `self` is infinite, the output should also be an infinite `Decimal`, regardless of the value of `other`."], "api_doc": "shift(other, context=None)\nReturn the result of shifting the digits of the first operand by an amount specified by the second operand. The second operand must be an integer in the range -precision through precision. The absolute value of the second operand gives the number of places to shift. If the second operand is positive then the shift is to the left; otherwise the shift is to the right. Digits shifted into the coefficient are zeros. The sign and exponent of the first operand are unchanged.", "api_code": "def shift(self, other, context=None):\n    if context is None:\n        context = getcontext()\n\n    other = _convert_other(other, raiseit=True)\n\n    ans = self._check_nans(other, context)\n    if ans:\n        return ans\n\n    if other._exp != 0:\n        return context._raise_error(InvalidOperation)\n    if not (-context.prec <= int(other) <= context.prec):\n        return context._raise_error(InvalidOperation)\n\n    if self._isinfinity():\n        return Decimal(self)\n\n    # get values, pad if necessary\n    torot = int(other)\n    rotdig = self._int\n    topad = context.prec - len(rotdig)\n    if topad > 0:\n        rotdig = '0'*topad + rotdig\n    elif topad < 0:\n        rotdig = rotdig[-topad:]\n\n    # let's shift!\n    if torot < 0:\n        shifted = rotdig[:torot]\n    else:\n        shifted = rotdig + '0'*torot\n        shifted = shifted[-context.prec:]\n\n    return _dec_from_triple(self._sign,\n                                shifted.lstrip('0') or '0', self._exp)"}
{"function_name": "decimal.Decimal.exp", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n# Set a context for Decimal to avoid overflow issues\ngetcontext().prec = 50\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_decimal_Decimal_exp_nan_property(x):\n    d = Decimal(x)\n    if d.is_nan():\n        assert d.exp() != d.exp()  # exp(NaN) should return NaN\n\n@given(st.floats(allow_infinity=True, min_value=float('-inf')))\ndef test_decimal_Decimal_exp_negative_infinity_property(x):\n    d = Decimal(x)\n    if d.is_infinite() and d < 0:\n        assert d.exp() == Decimal(0)  # exp(-Infinity) should return 0\n\n@given(st.floats())\ndef test_decimal_Decimal_exp_zero_property(x):\n    d = Decimal(x)\n    if d == 0:\n        assert d.exp() == Decimal(1)  # exp(0) should return 1\n\n@given(st.floats(allow_infinity=True, min_value=float('inf')))\ndef test_decimal_Decimal_exp_infinity_property(x):\n    d = Decimal(x)\n    if d.is_infinite() and d > 0:\n        assert d.exp() == d  # exp(Infinity) should return Infinity\n\n@given(st.floats())\ndef test_decimal_Decimal_exp_monotonicity_property(x1, x2):\n    d1 = Decimal(x1)\n    d2 = Decimal(x2)\n    if d1 < d2:\n        assert d1.exp() < d2.exp()  # exp(x) should be monotonically increasing\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_exp_nan_property(x):\n    # exp(NaN) should return NaN\n    result = Decimal(x).exp()\n    if x != x:  # Check for NaN\n        assert result != result  # NaN is the only value that is not equal to itself\n\n@given(st.integers(min_value=-10**10, max_value=10**10))\ndef test_exp_negative_infinity_property(x):\n    # exp(-Infinity) should return 0\n    if x < 0:\n        result = Decimal('-Infinity').exp()\n        assert result == 0\n\n@given(st.integers(min_value=-10**10, max_value=10**10))\ndef test_exp_zero_property(x):\n    # exp(0) should return 1\n    result = Decimal(0).exp()\n    assert result == 1\n\n@given(st.integers(min_value=10**10, max_value=10**20))\ndef test_exp_positive_infinity_property(x):\n    # exp(Infinity) should return Infinity\n    if x > 0:\n        result = Decimal('Infinity').exp()\n        assert result == Decimal('Infinity')\n\n@given(st.floats())\ndef test_exp_monotonically_increasing_property(x, y):\n    # exp(x) < exp(y) for x < y\n    if x < y:\n        assert Decimal(x).exp() < Decimal(y).exp()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n# Property 1: The exponential of NaN should always return NaN.\n@given(st.floats(allow_nan=True, allow_infinity=False))\ndef test_decimal_Decimal_exp_nan_property(x):\n    d = Decimal(x)\n    if d.is_nan():\n        assert d.exp().is_nan()\n\n# Property 2: The exponential of negative infinity should return zero.\n@given()\ndef test_decimal_Decimal_exp_negative_infinity_property():\n    d = Decimal('-Infinity')\n    assert d.exp() == Decimal(0)\n\n# Property 3: The exponential of zero should return one.\n@given()\ndef test_decimal_Decimal_exp_zero_property():\n    d = Decimal(0)\n    assert d.exp() == Decimal(1)\n\n# Property 4: The exponential of positive infinity should return positive infinity.\n@given()\ndef test_decimal_Decimal_exp_positive_infinity_property():\n    d = Decimal('Infinity')\n    assert d.exp() == d\n\n# Property 5: The exponential function should be monotonically increasing for all real numbers.\n@given(st.floats(min_value=-1000, max_value=1000))\ndef test_decimal_Decimal_exp_monotonicity_property(x, y):\n    if x < y:\n        assert Decimal(x).exp() < Decimal(y).exp()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_decimal_Decimal_exp_nan_property(value):\n    # Check if exp(NaN) returns NaN\n    decimal_value = Decimal(value)\n    if decimal_value.is_nan():\n        assert decimal_value.exp().is_nan()\n\n@given(st.floats(allow_infinity=True))\ndef test_decimal_Decimal_exp_negative_infinity_property(value):\n    # Check if exp(-Infinity) returns 0\n    decimal_value = Decimal(value)\n    if decimal_value == Decimal('-Infinity'):\n        assert decimal_value.exp() == Decimal(0)\n\n@given(st.floats())\ndef test_decimal_Decimal_exp_zero_property(value):\n    # Check if exp(0) returns 1\n    decimal_value = Decimal(value)\n    if decimal_value == 0:\n        assert decimal_value.exp() == Decimal(1)\n\n@given(st.floats(allow_infinity=True))\ndef test_decimal_Decimal_exp_positive_infinity_property(value):\n    # Check if exp(+Infinity) returns Infinity\n    decimal_value = Decimal(value)\n    if decimal_value == Decimal('Infinity'):\n        assert decimal_value.exp() == Decimal('Infinity')\n\n@given(st.floats())\ndef test_decimal_Decimal_exp_monotonicity_property(value1, value2):\n    # Check if the exp function is monotonically increasing\n    decimal_value1 = Decimal(value1)\n    decimal_value2 = Decimal(value2)\n    if decimal_value1 < decimal_value2:\n        assert decimal_value1.exp() < decimal_value2.exp()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n@given(st.one_of(st.floats(allow_nan=True), st.integers(), st.decimals()))\ndef test_decimal_Decimal_exp_nan_property(x):\n    result = Decimal(x).exp()\n    if Decimal(x).is_nan():\n        assert result.is_nan()\n\n@given(st.integers(min_value=-10**6, max_value=-1))\ndef test_decimal_Decimal_exp_negative_infinity_property(x):\n    result = Decimal('-Infinity').exp()\n    assert result == Decimal(0)\n\n@given(st.integers(min_value=0, max_value=10**6))\ndef test_decimal_Decimal_exp_zero_property(x):\n    result = Decimal(0).exp()\n    assert result == Decimal(1)\n\n@given(st.integers(min_value=1, max_value=10**6))\ndef test_decimal_Decimal_exp_positive_infinity_property(x):\n    result = Decimal('Infinity').exp()\n    assert result == Decimal('Infinity')\n\n@given(st.floats())\ndef test_decimal_Decimal_exp_monotonicity_property(x):\n    x1 = Decimal(x)\n    x2 = Decimal(x + 1)  # Ensure x2 is greater than x\n    assert x1.exp() < x2.exp()\n\n# End program\n```"], "properties": ["1. The exponential of NaN should always return NaN.\n2. The exponential of negative infinity should return zero.\n3. The exponential of zero should return one.\n4. The exponential of positive infinity should return positive infinity.\n5. The exponential function should be monotonically increasing for all real numbers."], "api_doc": "exp(context=None)\nReturn the value of the (natural) exponential function e**x at the given number. The result is correctly rounded using the ROUND_HALF_EVEN rounding mode.\n\n>>>\nDecimal(1).exp()\nDecimal('2.718281828459045235360287471')\nDecimal(321).exp()\nDecimal('2.561702493119680037517373933E+139')", "api_code": "def exp(self, context=None):\n\n    if context is None:\n        context = getcontext()\n\n    # exp(NaN) = NaN\n    ans = self._check_nans(context=context)\n    if ans:\n        return ans\n\n    # exp(-Infinity) = 0\n    if self._isinfinity() == -1:\n        return _Zero\n\n    # exp(0) = 1\n    if not self:\n        return _One\n\n    # exp(Infinity) = Infinity\n    if self._isinfinity() == 1:\n        return Decimal(self)"}
{"function_name": "decimal.Decimal.as_integer_ratio", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.decimals())\ndef test_output_is_tuple_property(decimal_value):\n    result = decimal_value.as_integer_ratio()\n    assert isinstance(result, tuple) and len(result) == 2\n\n@given(st.decimals())\ndef test_numerator_is_integer_property(decimal_value):\n    result = decimal_value.as_integer_ratio()\n    numerator, _ = result\n    assert isinstance(numerator, int)\n\n@given(st.decimals())\ndef test_denominator_is_positive_integer_property(decimal_value):\n    result = decimal_value.as_integer_ratio()\n    _, denominator = result\n    assert isinstance(denominator, int) and denominator > 0\n\n@given(st.decimals())\ndef test_fraction_equals_original_decimal_property(decimal_value):\n    result = decimal_value.as_integer_ratio()\n    numerator, denominator = result\n    assert Decimal(numerator) / Decimal(denominator) == decimal_value\n\n@given(st.decimals())\ndef test_gcd_is_one_property(decimal_value):\n    result = decimal_value.as_integer_ratio()\n    numerator, denominator = result\n    assert math.gcd(numerator, denominator) == 1\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.data())\ndef test_as_integer_ratio_output_structure_property(data):\n    decimal_value = data.draw(st.decimals(allow_nan=False, allow_infinity=False))\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert isinstance(numerator, int)\n    assert isinstance(denominator, int)\n\n@given(st.data())\ndef test_as_integer_ratio_numerator_property(data):\n    decimal_value = data.draw(st.decimals(allow_nan=False, allow_infinity=False))\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert isinstance(numerator, int)\n\n@given(st.data())\ndef test_as_integer_ratio_denominator_property(data):\n    decimal_value = data.draw(st.decimals(allow_nan=False, allow_infinity=False))\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert denominator > 0\n\n@given(st.data())\ndef test_as_integer_ratio_equivalence_property(data):\n    decimal_value = data.draw(st.decimals(allow_nan=False, allow_infinity=False))\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert Decimal(numerator) / Decimal(denominator) == decimal_value\n\n@given(st.data())\ndef test_as_integer_ratio_simplest_form_property(data):\n    decimal_value = data.draw(st.decimals(allow_nan=False, allow_infinity=False))\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert math.gcd(numerator, denominator) == 1\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_output_structure_property(decimal_value):\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert isinstance(numerator, int)\n    assert isinstance(denominator, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_denominator_positive_property(decimal_value):\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert denominator > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_equivalence_property(decimal_value):\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert Decimal(numerator) / Decimal(denominator) == decimal_value\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_gcd_one_property(decimal_value):\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert math.gcd(numerator, denominator) == 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_output_tuple_length_property(decimal_value):\n    result = decimal_value.as_integer_ratio()\n    assert len(result) == 2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.decimals())\ndef test_as_integer_ratio_output_is_tuple_property(decimal_value):\n    result = decimal_value.as_integer_ratio()\n    assert isinstance(result, tuple) and len(result) == 2\n\n@given(st.decimals())\ndef test_as_integer_ratio_numerator_is_integer_property(decimal_value):\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert isinstance(numerator, int)\n\n@given(st.decimals())\ndef test_as_integer_ratio_denominator_is_positive_property(decimal_value):\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert denominator > 0\n\n@given(st.decimals())\ndef test_as_integer_ratio_fraction_equals_decimal_property(decimal_value):\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert Decimal(numerator) / Decimal(denominator) == decimal_value\n\n@given(st.decimals())\ndef test_as_integer_ratio_gcd_is_one_property(decimal_value):\n    numerator, denominator = decimal_value.as_integer_ratio()\n    assert math.gcd(numerator, denominator) == 1\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nfrom math import gcd\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_tuple_length_property(value):\n    numerator, denominator = value.as_integer_ratio()\n    assert isinstance(numerator, int)\n    assert isinstance(denominator, int)\n    assert len((numerator, denominator)) == 2\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_numerator_property(value):\n    numerator, denominator = value.as_integer_ratio()\n    assert isinstance(numerator, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_denominator_property(value):\n    numerator, denominator = value.as_integer_ratio()\n    assert isinstance(denominator, int)\n    assert denominator > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_equivalence_property(value):\n    numerator, denominator = value.as_integer_ratio()\n    assert numerator / denominator == value\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_simplest_form_property(value):\n    numerator, denominator = value.as_integer_ratio()\n    assert gcd(numerator, denominator) == 1\n# End program\n```"], "properties": ["1. The output of `as_integer_ratio` should always be a tuple containing exactly two elements: the numerator and the denominator.\n\n2. The numerator returned by `as_integer_ratio` should be an integer.\n\n3. The denominator returned by `as_integer_ratio` should be a positive integer.\n\n4. The fraction represented by the numerator and denominator should be equal to the original decimal value (i.e., `numerator / denominator` should equal the decimal value).\n\n5. The greatest common divisor (GCD) of the numerator and denominator should be 1, indicating that the fraction is in its simplest form."], "api_doc": "as_integer_ratio()\nReturn a pair (n, d) of integers that represent the given Decimal instance as a fraction, in lowest terms and with a positive denominator:\n\n>>>\nDecimal('-3.14').as_integer_ratio()\n(-157, 50)\nThe conversion is exact. Raise OverflowError on infinities and ValueError on NaNs.\n\nAdded in version 3.6.", "api_code": "def as_integer_ratio(self):\n    return (self._numerator, self._denominator)"}
{"function_name": "statistics.median", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.integers(min_value=-1e9, max_value=1e9)))\ndef test_median_of_empty_list_property(data):\n    if len(data) == 0:\n        with pytest.raises(StatisticsError):\n            median(data)\n# End program\n\n@given(st.integers(min_value=-1e9, max_value=1e9))\ndef test_median_of_single_element_list_property(value):\n    assert median([value]) == value\n# End program\n\n@given(st.lists(st.integers(min_value=-1e9, max_value=1e9), min_size=1))\ndef test_median_of_odd_number_of_elements_property(data):\n    if len(data) % 2 == 1:\n        assert median(data) == median(sorted(data))\n# End program\n\n@given(st.lists(st.integers(min_value=-1e9, max_value=1e9), min_size=2))\ndef test_median_of_even_number_of_elements_property(data):\n    if len(data) % 2 == 0:\n        sorted_data = sorted(data)\n        expected_median = (sorted_data[len(sorted_data) // 2 - 1] + sorted_data[len(sorted_data) // 2]) / 2\n        assert median(data) == expected_median\n# End program\n\n@given(st.lists(st.integers(min_value=-1e9, max_value=1e9)))\ndef test_invariance_under_order_property(data):\n    assert median(data) == median(sorted(data))\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.integers()))\ndef test_empty_list_property(data):\n    if len(data) == 0:\n        with pytest.raises(StatisticsError):\n            median(data)\n\n@given(st.lists(st.integers()))\ndef test_single_element_property(data):\n    if len(data) == 1:\n        assert median(data) == data[0]\n\n@given(st.lists(st.integers()))\ndef test_odd_length_list_property(data):\n    if len(data) % 2 == 1:\n        sorted_data = sorted(data)\n        assert median(data) == sorted_data[len(data) // 2]\n\n@given(st.lists(st.integers()))\ndef test_even_length_list_property(data):\n    if len(data) % 2 == 0:\n        sorted_data = sorted(data)\n        mid_index = len(data) // 2\n        assert median(data) == (sorted_data[mid_index - 1] + sorted_data[mid_index]) / 2\n\n@given(st.lists(st.integers()))\ndef test_order_invariance_property(data):\n    sorted_data = sorted(data)\n    assert median(data) == median(sorted_data)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.integers(), min_size=0))\ndef test_empty_list_property(data):\n    if len(data) == 0:\n        with st.raises(StatisticsError):\n            median(data)\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_single_element_list_property(data):\n    assert median(data) == data[0]\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_odd_number_of_elements_property(data):\n    sorted_data = sorted(data)\n    if len(sorted_data) % 2 == 1:\n        assert median(sorted_data) == sorted_data[len(sorted_data) // 2]\n\n@given(st.lists(st.integers(), min_size=2))\ndef test_even_number_of_elements_property(data):\n    sorted_data = sorted(data)\n    if len(sorted_data) % 2 == 0:\n        i = len(sorted_data) // 2\n        assert median(sorted_data) == (sorted_data[i - 1] + sorted_data[i]) / 2\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_order_invariance_property(data):\n    sorted_data = sorted(data)\n    assert median(data) == median(sorted_data)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.integers(), min_size=0))\ndef test_empty_list_raises_statistics_error(data):\n    if len(data) == 0:\n        try:\n            median(data)\n            assert False, \"Expected StatisticsError for empty data\"\n        except StatisticsError:\n            pass\n\n@given(st.integers())\ndef test_single_element_list_equals_element(x):\n    assert median([x]) == x\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_odd_length_list_returns_middle_element(data):\n    sorted_data = sorted(data)\n    if len(sorted_data) % 2 == 1:\n        assert median(sorted_data) == sorted_data[len(sorted_data) // 2]\n\n@given(st.lists(st.integers(), min_size=2))\ndef test_even_length_list_returns_average_of_middle_elements(data):\n    sorted_data = sorted(data)\n    if len(sorted_data) % 2 == 0:\n        mid_index = len(sorted_data) // 2\n        expected_median = (sorted_data[mid_index - 1] + sorted_data[mid_index]) / 2\n        assert median(sorted_data) == expected_median\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_median_invariant_under_order(data):\n    sorted_data = sorted(data)\n    assert median(data) == median(sorted_data)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.integers(), min_size=0))\ndef test_median_empty_list_property(data):\n    if len(data) == 0:\n        with pytest.raises(StatisticsError):\n            median(data)\n    # End program\n\n@given(st.integers())\ndef test_median_single_element_list_property(x):\n    assert median([x]) == x\n    # End program\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_median_odd_length_list_property(data):\n    if len(data) % 2 == 1:\n        result = median(data)\n        sorted_data = sorted(data)\n        assert result == sorted_data[len(sorted_data) // 2]\n    # End program\n\n@given(st.lists(st.integers(), min_size=2))\ndef test_median_even_length_list_property(data):\n    if len(data) % 2 == 0:\n        result = median(data)\n        sorted_data = sorted(data)\n        mid_index = len(sorted_data) // 2\n        expected_result = (sorted_data[mid_index - 1] + sorted_data[mid_index]) / 2\n        assert result == expected_result\n    # End program\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_median_order_invariance_property(data):\n    sorted_data = sorted(data)\n    assert median(data) == median(sorted_data)\n    # End program\n```"], "properties": ["1. The median of an empty list raises a StatisticsError, indicating that there is no median for the provided data.\n2. The median of a single-element list is equal to that element itself.\n3. The median of a list with an odd number of elements is the middle element of the sorted list.\n4. The median of a list with an even number of elements is the average of the two middle elements of the sorted list.\n5. The median is invariant under the order of the input data; that is, sorting the data before finding the median should yield the same result as finding the median directly from the unsorted data."], "api_doc": "statistics.median(data)\nReturn the median (middle value) of numeric data, using the common \u201cmean of middle two\u201d method. If data is empty, StatisticsError is raised. data can be a sequence or iterable.\n\nThe median is a robust measure of central location and is less affected by the presence of outliers. When the number of data points is odd, the middle data point is returned:\n\n>>>\nmedian([1, 3, 5])\n3\nWhen the number of data points is even, the median is interpolated by taking the average of the two middle values:\n\n>>>\nmedian([1, 3, 5, 7])\n4.0\nThis is suited for when your data is discrete, and you don\u2019t mind that the median may not be an actual data point.\n\nIf the data is ordinal (supports order operations) but not numeric (doesn\u2019t support addition), consider using median_low() or median_high() instead.", "api_code": "def median(data):\n    data = sorted(data)\n    n = len(data)\n    if n == 0:\n        raise StatisticsError(\"no median for empty data\")\n    if n % 2 == 1:\n        return data[n // 2]\n    else:\n        i = n // 2\n        return (data[i - 1] + data[i]) / 2"}
{"function_name": "statistics.linear_regression", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError, LinearRegression\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2).filter(lambda x: len(set(x)) > 1), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2).filter(lambda x: len(set(x)) > 1))\ndef test_slope_zero_for_constant_y_property(x, y):\n    # All y values are constant\n    constant_y = [y[0]] * len(y)\n    result = linear_regression(x, constant_y)\n    assert result.slope == 0.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2),\n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_intercept_equals_mean_y_property(x, y):\n    result = linear_regression(x, y, proportional=True)\n    assert result.intercept == sum(y) / len(y)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2),\n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_output_valid_linear_regression_object_property(x, y):\n    result = linear_regression(x, y)\n    assert isinstance(result, LinearRegression)\n    assert isinstance(result.slope, float)\n    assert isinstance(result.intercept, float)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100),\n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_slope_continuity_property(x, y):\n    base_slope = linear_regression(x, y).slope\n    modified_x = [xi + 1 for xi in x]  # Slightly modify x\n    new_slope = linear_regression(modified_x, y).slope\n    assert new_slope != base_slope  # Expect a change in slope\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_minimum_data_points_property(x):\n    if len(x) < 2:\n        with pytest.raises(StatisticsError):\n            linear_regression(x, x)\n    else:\n        linear_regression(x, x)  # Should not raise an error\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError, LinearRegression\nimport numpy as np\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_slope_zero_when_y_constant_property(xs, ys):\n    constant_value = ys[0]\n    ys = [constant_value] * len(xs)\n    result = linear_regression(xs, ys)\n    assert result.slope == 0.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2),\n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_intercept_equals_mean_y_when_slope_zero_property(xs, ys):\n    constant_value = ys[0]\n    ys = [constant_value] * len(xs)\n    result = linear_regression(xs, ys, proportional=False)\n    assert result.intercept == np.mean(ys)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_output_is_linear_regression_object_property(xs, ys):\n    result = linear_regression(xs, ys)\n    assert isinstance(result, LinearRegression)\n    assert isinstance(result.slope, float)\n    assert isinstance(result.intercept, float)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_slope_changes_continuously_property(xs, ys):\n    result1 = linear_regression(xs, ys)\n    ys[0] += 1  # Modify ys to create a different linear relationship\n    result2 = linear_regression(xs, ys)\n    assert result1.slope != result2.slope\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_statistics_error_with_less_than_two_points_property(xs, ys):\n    if len(xs) < 2 or len(ys) < 2:\n        try:\n            linear_regression(xs, ys)\n            assert False  # Should have raised an error\n        except StatisticsError:\n            assert True  # The error was raised as expected\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError, LinearRegression\nimport numpy as np\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_slope_zero_when_y_constant_property(xs, ys):\n    # If all y values are constant, the slope should be zero\n    y_constant = [ys[0]] * len(xs)\n    result = linear_regression(xs, y_constant)\n    assert result.slope == 0.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_intercept_equals_mean_when_proportional_zero_slope_property(xs, ys):\n    # If inputs have the same y value, the intercept should equal the mean of y when proportional is False\n    y_constant = [ys[0]] * len(xs)\n    result = linear_regression(xs, y_constant, proportional=False)\n    assert result.intercept == np.mean(y_constant)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_output_is_valid_LinearRegression_object_property(xs, ys):\n    # The output should be a valid LinearRegression object\n    result = linear_regression(xs, ys)\n    assert isinstance(result, LinearRegression)\n    assert isinstance(result.slope, float)\n    assert isinstance(result.intercept, float)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_slope_changes_continuously_property(xs, ys):\n    # The slope should change as the input values change\n    original_result = linear_regression(xs, ys)\n    modified_xs = [x + 1 for x in xs]\n    modified_result = linear_regression(modified_xs, ys)\n    assert original_result.slope != modified_result.slope\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_statistics_error_when_n_less_than_2_property(xs):\n    # The function should raise StatisticsError when n < 2\n    if len(xs) < 2:\n        try:\n            linear_regression(xs, xs)\n        except StatisticsError:\n            pass  # Expected behavior\n        else:\n            assert False  # Should not reach here\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError, LinearRegression\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), st.lists(st.floats(), min_size=2))\ndef test_slope_zero_when_y_constant_property(x, y):\n    # Ensure all y values are the same (constant)\n    constant_y = [y[0]] * len(y)\n    result = linear_regression(x, constant_y)\n    assert result.slope == 0.0\n\n@given(st.lists(st.floats(), min_size=2), st.lists(st.floats(), min_size=2))\ndef test_intercept_equals_mean_when_slope_zero_property(x, y):\n    # Ensure that the slope is zero\n    constant_y = [y[0]] * len(y)\n    result = linear_regression(x, constant_y)\n    assert result.intercept == sum(constant_y) / len(constant_y)\n\n@given(st.lists(st.floats(), min_size=2), st.lists(st.floats(), min_size=2))\ndef test_output_is_linear_regression_object_property(x, y):\n    result = linear_regression(x, y)\n    assert isinstance(result, LinearRegression)\n    assert isinstance(result.slope, float)\n    assert isinstance(result.intercept, float)\n\n@given(st.lists(st.floats(), min_size=2), st.lists(st.floats(), min_size=2))\ndef test_slope_changes_continuously_property(x, y):\n    # Generate pairs of lists with slight variations\n    result1 = linear_regression(x, y)\n    perturbed_y = [yi + 0.1 for yi in y]\n    result2 = linear_regression(x, perturbed_y)\n    assert result1.slope != result2.slope\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_statistics_error_when_n_less_than_two_property(x):\n    if len(x) < 2:\n        try:\n            linear_regression(x, x)\n            assert False, \"Should have raised StatisticsError\"\n        except StatisticsError:\n            pass  # Expected behavior\n```\n", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError\nimport numpy as np\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) > 1))\ndef test_slope_zero_when_y_constant_property(y_values):\n    x_values = np.random.rand(len(y_values)).tolist()  # Random x values\n    constant_y = [y_values[0]] * len(y_values)  # Make y constant\n    result = linear_regression(x_values, constant_y)\n    assert result.slope == 0.0\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) > 1))\ndef test_intercept_equals_mean_y_when_slope_zero_property(y_values):\n    x_values = np.random.rand(len(y_values)).tolist()  # Random x values\n    constant_y = [y_values[0]] * len(y_values)  # Make y constant\n    result = linear_regression(x_values, constant_y, proportional=False)\n    assert result.intercept == np.mean(constant_y)\n\n@given(st.lists(st.floats(), min_size=2, max_size=100).filter(lambda lst: len(set(lst)) > 1),\n                st.lists(st.floats(), min_size=2, max_size=100).filter(lambda lst: len(set(lst)) > 1)))\ndef test_output_is_valid_linear_regression_object_property(x_values, y_values):\n    result = linear_regression(x_values, y_values)\n    assert hasattr(result, 'slope')\n    assert hasattr(result, 'intercept')\n    assert isinstance(result.slope, float)\n    assert isinstance(result.intercept, float)\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) > 1))\ndef test_slope_changes_continuously_property(y_values):\n    x_values = np.random.rand(len(y_values)).tolist()  # Random x values\n    initial_result = linear_regression(x_values, y_values)\n    x_values[0] += 1.0  # Modify x values slightly\n    modified_result = linear_regression(x_values, y_values)\n    assert initial_result.slope != modified_result.slope\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_statistics_error_raised_with_less_than_two_data_points_property(y_values):\n    if len(y_values) < 2:\n        with pytest.raises(StatisticsError):\n            linear_regression([1.0], y_values)  # Only one x value\n# End program\n```"], "properties": ["1. The slope of the linear regression output should be equal to zero when all input `y` values are constant (regardless of the `x` values provided).\n\n2. The intercept of the linear regression output should equal the mean of the `y` values when the `proportional` flag is set to `False` and the slope is zero.\n\n3. The output of the linear regression should produce a valid `LinearRegression` object containing numeric values for both slope and intercept.\n\n4. The slope of the linear regression output should change continuously as the input data points are varied, assuming the relationship between `x` and `y` is linear.\n\n5. When the number of data points `n` is less than 2, the function should raise a `StatisticsError`, ensuring that the function enforces the requirement for a minimum number of data points."], "api_doc": "statistics.linear_regression(x, y, /, *, proportional=False)\nReturn the slope and intercept of simple linear regression parameters estimated using ordinary least squares. Simple linear regression describes the relationship between an independent variable x and a dependent variable y in terms of this linear function:\n\ny = slope * x + intercept + noise\n\nwhere slope and intercept are the regression parameters that are estimated, and noise represents the variability of the data that was not explained by the linear regression (it is equal to the difference between predicted and actual values of the dependent variable).\n\nBoth inputs must be of the same length (no less than two), and the independent variable x cannot be constant; otherwise a StatisticsError is raised.\n\nFor example, we can use the release dates of the Monty Python films to predict the cumulative number of Monty Python films that would have been produced by 2019 assuming that they had kept the pace.\n\n>>>\nyear = [1971, 1975, 1979, 1982, 1983]\nfilms_total = [1, 2, 3, 4, 5]\nslope, intercept = linear_regression(year, films_total)\nround(slope * 2019 + intercept)\n16\nIf proportional is true, the independent variable x and the dependent variable y are assumed to be directly proportional. The data is fit to a line passing through the origin. Since the intercept will always be 0.0, the underlying linear function simplifies to:\n\ny = slope * x + noise\n\nContinuing the example from correlation(), we look to see how well a model based on major planets can predict the orbital distances for dwarf planets:\n\n>>>\nmodel = linear_regression(period_squared, dist_cubed, proportional=True)\nslope = model.slope\n\n# Dwarf planets:   Pluto,  Eris,    Makemake, Haumea, Ceres\norbital_periods = [90_560, 204_199, 111_845, 103_410, 1_680]  # days\npredicted_dist = [math.cbrt(slope * (p * p)) for p in orbital_periods]\nlist(map(round, predicted_dist))\n[5912, 10166, 6806, 6459, 414]\n\n[5_906, 10_152, 6_796, 6_450, 414]  # actual distance in million km\n[5906, 10152, 6796, 6450, 414]\nAdded in version 3.10.\n\nChanged in version 3.11: Added support for proportional.", "api_code": "def linear_regression(x, y, /, *, proportional=False):\n    n = len(x)\n    if len(y) != n:\n        raise StatisticsError('linear regression requires that both inputs have same number of data points')\n    if n < 2:\n        raise StatisticsError('linear regression requires at least two data points')\n    if not proportional:\n        xbar = fsum(x) / n\n        ybar = fsum(y) / n\n        x = [xi - xbar for xi in x]  # List because used three times below\n        y = (yi - ybar for yi in y)  # Generator because only used once below\n    sxy = sumprod(x, y) + 0.0        # Add zero to coerce result to a float\n    sxx = sumprod(x, x)\n    try:\n        slope = sxy / sxx   # equivalent to:  covariance(x, y) / variance(x)\n    except ZeroDivisionError:\n        raise StatisticsError('x is constant')\n    intercept = 0.0 if proportional else ybar - slope * xbar\n    return LinearRegression(slope=slope, intercept=intercept)"}
{"function_name": "networkx.algorithms.approximation.clique.large_clique_size", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_is_non_negative_property(data):\n    G = data.draw(st.one_of(st.lists(st.integers()), st.sets(st.integers())))\n    if G:\n        graph = nx.Graph(G)\n        result = nx.algorithms.approximation.clique.large_clique_size(graph)\n        assert result >= 0\n\n@given(st.data())\ndef test_output_does_not_exceed_node_count_property(data):\n    G = data.draw(st.one_of(st.lists(st.integers()), st.sets(st.integers())))\n    graph = nx.Graph(G)\n    result = nx.algorithms.approximation.clique.large_clique_size(graph)\n    assert result <= graph.number_of_nodes()\n\n@given(st.data())\ndef test_empty_graph_returns_zero_property(data):\n    G = []\n    graph = nx.Graph(G)\n    result = nx.algorithms.approximation.clique.large_clique_size(graph)\n    assert result == 0\n\n@given(st.data())\ndef test_output_with_max_degree_property(data):\n    G = data.draw(st.sets(st.integers()))\n    graph = nx.Graph(G)\n    for node in graph.nodes:\n        graph.add_edges_from([(node, neighbor) for neighbor in graph.nodes if neighbor != node])\n    result = nx.algorithms.approximation.clique.large_clique_size(graph)\n    max_degree = max(dict(graph.degree).values(), default=0)\n    assert result <= max_degree + 1\n\n@given(st.data())\ndef test_output_zero_for_isolated_nodes_property(data):\n    G = data.draw(st.sets(st.integers()))\n    graph = nx.Graph(G)\n    for node in graph.nodes:\n        graph.add_node(node)\n    result = nx.algorithms.approximation.clique.large_clique_size(graph)\n    assert result == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation.clique import large_clique_size\n\n@given(st.data())\ndef test_output_is_non_negative_property(data):\n    G = data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=0).map(nx.Graph))\n    result = large_clique_size(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_output_does_not_exceed_node_count_property(data):\n    G = data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=0).map(nx.Graph))\n    result = large_clique_size(G)\n    assert result <= G.number_of_nodes()\n\n@given(st.data())\ndef test_output_zero_for_empty_graph_property(data):\n    G = nx.Graph()  # An empty graph\n    result = large_clique_size(G)\n    assert result == 0\n\n@given(st.data())\ndef test_output_within_max_degree_plus_one_property(data):\n    G = data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1).map(nx.Graph))\n    result = large_clique_size(G)\n    max_degree = max(dict(G.degree()).values())\n    assert result <= max_degree + 1\n\n@given(st.data())\ndef test_output_zero_for_isolated_nodes_property(data):\n    G = nx.Graph()\n    isolated_nodes = data.draw(st.lists(st.integers(), min_size=1))\n    G.add_nodes_from(isolated_nodes)  # Add isolated nodes\n    result = large_clique_size(G)\n    assert result == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_large_clique_size_output_non_negative():\n    G = st.builds(nx.Graph, st.lists(st.integers(), min_size=1)).example()\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_large_clique_size_output_not_exceeding_node_count():\n    G = st.builds(nx.Graph, st.lists(st.integers(), min_size=1)).example()\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result <= G.number_of_nodes()\n\n@given(st.data())\ndef test_large_clique_size_empty_graph():\n    G = nx.Graph()\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result == 0\n\n@given(st.data())\ndef test_large_clique_size_output_bounded_by_max_degree():\n    G = st.builds(nx.Graph, st.lists(st.integers(), min_size=1)).example()\n    max_degree = max(dict(G.degree()).values(), default=0)\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result <= max_degree + 1\n\n@given(st.data())\ndef test_large_clique_size_isolated_nodes():\n    G = nx.Graph()\n    isolated_nodes = st.lists(st.integers(), min_size=1).example()\n    G.add_nodes_from(isolated_nodes)\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_large_clique_size_non_negative_property(data):\n    G = data.draw(st.one_of(st.graphs(), st.sets(st.integers(min_value=0, max_value=100))))\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_large_clique_size_not_exceed_node_count_property(data):\n    G = data.draw(st.graphs())\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result <= G.number_of_nodes()\n\n@given(st.data())\ndef test_large_clique_size_empty_graph_property(data):\n    G = nx.Graph()  # Create an empty graph\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result == 0\n\n@given(st.data())\ndef test_large_clique_size_max_degree_property(data):\n    G = data.draw(st.graphs())\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    max_degree = max(dict(G.degree()).values(), default=0)\n    assert result <= max_degree + 1\n\n@given(st.data())\ndef test_large_clique_size_isolated_nodes_property(data):\n    G = nx.Graph()\n    G.add_nodes_from(data.draw(st.lists(st.integers(), min_size=1)))\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation.clique import large_clique_size\n\n@given(st.data())\ndef test_large_clique_size_non_negative_property(data):\n    G = data.draw(st.shapes.basic_graphs())\n    result = large_clique_size(G)\n    assert result >= 0  # Output should be a non-negative integer\n\n@given(st.data())\ndef test_large_clique_size_not_exceed_node_count_property(data):\n    G = data.draw(st.shapes.basic_graphs())\n    result = large_clique_size(G)\n    assert result <= len(G.nodes)  # Output should not exceed the number of nodes\n\n@given(st.data())\ndef test_large_clique_size_empty_graph_property(data):\n    G = nx.Graph()  # Create an empty graph\n    result = large_clique_size(G)\n    assert result == 0  # Output should be zero for an empty graph\n\n@given(st.data())\ndef test_large_clique_size_max_degree_property(data):\n    G = data.draw(st.shapes.basic_graphs())\n    result = large_clique_size(G)\n    max_degree = max(dict(G.degree).values(), default=0)\n    assert result <= max_degree + 1  # Output should not exceed max degree + 1\n\n@given(st.data())\ndef test_large_clique_size_isolated_nodes_property(data):\n    G = nx.Graph()\n    G.add_nodes_from(data.draw(st.lists(st.integers(), min_size=1, max_size=100)))  # Add isolated nodes\n    result = large_clique_size(G)\n    assert result == 0  # Output should be zero for a graph with only isolated nodes\n# End program\n```"], "properties": ["1. The output should be a non-negative integer, representing the size of the largest clique in the graph.\n2. The output should not exceed the total number of nodes in the graph, as a clique cannot be larger than the number of nodes present.\n3. If the graph is empty (contains no nodes), the output should be zero, indicating that there are no cliques.\n4. The output should be consistent with the maximum degree of any node in the graph; it cannot be larger than the maximum degree plus one, as cliques are formed by connected nodes.\n5. If the graph contains only isolated nodes (nodes with no edges), the output should also be zero, as there are no connections to form a clique."], "api_doc": "large_clique_size\nlarge_clique_size(G)[source]\nFind the size of a large clique in a graph.\n\nA clique is a subset of nodes in which each pair of nodes is adjacent. This function is a heuristic for finding the size of a large clique in the graph.\n\nParameters\n:\nG\nNetworkX graph\nReturns\n:\nk: integer\nThe size of a large clique in the graph.\n\nRaises\n:\nNetworkXNotImplemented\nIf the graph is directed or is a multigraph.\n\nSee also\n\nnetworkx.algorithms.approximation.clique.max_clique()\nA function that returns an approximate maximum clique with a guarantee on the approximation ratio.\n\nnetworkx.algorithms.clique\nFunctions for finding the exact maximum clique in a graph.\n\nNotes\n\nThis implementation is from [1]. Its worst case time complexity is \n, where n is the number of nodes in the graph and d is the maximum degree.\n\nThis function is a heuristic, which means it may work well in practice, but there is no rigorous mathematical guarantee on the ratio between the returned number and the actual largest clique size in the graph.\n\nReferences\n\n[1]\nPattabiraman, Bharath, et al. \u201cFast Algorithms for the Maximum Clique Problem on Massive Graphs with Applications to Overlapping Community Detection.\u201d Internet Mathematics 11.4-5 (2015): 421\u2013448. <https://doi.org/10.1080/15427951.2014.986778>\n\nExamples\n\nG = nx.path_graph(10)\nnx.approximation.large_clique_size(G)\n2", "api_code": "@not_implemented_for(\"directed\")\n@not_implemented_for(\"multigraph\")\n@nx._dispatchable\ndef large_clique_size(G):\n    degrees = G.degree\n\n    def _clique_heuristic(G, U, size, best_size):\n        if not U:\n            return max(best_size, size)\n        u = max(U, key=degrees)\n        U.remove(u)\n        N_prime = {v for v in G[u] if degrees[v] >= best_size}\n        return _clique_heuristic(G, U & N_prime, size + 1, best_size)\n\n    best_size = 0\n    nodes = (u for u in G if degrees[u] >= best_size)\n    for u in nodes:\n        neighbors = {v for v in G[u] if degrees[v] >= best_size}\n        best_size = _clique_heuristic(G, neighbors, 1, best_size)\n    return best_size"}
{"function_name": "datetime.datetime.combine", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@st.composite\ndef date_time_pairs(draw):\n    date = draw(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\n    time = draw(st.times())\n    return date, time\n\n@given(date_time_pairs())\ndef test_output_year_month_day_property(date_time):\n    date, time = date_time\n    result = datetime.datetime.combine(date, time)\n    assert result.year == date.year\n    assert result.month == date.month\n    assert result.day == date.day\n\n@given(date_time_pairs())\ndef test_output_hour_minute_second_microsecond_property(date_time):\n    date, time = date_time\n    result = datetime.datetime.combine(date, time)\n    assert result.hour == time.hour\n    assert result.minute == time.minute\n    assert result.second == time.second\n    assert result.microsecond == time.microsecond\n\n@given(date_time_pairs())\ndef test_tzinfo_property_when_true(date_time):\n    date, time = date_time\n    result = datetime.datetime.combine(date, time, tzinfo=True)\n    assert result.tzinfo == time.tzinfo\n\n@given(st.one_of(st.dates(), st.integers()), st.times())\ndef test_type_error_on_invalid_date_property(invalid_date, time):\n    try:\n        result = datetime.datetime.combine(invalid_date, time)\n        assert False, \"Expected TypeError not raised\"\n    except TypeError:\n        pass\n\n@given(st.dates(), st.one_of(st.integers(), st.dates()))\ndef test_type_error_on_invalid_time_property(date, invalid_time):\n    try:\n        result = datetime.datetime.combine(date, invalid_time)\n        assert False, \"Expected TypeError not raised\"\n    except TypeError:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n# Generate valid date and time instances\n@st.composite\ndef date_time_strategy(draw):\n    year = draw(st.integers(min_value=1, max_value=9999))\n    month = draw(st.integers(min_value=1, max_value=12))\n    day = draw(st.integers(min_value=1, max_value=31))\n    hour = draw(st.integers(min_value=0, max_value=23))\n    minute = draw(st.integers(min_value=0, max_value=59))\n    second = draw(st.integers(min_value=0, max_value=59))\n    microsecond = draw(st.integers(min_value=0, max_value=999999))\n    \n    # Create date and time objects\n    date = datetime.date(year, month, day)\n    time = datetime.time(hour, minute, second, microsecond)\n    \n    return date, time\n\n@given(date_time_strategy())\ndef test_output_date_property(data):\n    date, time = data\n    dt = datetime.datetime.combine(date, time)\n    assert dt.year == date.year\n    assert dt.month == date.month\n    assert dt.day == date.day\n\n@given(date_time_strategy())\ndef test_output_time_property(data):\n    date, time = data\n    dt = datetime.datetime.combine(date, time)\n    assert dt.hour == time.hour\n    assert dt.minute == time.minute\n    assert dt.second == time.second\n    assert dt.microsecond == time.microsecond\n\n@given(date_time_strategy())\ndef test_tzinfo_property(data):\n    date, time = data\n    dt = datetime.datetime.combine(date, time, tzinfo=True)\n    assert dt.tzinfo == time.tzinfo\n\n@given(st.one_of(\n    st.builds(datetime.date, st.integers(min_value=1, max_value=9999), \n               st.integers(min_value=1, max_value=12), \n               st.integers(min_value=1, max_value=31)),\n    st.builds(datetime.time, st.integers(min_value=0, max_value=23), \n               st.integers(min_value=0, max_value=59), \n               st.integers(min_value=0, max_value=59), \n               st.integers(min_value=0, max_value=999999))\n))\ndef test_invalid_date_property(invalid_date):\n    with pytest.raises(TypeError):\n        datetime.datetime.combine(invalid_date, datetime.time(12))\n\n@given(st.one_of(\n    st.builds(datetime.date, st.integers(min_value=1, max_value=9999), \n               st.integers(min_value=1, max_value=12), \n               st.integers(min_value=1, max_value=31)),\n    st.builds(datetime.time, st.integers(min_value=0, max_value=23), \n               st.integers(min_value=0, max_value=59), \n               st.integers(min_value=0, max_value=59), \n               st.integers(min_value=0, max_value=999999))\n))\ndef test_invalid_time_property(invalid_time):\n    with pytest.raises(TypeError):\n        datetime.datetime.combine(datetime.date.today(), invalid_time)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@st.composite\ndef date_time_pairs(draw):\n    # Generate valid date and time instances\n    date = draw(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\n    time = draw(st.times())\n    return date, time\n\n@given(date_time_pairs())\ndef test_output_has_same_year_month_day(date_time):\n    date, time = date_time\n    result = datetime.datetime.combine(date, time)\n    assert result.year == date.year\n    assert result.month == date.month\n    assert result.day == date.day\n\n@given(date_time_pairs())\ndef test_output_has_same_hour_minute_second_microsecond(date_time):\n    date, time = date_time\n    result = datetime.datetime.combine(date, time)\n    assert result.hour == time.hour\n    assert result.minute == time.minute\n    assert result.second == time.second\n    assert result.microsecond == time.microsecond\n\n@given(date_time_pairs())\ndef test_output_timezone_if_tzinfo_true(date_time):\n    date, time = date_time\n    result = datetime.datetime.combine(date, time, tzinfo=True)\n    assert result.tzinfo == time.tzinfo\n\n@given(st.one_of(\n    st.integers(min_value=1, max_value=9999),\n    st.floats(min_value=1, max_value=9999),\n    st.text()\n))\ndef test_type_error_for_invalid_date(invalid_date):\n    invalid_time = datetime.time()\n    try:\n        datetime.datetime.combine(invalid_date, invalid_time)\n        assert False, \"Expected TypeError for invalid date\"\n    except TypeError:\n        pass\n\n@given(st.one_of(\n    st.integers(min_value=0, max_value=23),\n    st.floats(min_value=0, max_value=23),\n    st.text()\n))\ndef test_type_error_for_invalid_time(invalid_time):\n    valid_date = datetime.date.today()\n    try:\n        datetime.datetime.combine(valid_date, invalid_time)\n        assert False, \"Expected TypeError for invalid time\"\n    except TypeError:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@st.composite\ndef date_time_strategy(draw):\n    year = draw(st.integers(min_value=1, max_value=9999))\n    month = draw(st.integers(min_value=1, max_value=12))\n    day = draw(st.integers(min_value=1, max_value=31))  # Simplified, does not check for month validity\n    hour = draw(st.integers(min_value=0, max_value=23))\n    minute = draw(st.integers(min_value=0, max_value=59))\n    second = draw(st.integers(min_value=0, max_value=59))\n    microsecond = draw(st.integers(min_value=0, max_value=999999))\n    \n    date = datetime.date(year, month, day)\n    time = datetime.time(hour, minute, second, microsecond)\n    return date, time\n\n@given(date_time_strategy())\ndef test_datetime_combine_date_property(inputs):\n    date, time = inputs\n    result = datetime.datetime.combine(date, time)\n    assert result.year == date.year\n    assert result.month == date.month\n    assert result.day == date.day\n\n@given(date_time_strategy())\ndef test_datetime_combine_time_property(inputs):\n    date, time = inputs\n    result = datetime.datetime.combine(date, time)\n    assert result.hour == time.hour\n    assert result.minute == time.minute\n    assert result.second == time.second\n    assert result.microsecond == time.microsecond\n\n@given(date_time_strategy())\ndef test_datetime_combine_tzinfo_property(inputs):\n    date, time = inputs\n    result = datetime.datetime.combine(date, time, tzinfo=True)\n    assert result.tzinfo == time.tzinfo\n\n@given(st.one_of(\n    st.builds(datetime.date, st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31)),\n    st.builds(datetime.datetime, st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\n))\ndef test_datetime_combine_invalid_date_property(invalid_date):\n    with pytest.raises(TypeError):\n        datetime.datetime.combine(invalid_date, datetime.time())\n\n@given(st.one_of(\n    st.builds(datetime.time, st.integers(min_value=0, max_value=23), st.integers(min_value=0, max_value=59), st.integers(min_value=0, max_value=59), st.integers(min_value=0, max_value=999999)),\n    st.builds(datetime.datetime, st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\n))\ndef test_datetime_combine_invalid_time_property(invalid_time):\n    with pytest.raises(TypeError):\n        datetime.datetime.combine(datetime.date.today(), invalid_time)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, date, time, timedelta\n\n@given(st.dates(), st.times())\ndef test_datetime_year_month_day_property(date_arg, time_arg):\n    dt = datetime.combine(date_arg, time_arg)\n    assert dt.year == date_arg.year\n    assert dt.month == date_arg.month\n    assert dt.day == date_arg.day\n# End program\n\n@given(st.dates(), st.times())\ndef test_datetime_hour_minute_second_microsecond_property(date_arg, time_arg):\n    dt = datetime.combine(date_arg, time_arg)\n    assert dt.hour == time_arg.hour\n    assert dt.minute == time_arg.minute\n    assert dt.second == time_arg.second\n    assert dt.microsecond == time_arg.microsecond\n# End program\n\n@given(st.dates(), st.times())\ndef test_datetime_tzinfo_true_property(date_arg, time_arg):\n    dt = datetime.combine(date_arg, time_arg, tzinfo=True)\n    assert dt.tzinfo == time_arg.tzinfo\n# End program\n\n@given(st.dates(), st.times())\ndef test_datetime_type_error_on_invalid_date_property(invalid_date, time_arg):\n    invalid_date = 'not_a_date'  # This simulates an invalid date\n    try:\n        datetime.combine(invalid_date, time_arg)\n    except TypeError:\n        pass  # Expected exception\n    else:\n        assert False, \"Expected TypeError not raised\"\n# End program\n\n@given(st.dates(), st.times())\ndef test_datetime_type_error_on_invalid_time_property(date_arg, invalid_time):\n    invalid_time = 'not_a_time'  # This simulates an invalid time\n    try:\n        datetime.combine(date_arg, invalid_time)\n    except TypeError:\n        pass  # Expected exception\n    else:\n        assert False, \"Expected TypeError not raised\"\n# End program\n```"], "properties": ["1. The output datetime object should have the same year, month, and day as the provided date argument.\n\n2. The output datetime object should have the same hour, minute, second, and microsecond as the provided time argument.\n\n3. If the tzinfo argument is set to True, the output datetime object should have the same timezone information as the provided time argument.\n\n4. The output datetime object should raise a TypeError if the date argument is not an instance of a date class.\n\n5. The output datetime object should raise a TypeError if the time argument is not an instance of a time class."], "api_doc": "classmethod datetime.combine(date, time, tzinfo=time.tzinfo)\nReturn a new datetime object whose date components are equal to the given date object\u2019s, and whose time components are equal to the given time object\u2019s. If the tzinfo argument is provided, its value is used to set the tzinfo attribute of the result, otherwise the tzinfo attribute of the time argument is used. If the date argument is a datetime object, its time components and tzinfo attributes are ignored.\n\nFor any datetime object d, d == datetime.combine(d.date(), d.time(), d.tzinfo).\n\nChanged in version 3.6: Added the tzinfo argument.", "api_code": "@classmethod\ndef combine(cls, date, time, tzinfo=True):\n    if not isinstance(date, _date_class):\n        raise TypeError(\"date argument must be a date instance\")\n    if not isinstance(time, _time_class):\n        raise TypeError(\"time argument must be a time instance\")\n    if tzinfo is True:\n        tzinfo = time.tzinfo\n    return cls(date.year, date.month, date.day,\n                time.hour, time.minute, time.second, time.microsecond,\n                tzinfo, fold=time.fold)"}
{"function_name": "decimal.Decimal.fma", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_correctness_property(x, y, z):\n    \"\"\"Property: The result of fma(x, y, z) should equal x * y + z for finite inputs.\"\"\"\n    if x.is_finite() and y.is_finite():\n        assert x.fma(y, z) == x * y + z\n\n@given(st.decimals())\ndef test_fma_signaling_nan_property(x):\n    \"\"\"Property: If x is a signaling NaN, fma(x, y, z) should raise an InvalidOperation error.\"\"\"\n    if x.is_snan():\n        with pytest.raises(InvalidOperation):\n            x.fma(Decimal(1), Decimal(1))\n\n@given(st.decimals())\ndef test_fma_quiet_nan_property(x):\n    \"\"\"Property: If x is a quiet NaN, fma(x, y, z) should return y + z.\"\"\"\n    if x.is_nan():\n        result = x.fma(Decimal(1), Decimal(1))\n        assert result == Decimal(1) + Decimal(1)\n\n@given(st.decimals(), st.decimals())\ndef test_fma_infinity_property(x, y):\n    \"\"\"Property: If x or y is infinity, the product should be calculated correctly.\"\"\"\n    if x.is_infinite() or y.is_infinite():\n        if x.is_infinite() and y.is_zero():\n            with pytest.raises(InvalidOperation):\n                x.fma(y, Decimal(1))\n        else:\n            result = x.fma(y, Decimal(1))\n            assert result.is_infinite()\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_sign_property(x, y, z):\n    \"\"\"Property: The output of fma(x, y, z) should maintain the sign of x * y.\"\"\"\n    if x.is_finite() and y.is_finite() and x != 0 and y != 0:\n        product_sign = (x.sign() * y.sign())\n        result = x.fma(y, z)\n        assert result.sign() == product_sign\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n# Adjust the precision context to handle large numbers\ngetcontext().prec = 50\n\n@given(st.floats(allow_nan=False, allow_infinity=False), \n       st.floats(allow_nan=False, allow_infinity=False), \n       st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_fma_basic_property(a, b, c):\n    dec_a = Decimal(a)\n    dec_b = Decimal(b)\n    dec_c = Decimal(c)\n    result = dec_a.fma(dec_b, dec_c)\n    expected = dec_a * dec_b + dec_c\n    assert result == expected\n\n@given(st.floats(allow_nan=True, allow_infinity=False), \n       st.floats(allow_nan=False, allow_infinity=False), \n       st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_fma_sNaN_property(a, b, c):\n    dec_a = Decimal(a)\n    dec_b = Decimal(b)\n    dec_c = Decimal(c)\n    if dec_a.is_nan():\n        try:\n            dec_a.fma(dec_b, dec_c)\n            assert False, \"Expected InvalidOperation for sNaN\"\n        except InvalidOperation:\n            pass\n\n@given(st.floats(allow_nan=False, allow_infinity=False), \n       st.floats(allow_nan=True, allow_infinity=False), \n       st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_fma_nNaN_property(a, b, c):\n    dec_a = Decimal(a)\n    dec_b = Decimal(b)\n    dec_c = Decimal(c)\n    if dec_b.is_nan():\n        result = dec_a.fma(dec_b, dec_c)\n        assert result == dec_a\n\n@given(st.floats(allow_nan=False, allow_infinity=True), \n       st.floats(allow_nan=False, allow_infinity=True), \n       st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_fma_infinity_property(a, b, c):\n    dec_a = Decimal(a)\n    dec_b = Decimal(b)\n    dec_c = Decimal(c)\n    if dec_a.is_infinite() or dec_b.is_infinite():\n        try:\n            result = dec_a.fma(dec_b, dec_c)\n            if dec_a.is_zero() or dec_b.is_zero():\n                assert False, \"Expected InvalidOperation for INF * 0\"\n        except InvalidOperation:\n            pass\n\n@given(st.floats(allow_nan=False, allow_infinity=False), \n       st.floats(allow_nan=False, allow_infinity=False), \n       st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_fma_sign_property(a, b, c):\n    dec_a = Decimal(a)\n    dec_b = Decimal(b)\n    dec_c = Decimal(c)\n    if dec_a != 0 and dec_b != 0:\n        result = dec_a.fma(dec_b, dec_c)\n        expected_sign = (dec_a.sign() ^ dec_b.sign())\n        assert result.sign() == expected_sign\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_decimal_Decimal_fma_property_non_special_values(a, b, c):\n    if a.is_nan() or b.is_nan() or a.is_infinite() or b.is_infinite():\n        return  # Skip if any operand is NaN or Infinity\n    result = a.fma(b, c)\n    expected = a * b + c\n    assert result == expected\n\n@given(st.decimals())\ndef test_decimal_Decimal_fma_property_signaling_nan(a):\n    context = getcontext()\n    context.traps[InvalidOperation] = True  # Enable trapping for NaN\n    s_nan = Decimal('sNaN')\n    with st.raises(InvalidOperation):\n        a.fma(s_nan, Decimal('1'), Decimal('1'))\n\n@given(st.decimals())\ndef test_decimal_Decimal_fma_property_quiet_nan(a):\n    n_nan = Decimal('n')\n    result = a.fma(n_nan, Decimal('1'))\n    assert result == a\n\n@given(st.decimals())\ndef test_decimal_Decimal_fma_property_infinity(a):\n    inf = Decimal('Infinity')\n    if a.is_zero():\n        with st.raises(InvalidOperation):\n            a.fma(inf, Decimal('1'), Decimal('1'))\n    else:\n        result = a.fma(inf, Decimal('1'), Decimal('1'))\n        assert result == inf  # Result should be Infinity\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_decimal_Decimal_fma_property_sign_adjustment(a, b, c):\n    if a.is_nan() or b.is_nan() or a.is_infinite() or b.is_infinite():\n        return  # Skip if any operand is NaN or Infinity\n    result = a.fma(b, c)\n    product_sign = (a.sign() ^ b.sign())\n    assert result.sign() == product_sign  # Check if the sign is maintained\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.floats(min_value=-1e100, max_value=1e100), \n       st.floats(min_value=-1e100, max_value=1e100), \n       st.floats(min_value=-1e100, max_value=1e100))\ndef test_decimal_Decimal_fma_basic_arithmetic_property(a, b, c):\n    dec_a = Decimal(a)\n    dec_b = Decimal(b)\n    dec_c = Decimal(c)\n    if not dec_a.is_nan() and not dec_b.is_nan():\n        assert dec_a.fma(dec_b, dec_c) == dec_a * dec_b + dec_c\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals())\ndef test_decimal_Decimal_fma_signaling_nan_property(dec_a, dec_b, dec_c):\n    dec_a = Decimal(dec_a)\n    dec_b = Decimal(dec_b)\n    if dec_a.is_nan() or dec_b.is_nan():\n        with st.raises(InvalidOperation):\n            dec_a.fma(dec_b, dec_c)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals())\ndef test_decimal_Decimal_fma_quiet_nan_property(dec_a, dec_c):\n    dec_a = Decimal(dec_a)\n    if dec_a.is_nan():\n        result = dec_a.fma(Decimal('1.0'), dec_c)\n        assert result == dec_a\n\n@given(st.decimals(allow_nan=False), \n       st.decimals())\ndef test_decimal_Decimal_fma_infinity_property(dec_a, dec_c):\n    dec_a = Decimal(dec_a)\n    if dec_a.is_infinite():\n        with st.raises(InvalidOperation):\n            dec_a.fma(Decimal('0'), dec_c)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals())\ndef test_decimal_Decimal_fma_sign_property(dec_a, dec_b, dec_c):\n    dec_a = Decimal(dec_a)\n    dec_b = Decimal(dec_b)\n    dec_c = Decimal(dec_c)\n    if not dec_a.is_zero() and not dec_b.is_zero():\n        sign_product = (dec_a.sign() ^ dec_b.sign())\n        result = dec_a.fma(dec_b, dec_c)\n        assert result.sign() == sign_product\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(min_value=-1e100, max_value=1e100), \n       st.decimals(min_value=-1e100, max_value=1e100), \n       st.decimals(min_value=-1e100, max_value=1e100))\ndef test_decimal_Decimal_fma_commutative_property(a, b, c):\n    result = a.fma(b, c)\n    expected = a * b + c\n    assert result == expected\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_decimal_Decimal_fma_sNaN_property(a, b, c):\n    if a.is_nan() or b.is_nan():\n        with pytest.raises(InvalidOperation):\n            a.fma(b, c)\n    else:\n        a.fma(b, c)  # No exception should be raised\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_decimal_Decimal_fma_n_property(a, b, c):\n    if a.is_nan() or b.is_nan():\n        result = a.fma(b, c)\n        assert result == (b if a.is_nan() else a)\n    else:\n        a.fma(b, c)  # No special condition should alter the result\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_decimal_Decimal_fma_infinity_property(a, b, c):\n    if a.is_infinite() or b.is_infinite():\n        if a.is_zero() and b.is_infinite():\n            with pytest.raises(InvalidOperation):\n                a.fma(b, c)\n        elif b.is_zero() and a.is_infinite():\n            with pytest.raises(InvalidOperation):\n                a.fma(b, c)\n        else:\n            result = a.fma(b, c)\n            # Check if the result is still infinity or the expected value\n            assert result.is_infinite() or result == (a * b + c)\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_decimal_Decimal_fma_sign_property(a, b, c):\n    if a.is_finite() and b.is_finite() and a != 0 and b != 0:\n        result = a.fma(b, c)\n        sign_product = (a.sign() ^ b.sign())\n        expected_sign = sign_product if (c.is_zero() or c.is_finite()) else None  # Adjust for c being zero\n        assert result.sign() == expected_sign\n\n# End program\n```"], "properties": ["1. The result of `fma(self, other, third)` should be equal to `self * other + third` when neither `self` nor `other` are special values (like NaN or Infinity).\n\n2. If either `self` or `other` is a signaling NaN (sNaN), the function should raise an `InvalidOperation` error.\n\n3. If either `self` or `other` is a quiet NaN (n), the function should return the non-NaN operand.\n\n4. If either `self` or `other` is positive or negative infinity (F), the product should be calculated correctly according to the rules of arithmetic for infinity, and should raise an `InvalidOperation` error if multiplied by zero.\n\n5. The output of `fma(self, other, third)` should maintain the sign of the product of `self` and `other` when both are finite and non-zero, and should be correctly adjusted by the addition of `third`."], "api_doc": "fma(other, third, context=None)\nFused multiply-add. Return self*other+third with no rounding of the intermediate product self*other.\n\n>>>\nDecimal(2).fma(3, 5)\nDecimal('11')", "api_code": "def fma(self, other, third, context=None):\n    if self._is_special or other._is_special:\n        if context is None:\n            context = getcontext()\n        if self._exp == 'N':\n            return context._raise_error(InvalidOperation, 'sNaN', self)\n        if other._exp == 'N':\n            return context._raise_error(InvalidOperation, 'sNaN', other)\n        if self._exp == 'n':\n            product = self\n        elif other._exp == 'n':\n            product = other\n        elif self._exp == 'F':\n            if not other:\n                return context._raise_error(InvalidOperation,\n                                            'INF * 0 in fma')\n            product = _SignedInfinity[self._sign ^ other._sign]\n        elif other._exp == 'F':\n            if not self:\n                return context._raise_error(InvalidOperation,\n                                            '0 * INF in fma')\n            product = _SignedInfinity[self._sign ^ other._sign]\n    else:\n        product = _dec_from_triple(self._sign ^ other._sign,\n                                    str(int(self._int) * int(other._int)),\n                                    self._exp + other._exp)\n\n    return product.__add__(third, context)"}
{"function_name": "pandas.DataFrame.sort_values", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\n\n@given(st.data())\ndef test_output_rows_count_property(data):\n    df = data.draw(st.data_frames(st.floats(), min_rows=0, max_rows=1000))\n    by_column = data.draw(st.one_of(st.sampled_from(df.columns), st.just([])))\n    ascending = data.draw(st.booleans())\n    result = df.sort_values(by=by_column, ascending=ascending)\n    \n    assert result.shape[0] == df.shape[0]\n\n@given(st.data())\ndef test_sorted_output_property(data):\n    df = data.draw(st.data_frames(st.floats(), min_rows=1, max_rows=1000))\n    by_column = data.draw(st.sampled_from(df.columns))\n    ascending = data.draw(st.booleans())\n    result = df.sort_values(by=by_column, ascending=ascending)\n    \n    assert result[by_column].is_monotonic_increasing == ascending\n\n@given(st.data())\ndef test_inplace_modification_property(data):\n    df = data.draw(st.data_frames(st.floats(), min_rows=1, max_rows=1000))\n    original_df = df.copy()\n    by_column = data.draw(st.sampled_from(df.columns))\n    ascending = data.draw(st.booleans())\n    df.sort_values(by=by_column, ascending=ascending, inplace=True)\n    \n    assert df.equals(original_df) is False\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.data_frames(st.floats(), min_rows=1, max_rows=1000))\n    by_column = data.draw(st.sampled_from(df.columns))\n    ascending = data.draw(st.booleans())\n    result = df.sort_values(by=by_column, ascending=ascending, ignore_index=True)\n    \n    assert result.index.equals(pd.RangeIndex(start=0, stop=result.shape[0], step=1))\n\n@given(st.data())\ndef test_empty_by_property(data):\n    df = data.draw(st.data_frames(st.floats(), min_rows=0, max_rows=1000))\n    result = df.sort_values(by=[], ascending=True)\n    \n    assert result.equals(df)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_output_row_count_property(data):\n    df = data.draw(st.data_frames(columns=st.text(), rows=st.integers(min_value=1, max_value=1000)))\n    by_column = df.columns[0] if not df.empty else []\n    result = df.sort_values(by=by_column)\n    assert result.shape[0] == df.shape[0]\n\n@given(st.data())\ndef test_sorted_output_property(data):\n    df = data.draw(st.data_frames(columns=st.text(), rows=st.integers(min_value=1, max_value=1000)))\n    by_column = df.columns[0] if not df.empty else []\n    result = df.sort_values(by=by_column)\n    if not df.empty:\n        assert all(result[by_column].iloc[i] <= result[by_column].iloc[i + 1] for i in range(len(result) - 1))\n\n@given(st.data())\ndef test_inplace_modification_property(data):\n    df = data.draw(st.data_frames(columns=st.text(), rows=st.integers(min_value=1, max_value=1000)))\n    by_column = df.columns[0] if not df.empty else []\n    original_id = id(df)\n    df.sort_values(by=by_column, inplace=True)\n    assert id(df) == original_id\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.data_frames(columns=st.text(), rows=st.integers(min_value=1, max_value=1000)))\n    by_column = df.columns[0] if not df.empty else []\n    result = df.sort_values(by=by_column, ignore_index=True)\n    assert result.index.equals(pd.RangeIndex(start=0, stop=result.shape[0], step=1))\n\n@given(st.data())\ndef test_empty_input_property(data):\n    df = pd.DataFrame()\n    result = df.sort_values(by=[])\n    assert result.equals(df)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_output_row_count_property(data):\n    df = data.draw(st.dataframes(\n        column_format='pandas',\n        columns=st.lists(st.text(), min_size=1),\n        rows=st.lists(st.lists(st.floats()), min_size=1, max_size=1000),  # Limiting for performance\n        max_size=1000  # Limit max size to avoid overflow\n    ))\n    sorted_df = df.sort_values(by=df.columns[0])  # Sort by the first column\n    assert len(sorted_df) == len(df)\n\n@given(st.data())\ndef test_sorted_order_property(data):\n    df = data.draw(st.dataframes(\n        column_format='pandas',\n        columns=st.lists(st.text(), min_size=1),\n        rows=st.lists(st.lists(st.integers()), min_size=1, max_size=1000),  # Limiting for performance\n        max_size=1000  # Limit max size to avoid overflow\n    ))\n    sorted_df = df.sort_values(by=df.columns[0], ascending=True)\n    assert sorted_df[df.columns[0]].is_monotonic_increasing\n\n@given(st.data())\ndef test_inplace_modification_property(data):\n    df = data.draw(st.dataframes(\n        column_format='pandas',\n        columns=st.lists(st.text(), min_size=1),\n        rows=st.lists(st.lists(st.floats()), min_size=1, max_size=1000),  # Limiting for performance\n        max_size=1000  # Limit max size to avoid overflow\n    ))\n    df_copy = df.copy()\n    df.sort_values(by=df.columns[0], inplace=True)\n    assert df.equals(df_copy) is False  # df should be modified\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.dataframes(\n        column_format='pandas',\n        columns=st.lists(st.text(), min_size=1),\n        rows=st.lists(st.lists(st.floats()), min_size=1, max_size=1000),  # Limiting for performance\n        max_size=1000  # Limit max size to avoid overflow\n    ))\n    sorted_df = df.sort_values(by=df.columns[0], ignore_index=True)\n    assert sorted_df.index.equals(pd.RangeIndex(start=0, stop=len(sorted_df), step=1))\n\n@given(st.data())\ndef test_empty_by_parameter_property(data):\n    df = data.draw(st.dataframes(\n        column_format='pandas',\n        columns=st.lists(st.text(), min_size=1),\n        rows=st.lists(st.lists(st.floats()), min_size=1, max_size=1000),  # Limiting for performance\n        max_size=1000  # Limit max size to avoid overflow\n    ))\n    sorted_df = df.sort_values(by=[])\n    assert sorted_df.equals(df)  # Should be identical to original\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\n\n@given(st.data())\ndef test_output_row_count_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), rows=st.integers(min_value=0, max_value=1000)))\n    sorted_df = df.sort_values(by=df.columns[0], ascending=True)\n    assert len(sorted_df) == len(df)\n\n@given(st.data())\ndef test_output_sort_order_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), rows=st.integers(min_value=0, max_value=1000)))\n    sorted_df = df.sort_values(by=df.columns[0], ascending=True)\n    assert sorted_df[df.columns[0]].is_monotonic_increasing\n\n@given(st.data())\ndef test_inplace_modification_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), rows=st.integers(min_value=0, max_value=1000)))\n    original_df = df.copy()\n    df.sort_values(by=df.columns[0], ascending=True, inplace=True)\n    assert df.equals(original_df) is False  # Check that the DataFrame has been modified\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), rows=st.integers(min_value=0, max_value=1000)))\n    sorted_df = df.sort_values(by=df.columns[0], ascending=True, ignore_index=True)\n    assert sorted_df.index.equals(pd.RangeIndex(start=0, stop=len(sorted_df), step=1))\n\n@given(st.data())\ndef test_empty_by_parameter_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), rows=st.integers(min_value=0, max_value=1000)))\n    sorted_df = df.sort_values(by=[], ascending=True)\n    assert sorted_df.equals(df)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_output_has_same_number_of_rows_property(data):\n    df = data.draw(st.dataframes(\n        index=st.range_indexes(),\n        columns=st.sampled_from(['A', 'B', 'C']),\n        shapes=st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=5)),\n        dtype=st.sampled_from([np.int32, np.float64, str])\n    ))\n    by_column = df.columns[0] if len(df.columns) > 0 else ''\n    result = df.sort_values(by=by_column)\n    assert result.shape[0] == df.shape[0]\n\n@given(st.data())\ndef test_sorted_output_based_on_ascending_property(data):\n    df = data.draw(st.dataframes(\n        index=st.range_indexes(),\n        columns=st.sampled_from(['A', 'B', 'C']),\n        shapes=st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=5)),\n        dtype=st.sampled_from([np.int32, np.float64, str])\n    ))\n    by_column = df.columns[0] if len(df.columns) > 0 else ''\n    result = df.sort_values(by=by_column, ascending=True)\n    assert all(result[by_column].iloc[i] <= result[by_column].iloc[i + 1] for i in range(len(result) - 1))\n\n@given(st.data())\ndef test_inplace_modification_property(data):\n    df = data.draw(st.dataframes(\n        index=st.range_indexes(),\n        columns=st.sampled_from(['A', 'B', 'C']),\n        shapes=st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=5)),\n        dtype=st.sampled_from([np.int32, np.float64, str])\n    ))\n    original_df = df.copy()\n    by_column = df.columns[0] if len(df.columns) > 0 else ''\n    df.sort_values(by=by_column, inplace=True)\n    assert df.equals(original_df) is False  # df should be modified\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.dataframes(\n        index=st.range_indexes(),\n        columns=st.sampled_from(['A', 'B', 'C']),\n        shapes=st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=5)),\n        dtype=st.sampled_from([np.int32, np.float64, str])\n    ))\n    by_column = df.columns[0] if len(df.columns) > 0 else ''\n    result = df.sort_values(by=by_column, ignore_index=True)\n    assert result.index.equals(pd.RangeIndex(start=0, stop=len(result), step=1))\n\n@given(st.data())\ndef test_empty_by_parameter_property(data):\n    df = data.draw(st.dataframes(\n        index=st.range_indexes(),\n        columns=st.sampled_from(['A', 'B', 'C']),\n        shapes=st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=5)),\n        dtype=st.sampled_from([np.int32, np.float64, str])\n    ))\n    result = df.sort_values(by=[], inplace=False)\n    assert result.equals(df)\n\n# End program\n```"], "properties": ["1. The output DataFrame should have the same number of rows as the input DataFrame, regardless of the sorting parameters.\n2. The output DataFrame should be sorted based on the specified column(s) in the order defined by the `ascending` parameter, ensuring that all values in the specified column(s) are in the correct order.\n3. If `inplace` is set to `True`, the original DataFrame should be modified, and the function should return `None`.\n4. When `ignore_index` is set to `True`, the resulting DataFrame should have a default integer index ranging from 0 to the number of rows minus one, regardless of the original index.\n5. If the `by` parameter is an empty list, the output should be identical to the input DataFrame, preserving the original order and structure."], "api_doc": "pandas.DataFrame.sort_values\nDataFrame.sort_values(by, *, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]\nSort by the values along either axis.\n\nParameters\n:\nby\nstr or list of str\nName or list of names to sort by.\n\nif axis is 0 or \u2018index\u2019 then by may contain index levels and/or column labels.\n\nif axis is 1 or \u2018columns\u2019 then by may contain column levels and/or index labels.\n\naxis\n\u201c{0 or \u2018index\u2019, 1 or \u2018columns\u2019}\u201d, default 0\nAxis to be sorted.\n\nascending\nbool or list of bool, default True\nSort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.\n\ninplace\nbool, default False\nIf True, perform operation in-place.\n\nkind\n{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.\n\nna_position\n{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\nPuts NaNs at the beginning if first; last puts NaNs at the end.\n\nignore_index\nbool, default False\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nkey\ncallable, optional\nApply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.\n\nReturns\n:\nDataFrame or None\nDataFrame with sorted values or None if inplace=True.\n\nSee also\n\nDataFrame.sort_index\nSort a DataFrame by the index.\n\nSeries.sort_values\nSimilar method for a Series.\n\nExamples\n\ndf = pd.DataFrame({\n    'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n    'col2': [2, 1, 9, 8, 7, 4],\n    'col3': [0, 1, 9, 4, 2, 3],\n    'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n})\ndf\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nSort by col1\n\ndf.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort by multiple columns\n\ndf.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort Descending\n\ndf.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D\nPutting NAs first\n\ndf.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\nSorting with a key function\n\ndf.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nNatural sort with the key argument, using the natsort <https://github.com/SethMMorton/natsort> package.\n\ndf = pd.DataFrame({\n   \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n   \"value\": [10, 20, 30, 40, 50]\n})\ndf\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\nfrom natsort import index_natsorted\ndf.sort_values(\n    by=\"time\",\n    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n)\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20", "api_code": "def sort_values(\n    self,\n    by: IndexLabel,\n    *,\n    axis: Axis = 0,\n    ascending: bool | list[bool] | tuple[bool, ...] = True,\n    inplace: bool = False,\n    kind: SortKind = \"quicksort\",\n    na_position: str = \"last\",\n    ignore_index: bool = False,\n    key: ValueKeyFunc | None = None,\n) -> DataFrame | None:\n    inplace = validate_bool_kwarg(inplace, \"inplace\")\n    axis = self._get_axis_number(axis)\n    ascending = validate_ascending(ascending)\n    if not isinstance(by, list):\n        by = [by]\n    # error: Argument 1 to \"len\" has incompatible type \"Union[bool, List[bool]]\";\n    # expected \"Sized\"\n    if is_sequence(ascending) and (\n        len(by) != len(ascending)  # type: ignore[arg-type]\n    ):\n        # error: Argument 1 to \"len\" has incompatible type \"Union[bool,\n        # List[bool]]\"; expected \"Sized\"\n        raise ValueError(\n            f\"Length of ascending ({len(ascending)})\"  # type: ignore[arg-type]\n            f\" != length of by ({len(by)})\"\n        )\n    if len(by) > 1:\n        keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n\n        # need to rewrap columns in Series to apply key function\n        if key is not None:\n            # error: List comprehension has incompatible type List[Series];\n            # expected List[ndarray]\n            keys = [\n                Series(k, name=name)  # type: ignore[misc]\n                for (k, name) in zip(keys, by)\n            ]\n\n        indexer = lexsort_indexer(\n            keys, orders=ascending, na_position=na_position, key=key\n        )\n    elif len(by):\n        # len(by) == 1\n\n        k = self._get_label_or_level_values(by[0], axis=axis)\n\n        # need to rewrap column in Series to apply key function\n        if key is not None:\n            # error: Incompatible types in assignment (expression has type\n            # \"Series\", variable has type \"ndarray\")\n            k = Series(k, name=by[0])  # type: ignore[assignment]\n\n        if isinstance(ascending, (tuple, list)):\n            ascending = ascending[0]\n\n        indexer = nargsort(\n            k, kind=kind, ascending=ascending, na_position=na_position, key=key\n        )\n    else:\n        if inplace:\n            return self._update_inplace(self)\n        else:\n            return self.copy(deep=None)\n\n    if is_range_indexer(indexer, len(indexer)):\n        result = self.copy(deep=(not inplace and not using_copy_on_write()))\n        if ignore_index:\n            result.index = default_index(len(result))\n\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result\n\n    new_data = self._mgr.take(\n        indexer, axis=self._get_block_manager_axis(axis), verify=False\n    )\n\n    if ignore_index:\n        new_data.set_axis(\n            self._get_block_manager_axis(axis), default_index(len(indexer))\n        )\n\n    result = self._constructor_from_mgr(new_data, axes=new_data.axes)\n    if inplace:\n        return self._update_inplace(result)\n    else:\n        return result.__finalize__(self, method=\"sort_values\")"}
{"function_name": "networkx.algorithms.assortativity.average_neighbor_degree", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n# Property 1: The average neighbor degree for any node should be non-negative.\n@given(st.data())\ndef test_average_neighbor_degree_non_negative_property(data):\n    G = data.draw(st.builds(nx.gnp_random_graph, st.integers(min_value=1, max_value=100), st.floats(min_value=0, max_value=1)))\n    nodes = list(G.nodes())\n    avg_neighbor_degrees = nx.algorithms.assortativity.average_neighbor_degree(G)\n    for node in nodes:\n        assert avg_neighbor_degrees[node] >= 0\n\n# Property 2: If a node has a degree of zero, its average neighbor degree should be exactly zero.\n@given(st.data())\ndef test_average_neighbor_degree_zero_degree_node_property(data):\n    G = nx.Graph()\n    G.add_node(1)  # Adding a single node with no edges\n    avg_neighbor_degrees = nx.algorithms.assortativity.average_neighbor_degree(G)\n    assert avg_neighbor_degrees[1] == 0.0\n\n# Property 3: The average neighbor degree for a node should equal the total degree of its neighbors divided by the node's degree.\n@given(st.data())\ndef test_average_neighbor_degree_correctness_property(data):\n    G = data.draw(st.builds(nx.gnp_random_graph, st.integers(min_value=1, max_value=100), st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degrees = nx.algorithms.assortativity.average_neighbor_degree(G)\n    for node in G.nodes():\n        neighbors = list(G.neighbors(node))\n        if neighbors:\n            total_neighbor_degree = sum(G.degree(neighbor) for neighbor in neighbors)\n            assert avg_neighbor_degrees[node] == total_neighbor_degree / G.degree(node)\n\n# Property 4: In an undirected graph, the average neighbor degree should be symmetric.\n@given(st.data())\ndef test_average_neighbor_degree_symmetry_property(data):\n    G = data.draw(st.builds(nx.gnp_random_graph, st.integers(min_value=1, max_value=100), st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degrees = nx.algorithms.assortativity.average_neighbor_degree(G)\n    for node in G.nodes():\n        for neighbor in G.neighbors(node):\n            assert avg_neighbor_degrees[node] == avg_neighbor_degrees[neighbor]\n\n# Property 5: The average neighbor degree should be invariant under the addition of isolated nodes.\n@given(st.data())\ndef test_average_neighbor_degree_isolated_nodes_property(data):\n    G = data.draw(st.builds(nx.gnp_random_graph, st.integers(min_value=1, max_value=100), st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degrees_before = nx.algorithms.assortativity.average_neighbor_degree(G)\n    G.add_node(1001)  # Adding an isolated node\n    avg_neighbor_degrees_after = nx.algorithms.assortativity.average_neighbor_degree(G)\n    assert avg_neighbor_degrees_before == avg_neighbor_degrees_after\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_average_neighbor_degree_non_negative_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    degrees = nx.algorithms.assortativity.average_neighbor_degree(G)\n    for avg in degrees.values():\n        assert avg >= 0\n\n@given(st.data())\ndef test_average_neighbor_degree_zero_degree_node_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    G.add_node(0)  # Add a node with zero degree\n    degrees = nx.algorithms.assortativity.average_neighbor_degree(G)\n    assert degrees.get(0, 0) == 0.0\n\n@given(st.data())\ndef test_average_neighbor_degree_correctness_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    for node in G.nodes:\n        avg = nx.algorithms.assortativity.average_neighbor_degree(G)[node]\n        neighbors = list(G.neighbors(node))\n        if len(neighbors) > 0:\n            total_degree = sum(G.degree[nbr] for nbr in neighbors)\n            assert avg == total_degree / G.degree[node]\n\n@given(st.data())\ndef test_average_neighbor_degree_symmetry_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=2)))\n    for nodeA in G.nodes:\n        for nodeB in G.neighbors(nodeA):\n            avgA = nx.algorithms.assortativity.average_neighbor_degree(G)[nodeA]\n            avgB = nx.algorithms.assortativity.average_neighbor_degree(G)[nodeB]\n            assert avgA == avgB\n\n@given(st.data())\ndef test_average_neighbor_degree_invariance_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    original_degrees = nx.algorithms.assortativity.average_neighbor_degree(G)\n    G.add_node(1000)  # Add an isolated node\n    updated_degrees = nx.algorithms.assortativity.average_neighbor_degree(G)\n    for node in original_degrees.keys():\n        assert updated_degrees[node] == original_degrees[node]\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_average_neighbor_degree_non_negative_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100),\n                             p=st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degree = nx.algorithms.assortativity.average_neighbor_degree(G)\n    \n    for degree in avg_neighbor_degree.values():\n        assert degree >= 0\n\n@given(st.data())\ndef test_average_neighbor_degree_zero_degree_property(data):\n    G = nx.Graph()\n    G.add_node(1)  # Adding a single node\n    avg_neighbor_degree = nx.algorithms.assortativity.average_neighbor_degree(G, nodes=[1])\n    \n    assert avg_neighbor_degree[1] == 0.0\n\n@given(st.data())\ndef test_average_neighbor_degree_correctness_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100),\n                             p=st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degree = nx.algorithms.assortativity.average_neighbor_degree(G)\n    \n    for n, deg in G.degree():\n        if deg > 0:\n            neighbors = list(G.neighbors(n))\n            total_neighbor_degree = sum(G.degree(neighbor) for neighbor in neighbors)\n            assert avg_neighbor_degree[n] == total_neighbor_degree / deg\n\n@given(st.data())\ndef test_average_neighbor_degree_symmetric_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100),\n                             p=st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degree = nx.algorithms.assortativity.average_neighbor_degree(G)\n    \n    for n in G.nodes():\n        for neighbor in G.neighbors(n):\n            assert avg_neighbor_degree[n] == avg_neighbor_degree[neighbor]\n\n@given(st.data())\ndef test_average_neighbor_degree_isolated_nodes_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100),\n                             p=st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degree_before = nx.algorithms.assortativity.average_neighbor_degree(G)\n    \n    G.add_node('isolated_node')  # Adding an isolated node\n    avg_neighbor_degree_after = nx.algorithms.assortativity.average_neighbor_degree(G)\n    \n    assert avg_neighbor_degree_before == avg_neighbor_degree_after\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n# Property 1: Average neighbor degree should be non-negative.\n@given(st.data())\ndef test_average_neighbor_degree_non_negative():\n    G = st.random_graphs.graphs()\n    nodes = list(G.nodes())\n    for node in nodes:\n        avg_degree = nx.algorithms.assortativity.average_neighbor_degree(G, nodes=[node])\n        assert avg_degree[node] >= 0\n\n# Property 2: Degree zero nodes should have an average neighbor degree of zero.\n@given(st.data())\ndef test_average_neighbor_degree_zero_degree_nodes():\n    G = st.random_graphs.graphs()\n    isolated_nodes = [node for node in G.nodes() if G.degree(node) == 0]\n    for node in isolated_nodes:\n        avg_degree = nx.algorithms.assortativity.average_neighbor_degree(G, nodes=[node])\n        assert avg_degree[node] == 0.0\n\n# Property 3: Average neighbor degree should equal total degree of neighbors divided by the node's degree.\n@given(st.data())\ndef test_average_neighbor_degree_correctness():\n    G = st.random_graphs.graphs()\n    for node in G.nodes():\n        avg_degree = nx.algorithms.assortativity.average_neighbor_degree(G, nodes=[node])\n        neighbor_degrees = [G.degree(nbr) for nbr in G.neighbors(node)]\n        if G.degree(node) > 0:\n            expected_avg = sum(neighbor_degrees) / G.degree(node)\n            assert avg_degree[node] == expected_avg\n\n# Property 4: Average neighbor degree should be symmetric in undirected graphs.\n@given(st.data())\ndef test_average_neighbor_degree_symmetry():\n    G = st.random_graphs.graphs(directed=False)\n    for node_a in G.nodes():\n        for node_b in G.neighbors(node_a):\n            avg_degree_a = nx.algorithms.assortativity.average_neighbor_degree(G, nodes=[node_a])\n            avg_degree_b = nx.algorithms.assortativity.average_neighbor_degree(G, nodes=[node_b])\n            assert avg_degree_a[node_b] == avg_degree_b[node_a]\n\n# Property 5: Adding isolated nodes should not affect average neighbor degrees of existing nodes.\n@given(st.data())\ndef test_average_neighbor_degree_isolated_nodes_invariance():\n    G = st.random_graphs.graphs()\n    original_avg = nx.algorithms.assortativity.average_neighbor_degree(G)\n    isolated_node = G.add_node(\"isolated\")\n    new_avg = nx.algorithms.assortativity.average_neighbor_degree(G)\n    for node in G.nodes():\n        assert original_avg[node] == new_avg[node]\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n# Property 1: The average neighbor degree for any node should be non-negative.\n@given(st.lists(st.tuples(st.integers(min_value=0), st.integers(min_value=0)), min_size=1))\ndef test_average_neighbor_degree_non_negative_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    avg_neighbor_deg = nx.algorithms.assortativity.average_neighbor_degree(G)\n    for deg in avg_neighbor_deg.values():\n        assert deg >= 0\n\n# Property 2: If a node has a degree of zero, its average neighbor degree should be exactly zero.\n@given(st.lists(st.tuples(st.integers(min_value=0), st.integers(min_value=0)), min_size=1),\n                st.integers(min_value=0, max_value=10))\ndef test_average_neighbor_degree_zero_degree_property(edges, isolated_node):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    G.add_node(isolated_node)\n    avg_neighbor_deg = nx.algorithms.assortativity.average_neighbor_degree(G)\n    assert avg_neighbor_deg[isolated_node] == 0.0\n\n# Property 3: The average neighbor degree should equal the total degree of its neighbors divided by the node's degree.\n@given(st.lists(st.tuples(st.integers(min_value=0), st.integers(min_value=0)), min_size=1))\ndef test_average_neighbor_degree_correctness_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    avg_neighbor_deg = nx.algorithms.assortativity.average_neighbor_degree(G)\n\n    for node in G.nodes():\n        if G.degree(node) > 0:\n            neighbor_degrees = [G.degree(neighbor) for neighbor in G.neighbors(node)]\n            expected_avg = sum(neighbor_degrees) / G.degree(node)\n            assert avg_neighbor_deg[node] == expected_avg\n\n# Property 4: In an undirected graph, the average neighbor degree should be symmetric.\n@given(st.lists(st.tuples(st.integers(min_value=0), st.integers(min_value=0)), min_size=1))\ndef test_average_neighbor_degree_symmetry_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    avg_neighbor_deg = nx.algorithms.assortativity.average_neighbor_degree(G)\n\n    for node in G.nodes():\n        for neighbor in G.neighbors(node):\n            assert avg_neighbor_deg[node] == avg_neighbor_deg[neighbor]\n\n# Property 5: The average neighbor degree should be invariant under the addition of nodes with no edges.\n@given(st.lists(st.tuples(st.integers(min_value=0), st.integers(min_value=0)), min_size=1),\n                st.integers(min_value=0, max_value=10))\ndef test_average_neighbor_degree_invariance_property(edges, isolated_node):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    avg_neighbor_deg_before = nx.algorithms.assortativity.average_neighbor_degree(G)\n\n    G.add_node(isolated_node)\n    avg_neighbor_deg_after = nx.algorithms.assortativity.average_neighbor_degree(G)\n\n    for node in G.nodes():\n        assert avg_neighbor_deg_before[node] == avg_neighbor_deg_after[node]\n\n# End program\n```"], "properties": ["1. The average neighbor degree for any node should be non-negative, as it represents an average of degrees of neighboring nodes.\n\n2. If a node has a degree of zero, its average neighbor degree should be exactly zero, since it has no neighbors.\n\n3. The average neighbor degree for a node should be equal to the total degree of its neighbors divided by the node's degree, ensuring that the calculation correctly reflects the average.\n\n4. In an undirected graph, the average neighbor degree should be symmetric; that is, the average neighbor degree of node A should equal that of node B if A and B are neighbors.\n\n5. The average neighbor degree should be invariant under the addition of nodes with no edges, meaning that adding isolated nodes to the graph should not affect the average neighbor degrees of existing nodes."], "api_doc": "average_neighbor_degree\naverage_neighbor_degree(G, source='out', target='out', nodes=None, weight=None)[source]\nReturns the average degree of the neighborhood of each node.\n\nIn an undirected graph, the neighborhood N(i) of node i contains the nodes that are connected to i by an edge.\n\nFor directed graphs, N(i) is defined according to the parameter source:\n\nif source is \u2018in\u2019, then N(i) consists of predecessors of node i.\n\nif source is \u2018out\u2019, then N(i) consists of successors of node i.\n\nif source is \u2018in+out\u2019, then N(i) is both predecessors and successors.\n\nThe average neighborhood degree of a node i is\n\n \n \nwhere N(i) are the neighbors of node i and k_j is the degree of node j which belongs to N(i). For weighted graphs, an analogous measure can be defined [1],\n\n \n \nwhere s_i is the weighted degree of node i, w_{ij} is the weight of the edge that links i and j and N(i) are the neighbors of node i.\n\nParameters\n:\nG\nNetworkX graph\nsource\nstring (\u201cin\u201d|\u201dout\u201d|\u201din+out\u201d), optional (default=\u201dout\u201d)\nDirected graphs only. Use \u201cin\u201d- or \u201cout\u201d-neighbors of source node.\n\ntarget\nstring (\u201cin\u201d|\u201dout\u201d|\u201din+out\u201d), optional (default=\u201dout\u201d)\nDirected graphs only. Use \u201cin\u201d- or \u201cout\u201d-degree for target node.\n\nnodes\nlist or iterable, optional (default=G.nodes)\nCompute neighbor degree only for specified nodes.\n\nweight\nstring or None, optional (default=None)\nThe edge attribute that holds the numerical value used as a weight. If None, then each edge has weight 1.\n\nReturns\n:\nd: dict\nA dictionary keyed by node to the average degree of its neighbors.\n\nRaises\n:\nNetworkXError\nIf either source or target are not one of \u2018in\u2019, \u2018out\u2019, or \u2018in+out\u2019. If either source or target is passed for an undirected graph.\n\nSee also\n\naverage_degree_connectivity\nReferences\n\n[1]\nA. Barrat, M. Barth\u00e9lemy, R. Pastor-Satorras, and A. Vespignani, \u201cThe architecture of complex weighted networks\u201d. PNAS 101 (11): 3747\u20133752 (2004).\n\nExamples\n\nG = nx.path_graph(4)\nG.edges[0, 1][\"weight\"] = 5\nG.edges[2, 3][\"weight\"] = 3\nnx.average_neighbor_degree(G)\n{0: 2.0, 1: 1.5, 2: 1.5, 3: 2.0}\nnx.average_neighbor_degree(G, weight=\"weight\")\n{0: 2.0, 1: 1.1666666666666667, 2: 1.25, 3: 2.0}\nG = nx.DiGraph()\nnx.add_path(G, [0, 1, 2, 3])\nnx.average_neighbor_degree(G, source=\"in\", target=\"in\")\n{0: 0.0, 1: 0.0, 2: 1.0, 3: 1.0}\nnx.average_neighbor_degree(G, source=\"out\", target=\"out\")\n{0: 1.0, 1: 1.0, 2: 0.0, 3: 0.0}", "api_code": "@nx._dispatchable(edge_attrs=\"weight\")\ndef average_neighbor_degree(G, source=\"out\", target=\"out\", nodes=None, weight=None):\n    if G.is_directed():\n        if source == \"in\":\n            source_degree = G.in_degree\n        elif source == \"out\":\n            source_degree = G.out_degree\n        elif source == \"in+out\":\n            source_degree = G.degree\n        else:\n            raise nx.NetworkXError(\n                f\"source argument {source} must be 'in', 'out' or 'in+out'\"\n            )\n\n        if target == \"in\":\n            target_degree = G.in_degree\n        elif target == \"out\":\n            target_degree = G.out_degree\n        elif target == \"in+out\":\n            target_degree = G.degree\n        else:\n            raise nx.NetworkXError(\n                f\"target argument {target} must be 'in', 'out' or 'in+out'\"\n            )\n    else:\n        if source != \"out\" or target != \"out\":\n            raise nx.NetworkXError(\n                f\"source and target arguments are only supported for directed graphs\"\n            )\n        source_degree = target_degree = G.degree\n\n    # precompute target degrees -- should *not* be weighted degree\n    t_deg = dict(target_degree())\n\n    # Set up both predecessor and successor neighbor dicts leaving empty if not needed\n    G_P = G_S = {n: {} for n in G}\n    if G.is_directed():\n        # \"in\" or \"in+out\" cases: G_P contains predecessors\n        if \"in\" in source:\n            G_P = G.pred\n        # \"out\" or \"in+out\" cases: G_S contains successors\n        if \"out\" in source:\n            G_S = G.succ\n    else:\n        # undirected leave G_P empty but G_S is the adjacency\n        G_S = G.adj\n\n    # Main loop: Compute average degree of neighbors\n    avg = {}\n    for n, deg in source_degree(nodes, weight=weight):\n        # handle degree zero average\n        if deg == 0:\n            avg[n] = 0.0\n            continue\n\n        # we sum over both G_P and G_S, but one of the two is usually empty.\n        if weight is None:\n            avg[n] = (\n                sum(t_deg[nbr] for nbr in G_S[n]) + sum(t_deg[nbr] for nbr in G_P[n])\n            ) / deg\n        else:\n            avg[n] = (\n                sum(dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_S[n].items())\n                + sum(dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_P[n].items())\n            ) / deg\n    return avg"}
{"function_name": "networkx.algorithms.dag.is_aperiodic", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.dag import is_aperiodic\n\n@given(st.data())\ndef test_is_aperiodic_undirected_graph_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    with pytest.raises(nx.NetworkXError):\n        is_aperiodic(G)\n\n@given(st.data())\ndef test_is_aperiodic_empty_graph_property(data):\n    G = nx.DiGraph()\n    with pytest.raises(nx.NetworkXPointlessConcept):\n        is_aperiodic(G)\n\n@given(st.data())\ndef test_is_aperiodic_single_scc_property(data):\n    G = nx.DiGraph()\n    nodes = data.draw(st.lists(st.integers(), min_size=1))\n    G.add_nodes_from(nodes)\n    G.add_edges_from(data.draw(st.lists(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes)), min_size=1)))\n    assert is_aperiodic(G) == True  # Assuming this specific input is aperiodic\n\n@given(st.data())\ndef test_is_aperiodic_multiple_scc_property(data):\n    G = nx.DiGraph()\n    nodes = data.draw(st.lists(st.integers(), min_size=1))\n    G.add_nodes_from(nodes)\n    G.add_edges_from(data.draw(st.lists(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes)), min_size=1)))\n    # Manually create a periodic component\n    G.add_edge(nodes[0], nodes[0])  # Adding a self-loop\n    assert is_aperiodic(G) == False\n\n@given(st.data())\ndef test_is_aperiodic_graph_with_cycles_property(data):\n    G = nx.DiGraph()\n    nodes = data.draw(st.lists(st.integers(), min_size=1))\n    G.add_nodes_from(nodes)\n    edges = data.draw(st.lists(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes)), min_size=1))\n    G.add_edges_from(edges)\n    # Ensure at least one cycle\n    G.add_edge(nodes[0], nodes[0])\n    assert is_aperiodic(G) == False\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx import algorithms\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_is_aperiodic_undirected_graph_property(nodes):\n    G = nx.Graph()  # Create an undirected graph\n    G.add_nodes_from(nodes)\n    with pytest.raises(nx.NetworkXError):\n        algorithms.dag.is_aperiodic(G)\n\n@given(st.lists(st.integers(), min_size=0))\ndef test_is_aperiodic_empty_graph_property(nodes):\n    G = nx.DiGraph()  # Create a directed graph\n    G.add_nodes_from(nodes)\n    if len(nodes) == 0:\n        with pytest.raises(nx.NetworkXPointlessConcept):\n            algorithms.dag.is_aperiodic(G)\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_is_aperiodic_single_scc_property(nodes):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    G.add_edges_from((nodes[i], nodes[(i + 1) % len(nodes)]) for i in range(len(nodes)))  # Create a cycle\n    assert algorithms.dag.is_aperiodic(G) is True\n\n@given(st.lists(st.integers(), min_size=2))\ndef test_is_aperiodic_multiple_scc_property(nodes):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    G.add_edge(nodes[0], nodes[1])\n    G.add_edge(nodes[1], nodes[0])\n    G.add_edge(nodes[0], max(nodes) + 1)  # Create a separate component\n    assert algorithms.dag.is_aperiodic(G) is False\n\n@given(st.lists(st.integers(), min_size=2))\ndef test_is_aperiodic_with_cycles_property(nodes):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    G.add_edges_from((nodes[i], nodes[i]) for i in range(len(nodes)))  # Create self-loops\n    assert algorithms.dag.is_aperiodic(G) is False\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_is_aperiodic_raises_for_undirected_graph_property(edges):\n    G = nx.Graph(edges)\n    with pytest.raises(nx.NetworkXError):\n        nx.algorithms.dag.is_aperiodic(G)\n\n@given(st.lists(st.integers(), min_size=0, max_size=100))\ndef test_is_aperiodic_raises_for_empty_graph_property(nodes):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    if len(nodes) == 0:\n        with pytest.raises(nx.NetworkXPointlessConcept):\n            nx.algorithms.dag.is_aperiodic(G)\n\n@given(st.lists(st.integers(), min_size=1, max_size=100))\ndef test_is_aperiodic_returns_true_for_single_scc_property(nodes):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    for i in range(len(nodes) - 1):\n        G.add_edge(nodes[i], nodes[i + 1])\n    G.add_edge(nodes[-1], nodes[0])  # create a cycle\n    assert nx.algorithms.dag.is_aperiodic(G) == True\n\n@given(st.lists(st.integers(), min_size=1, max_size=100))\ndef test_is_aperiodic_returns_false_for_multiple_scc_property(nodes):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    for i in range(len(nodes) - 1):\n        G.add_edge(nodes[i], nodes[i + 1])\n    G.add_edge(nodes[-1], nodes[0])  # create a cycle\n    G.add_edge(nodes[0], nodes[-1])  # introduce another edge to create a second component\n    assert nx.algorithms.dag.is_aperiodic(G) == False\n\n@given(st.lists(st.integers(), min_size=1, max_size=100))\ndef test_is_aperiodic_returns_false_for_graph_with_cycles_property(nodes):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    for i in range(len(nodes)):\n        G.add_edge(nodes[i], nodes[i])  # create self-loops\n    assert nx.algorithms.dag.is_aperiodic(G) == False\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_is_aperiodic_raises_error_for_undirected_graph():\n    G = nx.Graph(st.data().draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    with pytest.raises(nx.NetworkXError):\n        nx.algorithms.dag.is_aperiodic(G)\n\n@given(st.data())\ndef test_is_aperiodic_raises_error_for_empty_graph():\n    G = nx.DiGraph()  # Create an empty directed graph\n    with pytest.raises(nx.NetworkXPointlessConcept):\n        nx.algorithms.dag.is_aperiodic(G)\n\n@given(st.data())\ndef test_is_aperiodic_returns_true_for_single_scc_dag():\n    G = nx.DiGraph()\n    G.add_edges_from([(0, 1), (1, 2), (2, 0)])  # A simple cycle\n    assert nx.algorithms.dag.is_aperiodic(G) is False  # Correcting the test case\n\n@given(st.data())\ndef test_is_aperiodic_returns_false_for_multiple_scc_dag():\n    G = nx.DiGraph()\n    G.add_edges_from([(0, 1), (1, 2)])  # First component\n    G.add_edges_from([(3, 4), (4, 3)])  # Second component with a cycle\n    assert nx.algorithms.dag.is_aperiodic(G) is False\n\n@given(st.data())\ndef test_is_aperiodic_returns_false_for_graph_with_cycles():\n    G = nx.DiGraph()\n    G.add_edges_from([(0, 1), (1, 0)])  # A cycle\n    assert nx.algorithms.dag.is_aperiodic(G) is False\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom hypothesis import HealthCheck, settings\n\n# Disable certain health checks to allow for larger graphs\nsettings.register_profile(\"large\", max_examples=10, suppress_health_check=[HealthCheck.too_slow])\nsettings.load_profile(\"large\")\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=1000))\ndef test_is_aperiodic_raises_for_undirected_graph_property(edges):\n    G = nx.Graph(edges)  # Create an undirected graph\n    try:\n        nx.algorithms.dag.is_aperiodic(G)\n        assert False, \"Expected NetworkXError for undirected graph\"\n    except nx.NetworkXError:\n        pass  # Correct behavior\n\n@given(st.lists(st.integers(), min_size=0, max_size=1000))\ndef test_is_aperiodic_raises_for_empty_graph_property(nodes):\n    G = nx.DiGraph()  # Create a directed graph\n    G.add_nodes_from(nodes)  # Add nodes\n    if len(nodes) == 0:\n        try:\n            nx.algorithms.dag.is_aperiodic(G)\n            assert False, \"Expected NetworkXPointlessConcept for empty graph\"\n        except nx.NetworkXPointlessConcept:\n            pass  # Correct behavior\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_is_aperiodic_true_for_single_scc_property(nodes, edges):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    G.add_edges_from(edges)\n    assert nx.algorithms.dag.is_aperiodic(G) in [True, False]  # Ensure it returns a boolean\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_is_aperiodic_false_for_multiple_scc_property(nodes, edges):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    G.add_edges_from(edges)\n    # Add an edge to create a periodic component\n    G.add_edge(nodes[0], nodes[0])  # Self-loop creates a cycle\n    assert not nx.algorithms.dag.is_aperiodic(G)  # Should return False\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_is_aperiodic_false_for_graph_with_cycles_property(nodes, edges):\n    G = nx.DiGraph()\n    G.add_nodes_from(nodes)\n    G.add_edges_from(edges)\n    # Create a cycle\n    if len(nodes) > 1:\n        G.add_edge(nodes[0], nodes[1])\n        G.add_edge(nodes[1], nodes[0])  # Create a cycle\n    assert not nx.algorithms.dag.is_aperiodic(G)  # Should return False\n# End program\n```"], "properties": ["1. If the input graph is undirected, the function should raise a `NetworkXError` indicating that \"is_aperiodic not defined for undirected graphs.\"\n\n2. If the input graph has no nodes, the function should raise a `NetworkXPointlessConcept` indicating that \"Graph has no nodes.\"\n\n3. If the input graph is a directed acyclic graph (DAG) with a single strongly connected component, the output should be `True` indicating that the graph is aperiodic.\n\n4. If the input graph is a directed graph with more than one strongly connected component, the output should be `False` if any of the components are periodic.\n\n5. For any directed graph that contains cycles, the output should be `False`, indicating that the graph is not aperiodic."], "api_doc": "is_aperiodic\nis_aperiodic(G)[source]\nReturns True if G is aperiodic.\n\nA directed graph is aperiodic if there is no integer k > 1 that divides the length of every cycle in the graph.\n\nParameters\n:\nG\nNetworkX DiGraph\nA directed graph\n\nReturns\n:\nbool\nTrue if the graph is aperiodic False otherwise\n\nRaises\n:\nNetworkXError\nIf G is not directed\n\nNotes\n\nThis uses the method outlined in [1], which runs in \n time given \n edges in G. Note that a graph is not aperiodic if it is acyclic as every integer trivial divides length 0 cycles.\n\nReferences\n\n[1]\nJarvis, J. P.; Shier, D. R. (1996), \u201cGraph-theoretic analysis of finite Markov chains,\u201d in Shier, D. R.; Wallenius, K. T., Applied Mathematical Modeling: A Multidisciplinary Approach, CRC Press.\n\nExamples\n\nA graph consisting of one cycle, the length of which is 2. Therefore k = 2 divides the length of every cycle in the graph and thus the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 1)])\nnx.is_aperiodic(DG)\nFalse\nA graph consisting of two cycles: one of length 2 and the other of length 3. The cycle lengths are coprime, so there is no single value of k where k > 1 that divides each cycle length and therefore the graph is aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 3), (3, 1), (1, 4), (4, 1)])\nnx.is_aperiodic(DG)\nTrue\nA graph consisting of two cycles: one of length 2 and the other of length 4. The lengths of the cycles share a common factor k = 2, and therefore the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 1), (3, 4), (4, 5), (5, 6), (6, 3)])\nnx.is_aperiodic(DG)\nFalse\nAn acyclic graph, therefore the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 3)])\nnx.is_aperiodic(DG)\nFalse", "api_code": "@nx._dispatchable\ndef is_aperiodic(G):\n    if not G.is_directed():\n        raise nx.NetworkXError(\"is_aperiodic not defined for undirected graphs\")\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept(\"Graph has no nodes.\")\n    s = arbitrary_element(G)\n    levels = {s: 0}\n    this_level = [s]\n    g = 0\n    lev = 1\n    while this_level:\n        next_level = []\n        for u in this_level:\n            for v in G[u]:\n                if v in levels:  # Non-Tree Edge\n                    g = gcd(g, levels[u] - levels[v] + 1)\n                else:  # Tree Edge\n                    next_level.append(v)\n                    levels[v] = lev\n        this_level = next_level\n        lev += 1\n    if len(levels) == len(G):  # All nodes in tree\n        return g == 1\n    else:\n        return g == 1 and nx.is_aperiodic(G.subgraph(set(G) - set(levels)))"}
{"function_name": "statistics.geometric_mean", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport math\nfrom statistics import geometric_mean, StatisticsError\n\n@given(st.lists(st.floats(min_value=0, allow_nan=True, allow_infinity=False), min_size=1))\ndef test_geometric_mean_positive_numbers_property(data):\n    # Filter out NaN values and ensure we are only dealing with positive numbers\n    data = [x for x in data if x > 0]\n    if data:\n        result = geometric_mean(data)\n        assert result > 0\n\n@given(st.lists(st.floats(min_value=0, max_value=0, allow_nan=False), min_size=1))\ndef test_geometric_mean_zero_property(data):\n    result = geometric_mean(data)\n    assert result == 0.0\n\n@given(st.lists(st.floats(), min_size=0))\ndef test_geometric_mean_empty_dataset_property(data):\n    try:\n        geometric_mean(data)\n        assert False  # Should not reach this line\n    except StatisticsError as e:\n        assert str(e) == 'Must have a non-empty dataset'\n\n@given(st.lists(st.floats(allow_nan=True), min_size=1))\ndef test_geometric_mean_nan_in_dataset_property(data):\n    data.append(float('nan'))  # Ensure there's at least one NaN\n    result = geometric_mean(data)\n    assert math.isnan(result)\n\n@given(st.lists(st.floats(min_value=float('-inf'), max_value=-1), min_size=1))\ndef test_geometric_mean_negative_numbers_property(data):\n    try:\n        geometric_mean(data)\n        assert False  # Should not reach this line\n    except StatisticsError as e:\n        assert str(e) == 'No negative inputs allowed'\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport math\nfrom statistics import StatisticsError, geometric_mean\n\n@given(st.lists(st.floats(min_value=0.0, allow_nan=True), min_size=1))\ndef test_geometric_mean_positive_numbers_property(data):\n    positive_data = [x for x in data if x > 0]\n    if positive_data:\n        result = geometric_mean(positive_data)\n        assert result > 0\n\n@given(st.lists(st.floats(min_value=0.0, allow_nan=True), min_size=1))\ndef test_geometric_mean_zero_in_data_property(data):\n    if 0.0 in data:\n        result = geometric_mean(data)\n        assert result == 0.0\n\n@given(st.lists(st.floats(), min_size=0))\ndef test_geometric_mean_empty_data_property(data):\n    if len(data) == 0:\n        with st.raises(StatisticsError):\n            geometric_mean(data)\n\n@given(st.lists(st.floats(allow_nan=True), min_size=1))\ndef test_geometric_mean_nan_in_data_property(data):\n    if any(math.isnan(x) for x in data):\n        result = geometric_mean(data)\n        assert math.isnan(result)\n\n@given(st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=1))\ndef test_geometric_mean_negative_numbers_property(data):\n    negative_data = [x for x in data if x < 0]\n    if negative_data:\n        with st.raises(StatisticsError):\n            geometric_mean(data)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport math\nfrom statistics import geometric_mean, StatisticsError\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False, exclude_min=True), min_size=1))\ndef test_geometric_mean_positive_numbers_property(data):\n    result = geometric_mean(data)\n    assert result > 0\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False, exclude_min=True), min_size=0))\ndef test_geometric_mean_zero_property(data):\n    if 0.0 in data:\n        result = geometric_mean(data)\n        assert result == 0.0\n    else:\n        result = geometric_mean(data)\n        assert isinstance(result, float)\n\n@given(st.lists(st.floats(allow_nan=False), min_size=0))\ndef test_geometric_mean_empty_dataset_property(data):\n    if not data:\n        try:\n            geometric_mean(data)\n            assert False, \"Expected StatisticsError for empty dataset\"\n        except StatisticsError:\n            pass\n\n@given(st.lists(st.one_of(st.floats(allow_nan=False, min_value=-1e10, max_value=-1e-1), st.floats(allow_nan=True)), min_size=1))\ndef test_geometric_mean_nan_values_property(data):\n    if any(math.isnan(x) for x in data):\n        result = geometric_mean(data)\n        assert math.isnan(result)\n    else:\n        result = geometric_mean(data)\n        assert isinstance(result, float)\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=-1e-1), min_size=1))\ndef test_geometric_mean_negative_numbers_property(data):\n    try:\n        geometric_mean(data)\n        assert False, \"Expected StatisticsError for negative inputs\"\n    except StatisticsError:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport math\nfrom statistics import geometric_mean, StatisticsError\n\n@given(st.lists(st.floats(min_value=0.0, allow_nan=False), min_size=1))\ndef test_geometric_mean_positive_property(data):\n    assert geometric_mean(data) > 0\n\n@given(st.lists(st.floats(min_value=0.0, allow_nan=False), min_size=0))\ndef test_geometric_mean_zero_property(data):\n    assert geometric_mean(data) == 0.0\n\n@given(st.lists(st.floats()))\ndef test_geometric_mean_empty_dataset_property(data):\n    if not data:\n        try:\n            geometric_mean(data)\n            assert False, \"Expected StatisticsError for empty dataset\"\n        except StatisticsError:\n            pass\n\n@given(st.lists(st.one_of(st.floats(allow_nan=True), st.floats(min_value=0.0, allow_nan=False)), min_size=1))\ndef test_geometric_mean_nan_property(data):\n    if any(math.isnan(x) for x in data):\n        assert math.isnan(geometric_mean(data))\n\n@given(st.lists(st.floats(min_value=-math.inf, max_value=-1.0), min_size=1))\ndef test_geometric_mean_negative_input_property(data):\n    try:\n        geometric_mean(data)\n        assert False, \"Expected StatisticsError for negative inputs\"\n    except StatisticsError:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport math\nfrom statistics import geometric_mean, StatisticsError\n\n@given(st.lists(st.floats(allow_nan=False, min_value=0.1, max_value=1e10), min_size=1))\ndef test_geometric_mean_positive_numbers_property(data):\n    result = geometric_mean(data)\n    assert result > 0\n\n@given(st.lists(st.floats(allow_nan=False, min_value=0.0, max_value=1e10), min_size=1))\ndef test_geometric_mean_zero_included_property(data):\n    if 0.0 in data:\n        result = geometric_mean(data)\n        assert result == 0.0\n\n@given(st.lists(st.floats(allow_nan=False, min_value=-1e10, max_value=1e10), min_size=0))\ndef test_geometric_mean_empty_dataset_property(data):\n    if len(data) == 0:\n        with pytest.raises(StatisticsError):\n            geometric_mean(data)\n\n@given(st.lists(st.floats(allow_nan=True, min_value=-1e10, max_value=1e10), min_size=1))\ndef test_geometric_mean_nan_included_property(data):\n    if any(math.isnan(x) for x in data):\n        result = geometric_mean(data)\n        assert math.isnan(result)\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=0.0), min_size=1))\ndef test_geometric_mean_negative_numbers_property(data):\n    with pytest.raises(StatisticsError):\n        geometric_mean(data)\n# End program\n```"], "properties": ["1. The geometric mean of a dataset containing only positive numbers should always be greater than zero.\n\n2. If the dataset contains a zero, the geometric mean should return zero.\n\n3. If the dataset is empty, the function should raise a `StatisticsError` indicating that a non-empty dataset is required.\n\n4. The geometric mean should return `NaN` if the dataset contains any `NaN` values.\n\n5. The geometric mean of a dataset containing negative numbers should raise a `StatisticsError`, indicating that negative inputs are not allowed."], "api_doc": "statistics.geometric_mean(data)\nConvert data to floats and compute the geometric mean.\n\nThe geometric mean indicates the central tendency or typical value of the data using the product of the values (as opposed to the arithmetic mean which uses their sum).\n\nRaises a StatisticsError if the input dataset is empty, if it contains a zero, or if it contains a negative value. The data may be a sequence or iterable.\n\nNo special efforts are made to achieve exact results. (However, this may change in the future.)\n\n>>>\nround(geometric_mean([54, 24, 36]), 1)\n36.0\nAdded in version 3.8.", "api_code": "def geometric_mean(data):\n    n = 0\n    found_zero = False\n    def count_positive(iterable):\n        nonlocal n, found_zero\n        for n, x in enumerate(iterable, start=1):\n            if x > 0.0 or math.isnan(x):\n                yield x\n            elif x == 0.0:\n                found_zero = True\n            else:\n                raise StatisticsError('No negative inputs allowed', x)\n    total = fsum(map(log, count_positive(data)))\n    if not n:\n        raise StatisticsError('Must have a non-empty dataset')\n    if math.isnan(total):\n        return math.nan\n    if found_zero:\n        return math.nan if total == math.inf else 0.0\n    return exp(total / n)"}
{"function_name": "datetime.datetime.fromisoformat", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n# Property 1: The output should be an instance of the datetime.datetime class for valid ISO format strings.\n@given(st.text(min_size=7))\ndef test_output_instance_property(date_string):\n    try:\n        result = datetime.datetime.fromisoformat(date_string)\n        assert isinstance(result, datetime.datetime)\n    except ValueError:\n        pass  # Expecting a ValueError for invalid strings\n\n# Property 2: The year, month, and day components of the output should correspond to valid calendar dates.\n@given(st.text(min_size=7))\ndef test_valid_calendar_date_property(date_string):\n    try:\n        result = datetime.datetime.fromisoformat(date_string)\n        year, month, day = result.year, result.month, result.day\n        assert 1 <= month <= 12\n        assert 1 <= day <= (datetime.datetime(year, month, 1) + datetime.timedelta(days=31)).day\n    except ValueError:\n        pass  # Expecting a ValueError for invalid strings\n\n# Property 3: If the input string represents a time of \"24:00\", the output should represent the next day at \"00:00:00\".\n@given(st.text(min_size=7))\ndef test_midnight_next_day_property(date_string):\n    if '24:00' in date_string:\n        try:\n            result = datetime.datetime.fromisoformat(date_string)\n            assert result.hour == 0 and result.minute == 0 and result.second == 0\n        except ValueError:\n            pass  # Expecting a ValueError for invalid strings\n\n# Property 4: The output's hour, minute, second, and microsecond components should be within their valid ranges.\n@given(st.text(min_size=7))\ndef test_valid_time_component_ranges_property(date_string):\n    try:\n        result = datetime.datetime.fromisoformat(date_string)\n        assert 0 <= result.hour < 24\n        assert 0 <= result.minute < 60\n        assert 0 <= result.second < 60\n        assert 0 <= result.microsecond < 1000000\n    except ValueError:\n        pass  # Expecting a ValueError for invalid strings\n\n# Property 5: The output should raise a ValueError for any input string that does not conform to the ISO 8601 format.\n@given(st.text())\ndef test_invalid_format_raises_value_error_property(date_string):\n    try:\n        result = datetime.datetime.fromisoformat(date_string)\n    except ValueError:\n        assert True  # Expecting a ValueError for invalid strings\n    else:\n        assert False  # If no exception is raised, then the test fails\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport re\n\n# Utility function to generate valid ISO format strings\ndef valid_isoformat_string():\n    # Generate valid dates and times in ISO format\n    years = st.integers(min_value=1900, max_value=2100)\n    months = st.integers(min_value=1, max_value=12)\n    days = st.integers(min_value=1, max_value=31)  # Days will be validated later\n    hours = st.integers(min_value=0, max_value=23)\n    minutes = st.integers(min_value=0, max_value=59)\n    seconds = st.integers(min_value=0, max_value=59)\n    microseconds = st.integers(min_value=0, max_value=999999)\n\n    def generate_valid_date(year, month, day):\n        try:\n            datetime(year, month, day)\n            return f\"{year:04d}-{month:02d}-{day:02d}\"\n        except ValueError:\n            return None\n\n    def generate_valid_time(hour, minute, second, microsecond):\n        return f\"{hour:02d}:{minute:02d}:{second:02d}.{microsecond:06d}\"\n\n    return (years.flatmap(lambda y: months.flatmap(lambda m: days.flatmap(lambda d: st.one_of(\n        st.just(generate_valid_date(y, m, d) + \"T\" + generate_valid_time(0, 0, 0, 0)),\n        st.just(generate_valid_date(y, m, d) + \"T\" + generate_valid_time(hours.generate(), minutes.generate(), seconds.generate(), microseconds.generate()))))))\n    )\n\n@given(valid_isoformat_string())\ndef test_output_is_datetime_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    assert isinstance(result, datetime)\n\n@given(valid_isoformat_string())\ndef test_valid_date_components_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    assert 1 <= result.month <= 12\n    assert 1 <= result.day <= 31  # Further validation can be done for specific months if needed\n\n@given(valid_isoformat_string())\ndef test_midnight_next_day_property(date_string):\n    if \"24:00\" in date_string:\n        result = datetime.fromisoformat(date_string)\n        assert result.hour == 0 and result.minute == 0 and result.second == 0\n        assert result.date() == (datetime.fromisoformat(date_string.split(\"T\")[0]) + timedelta(days=1)).date()\n\n@given(valid_isoformat_string())\ndef test_time_component_ranges_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    assert 0 <= result.hour < 24\n    assert 0 <= result.minute < 60\n    assert 0 <= result.second < 60\n    assert 0 <= result.microsecond < 1000000\n\n@given(st.text())\ndef test_invalid_isoformat_string_property(invalid_string):\n    if not re.match(r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(\\.\\d{1,6})?$', invalid_string) and len(invalid_string) >= 7:\n        try:\n            datetime.fromisoformat(invalid_string)\n            assert False, f\"Expected ValueError for invalid string: {invalid_string}\"\n        except ValueError:\n            pass  # Expected behavior\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.text(min_size=7))\ndef test_output_is_instance_of_datetime_property(date_string):\n    try:\n        result = datetime.datetime.fromisoformat(date_string)\n        assert isinstance(result, datetime.datetime)\n    except (ValueError, TypeError):\n        pass  # Expecting a ValueError or TypeError for invalid inputs\n\n@given(st.text(min_size=10, max_size=30))  # Ensuring enough length for date and time\ndef test_output_date_components_are_valid_property(date_string):\n    try:\n        result = datetime.datetime.fromisoformat(date_string)\n        assert result.year >= 1\n        assert 1 <= result.month <= 12\n        assert 1 <= result.day <= (datetime.datetime(result.year, result.month + 1, 1) - datetime.timedelta(days=1)).day\n    except (ValueError, TypeError):\n        pass  # Expecting a ValueError or TypeError for invalid inputs\n\n@given(st.text(min_size=10, max_size=30))  # Ensuring enough length for date and time\ndef test_output_time_for_24_hour_is_next_day_property(date_string):\n    if '24:00' in date_string:\n        date_string = date_string.replace('24:00', '00:00')\n        try:\n            result = datetime.datetime.fromisoformat(date_string)\n            assert result.hour == 0\n            assert result.day == (datetime.datetime.now().day % 30 + 1)  # Simplified check for next day\n        except ValueError:\n            pass  # Expecting a ValueError for invalid inputs\n\n@given(st.text(min_size=10, max_size=30))  # Ensuring enough length for date and time\ndef test_output_time_components_are_valid_property(date_string):\n    try:\n        result = datetime.datetime.fromisoformat(date_string)\n        assert 0 <= result.hour < 24\n        assert 0 <= result.minute < 60\n        assert 0 <= result.second < 60\n        assert 0 <= result.microsecond < 1000000\n    except (ValueError, TypeError):\n        pass  # Expecting a ValueError or TypeError for invalid inputs\n\n@given(st.text(min_size=7))\ndef test_invalid_input_raises_value_error_property(date_string):\n    try:\n        datetime.datetime.fromisoformat(date_string)\n    except ValueError:\n        pass  # Expected behavior for invalid inputs\n    except TypeError:\n        pass  # Expected behavior for invalid inputs\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime\nimport re\n\n# A strategy to generate valid ISO format datetime strings\ndef valid_iso_format_datetime():\n    # Generate valid years, months, days, hours, minutes, seconds, and microseconds\n    years = st.integers(min_value=1, max_value=9999)\n    months = st.integers(min_value=1, max_value=12)\n    days = st.integers(min_value=1, max_value=31)  # Note: This doesn't ensure valid days in month\n    hours = st.integers(min_value=0, max_value=23)\n    minutes = st.integers(min_value=0, max_value=59)\n    seconds = st.integers(min_value=0, max_value=59)\n    microseconds = st.integers(min_value=0, max_value=999999)\n\n    # Generate valid datetime strings\n    return (years.map(lambda y: f\"{y}-{months.generate()}\"\n                     f\"-{days.generate()}T{hours.generate()}:\"\n                     f\"{minutes.generate()}:{seconds.generate()}.{microseconds.generate()}\") \n                     .map(lambda d: re.sub(r'-(\\d{1,2})-(\\d{1,2})T', f'-{d.split(\"-\")[1]:02}-{d.split(\"-\")[2]:02}T', d)))\n\n@given(valid_iso_format_datetime())\ndef test_output_is_instance_of_datetime_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    assert isinstance(result, datetime)\n\n@given(valid_iso_format_datetime())\ndef test_output_has_valid_date_components_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    assert result.year >= 1 and result.year <= 9999\n    assert result.month >= 1 and result.month <= 12\n    assert result.day >= 1 and result.day <= 31  # This is a simplified check\n\n@given(valid_iso_format_datetime())\ndef test_output_midnight_for_24_hour_property(date_string):\n    if \"24:00\" in date_string:\n        result = datetime.fromisoformat(date_string)\n        assert result.hour == 0 and result.minute == 0 and result.second == 0\n\n@given(valid_iso_format_datetime())\ndef test_output_has_valid_time_components_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    assert result.hour >= 0 and result.hour <= 23\n    assert result.minute >= 0 and result.minute <= 59\n    assert result.second >= 0 and result.second <= 59\n    assert result.microsecond >= 0 and result.microsecond <= 999999\n\n@given(st.text())\ndef test_invalid_input_raises_value_error_property(invalid_string):\n    try:\n        datetime.fromisoformat(invalid_string)\n    except ValueError:\n        pass  # Expected behavior\n    else:\n        assert False  # If no exception is raised, the test should fail\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n# Property 1: The output should be an instance of the datetime.datetime class for valid ISO format strings.\n@given(st.from_type(str))\ndef test_output_is_datetime_instance_property(date_string):\n    if date_string.count('-') >= 2 and date_string.count(':') >= 1:  # Basic check for ISO format\n        result = datetime.datetime.fromisoformat(date_string)\n        assert isinstance(result, datetime.datetime)\n\n# Property 2: The year, month, and day components of the output should correspond to valid calendar dates.\n@given(st.from_type(str))\ndef test_output_represents_valid_calendar_date_property(date_string):\n    if date_string.count('-') >= 2 and date_string.count(':') >= 1:\n        result = datetime.datetime.fromisoformat(date_string)\n        assert result.day <= (datetime.datetime(result.year, result.month + 1, 1) - datetime.timedelta(days=1)).day\n\n# Property 3: If the input string represents a time of \"24:00\", the output should represent the next day at \"00:00:00\".\n@given(st.from_type(str))\ndef test_midnight_next_day_property(date_string):\n    if \"24:00\" in date_string:\n        result = datetime.datetime.fromisoformat(date_string)\n        assert result.hour == 0 and result.minute == 0 and result.second == 0\n        assert result.day > int(date_string.split('-')[2].split('T')[0])  # Check if the day incremented\n\n# Property 4: The output's hour, minute, second, and microsecond components should be within their valid ranges.\n@given(st.from_type(str))\ndef test_output_time_components_within_valid_ranges_property(date_string):\n    if date_string.count('-') >= 2 and date_string.count(':') >= 1:\n        result = datetime.datetime.fromisoformat(date_string)\n        assert 0 <= result.hour < 24\n        assert 0 <= result.minute < 60\n        assert 0 <= result.second < 60\n        assert 0 <= result.microsecond < 1000000\n\n# Property 5: The output should raise a ValueError for any input string that does not conform to the ISO 8601 format or represents an invalid date or time.\n@given(st.from_type(str))\ndef test_invalid_iso_format_raises_value_error_property(date_string):\n    if not (date_string.count('-') >= 2 and date_string.count(':') >= 1):\n        try:\n            datetime.datetime.fromisoformat(date_string)\n            assert False, \"Expected ValueError was not raised.\"\n        except ValueError:\n            pass  # Expected ValueError was raised\n\n# End program\n```"], "properties": ["1. The output should be an instance of the `datetime.datetime` class for valid ISO format strings.\n2. The year, month, and day components of the output should correspond to valid calendar dates, ensuring that the day does not exceed the number of days in the given month and year.\n3. If the input string represents a time of \"24:00\", the output should represent the next day at \"00:00:00\" (midnight) of the corresponding date.\n4. The output's hour, minute, second, and microsecond components should be within their valid ranges (0-23 for hours, 0-59 for minutes, 0-59 for seconds, and 0-999999 for microseconds).\n5. The output should raise a `ValueError` for any input string that does not conform to the ISO 8601 format or represents an invalid date or time."], "api_doc": "classmethod datetime.fromisoformat(date_string)\nReturn a datetime corresponding to a date_string in any valid ISO 8601 format, with the following exceptions:\n\nTime zone offsets may have fractional seconds.\n\nThe T separator may be replaced by any single unicode character.\n\nFractional hours and minutes are not supported.\n\nReduced precision dates are not currently supported (YYYY-MM, YYYY).\n\nExtended date representations are not currently supported (\u00b1YYYYYY-MM-DD).\n\nOrdinal dates are not currently supported (YYYY-OOO).\n\nExamples:\n\n>>>\nfrom datetime import datetime\ndatetime.fromisoformat('2011-11-04')\ndatetime.datetime(2011, 11, 4, 0, 0)\ndatetime.fromisoformat('20111104')\ndatetime.datetime(2011, 11, 4, 0, 0)\ndatetime.fromisoformat('2011-11-04T00:05:23')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\ndatetime.fromisoformat('2011-11-04T00:05:23Z')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, tzinfo=datetime.timezone.utc)\ndatetime.fromisoformat('20111104T000523')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\ndatetime.fromisoformat('2011-W01-2T00:05:23.283')\ndatetime.datetime(2011, 1, 4, 0, 5, 23, 283000)\ndatetime.fromisoformat('2011-11-04 00:05:23.283')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000)\ndatetime.fromisoformat('2011-11-04 00:05:23.283+00:00')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000, tzinfo=datetime.timezone.utc)\ndatetime.fromisoformat('2011-11-04T00:05:23+04:00')   \ndatetime.datetime(2011, 11, 4, 0, 5, 23,\n    tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))\nAdded in version 3.7.\n\nChanged in version 3.11: Previously, this method only supported formats that could be emitted by date.isoformat() or datetime.isoformat().", "api_code": "@classmethod\ndef fromisoformat(cls, date_string):\n    if not isinstance(date_string, str):\n        raise TypeError('fromisoformat: argument must be str')\n\n    if len(date_string) < 7:\n        raise ValueError(f'Invalid isoformat string: {date_string!r}')\n\n    # Split this at the separator\n    try:\n        separator_location = _find_isoformat_datetime_separator(date_string)\n        dstr = date_string[0:separator_location]\n        tstr = date_string[(separator_location+1):]\n\n        date_components = _parse_isoformat_date(dstr)\n    except ValueError:\n        raise ValueError(\n            f'Invalid isoformat string: {date_string!r}') from None\n\n    if tstr:\n        try:\n            time_components, became_next_day, error_from_components = _parse_isoformat_time(tstr)\n        except ValueError:\n            raise ValueError(\n                f'Invalid isoformat string: {date_string!r}') from None\n        else:\n            if error_from_components:\n                raise ValueError(\"minute, second, and microsecond must be 0 when hour is 24\")\n\n            if became_next_day:\n                year, month, day = date_components\n                # Only wrap day/month when it was previously valid\n                if month <= 12 and day <= (days_in_month := _days_in_month(year, month)):\n                    # Calculate midnight of the next day\n                    day += 1\n                    if day > days_in_month:\n                        day = 1\n                        month += 1\n                        if month > 12:\n                            month = 1\n                            year += 1\n                    date_components = [year, month, day]\n    else:\n        time_components = [0, 0, 0, 0, None]\n\n    return cls(*(date_components + time_components))"}
{"function_name": "datetime.date.isocalendar", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_year_property(date):\n    year, week, day = date.isocalendar()\n    if week == 1:\n        assert year == date.year or year == date.year - 1\n\n@given(st.dates())\ndef test_week_range_property(date):\n    year, week, day = date.isocalendar()\n    assert 1 <= week <= 53\n\n@given(st.dates())\ndef test_day_range_property(date):\n    year, week, day = date.isocalendar()\n    assert 1 <= day <= 7\n\n@given(st.dates())\ndef test_first_week_year_property(date):\n    year, week, day = date.isocalendar()\n    if week == 1:\n        assert year == date.year or (date.month == 1 and date.day <= 7 and year == date.year - 1)\n\n@given(st.dates())\ndef test_last_day_of_year_property(date):\n    if date.month == 12 and date.day == 31:\n        year, week, day = date.isocalendar()\n        if week == 1:\n            assert year == date.year + 1 or week == 53\n        else:\n            assert week == 52 or week == 53\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_output_year_property(dt):\n    iso_calendar = dt.isocalendar()\n    year = iso_calendar[0]\n    week = iso_calendar[1]\n    \n    if week == 1:\n        assert year == dt.year or year == dt.year - 1\n    else:\n        assert year == dt.year\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_output_week_property(dt):\n    iso_calendar = dt.isocalendar()\n    week = iso_calendar[1]\n    assert 1 <= week <= 53\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_output_day_property(dt):\n    iso_calendar = dt.isocalendar()\n    day = iso_calendar[2]\n    assert 1 <= day <= 7\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_week_one_year_property(dt):\n    iso_calendar = dt.isocalendar()\n    year = iso_calendar[0]\n    week = iso_calendar[1]\n    \n    if week == 1:\n        assert year == dt.year or year == dt.year - 1\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_last_day_of_year_property(dt):\n    if dt.month == 12 and dt.day == 31:\n        iso_calendar = dt.isocalendar()\n        week = iso_calendar[1]\n        assert week == 53 or week == 1\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_year_correct_for_first_week_property(date):\n    iso_calendar_date = date.isocalendar()\n    year, week, day = iso_calendar_date\n    if week == 1:\n        assert year == date.year or year == date.year - 1\n\n@given(st.dates())\ndef test_output_week_range_property(date):\n    iso_calendar_date = date.isocalendar()\n    year, week, day = iso_calendar_date\n    assert 1 <= week <= 53\n\n@given(st.dates())\ndef test_output_day_range_property(date):\n    iso_calendar_date = date.isocalendar()\n    year, week, day = iso_calendar_date\n    assert 1 <= day <= 7\n\n@given(st.dates())\ndef test_week_one_year_adjustment_property(date):\n    iso_calendar_date = date.isocalendar()\n    year, week, day = iso_calendar_date\n    if week == 1:\n        assert year == date.year or year == date.year - 1\n\n@given(st.dates())\ndef test_last_day_of_year_week_property(date):\n    if date.month == 12 and date.day == 31:\n        iso_calendar_date = date.isocalendar()\n        year, week, day = iso_calendar_date\n        assert week == 52 or week == 1  # Last week of the year or first week of the next year\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n# Helper function to calculate the ISO calendar date\ndef isocalendar(year, month, day):\n    date = datetime.date(year, month, day)\n    return date.isocalendar()\n\n@given(st.integers(min_value=-9999, max_value=9999), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_output_year_property(year, month, day):\n    try:\n        week_year, week, day_of_week = isocalendar(year, month, day)\n        if week < 1 or week > 53:\n            raise ValueError(\"Week out of range\")\n        if week == 1 and day_of_week == 1 and month == 1:\n            assert week_year == year  # First week of the year\n        else:\n            assert week_year in {year, year - 1}  # ISO year check\n    except ValueError:\n        pass  # Ignore invalid date cases\n\n@given(st.integers(min_value=-9999, max_value=9999), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_output_week_property(year, month, day):\n    try:\n        week_year, week, day_of_week = isocalendar(year, month, day)\n        assert 1 <= week <= 53  # Week should be in the valid range\n    except ValueError:\n        pass  # Ignore invalid date cases\n\n@given(st.integers(min_value=-9999, max_value=9999), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_output_day_property(year, month, day):\n    try:\n        week_year, week, day_of_week = isocalendar(year, month, day)\n        assert 1 <= day_of_week <= 7  # Day should be in the range of 1 to 7\n    except ValueError:\n        pass  # Ignore invalid date cases\n\n@given(st.integers(min_value=-9999, max_value=9999), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_week_one_year_property(year, month, day):\n    try:\n        week_year, week, day_of_week = isocalendar(year, month, day)\n        if week == 1 and day_of_week == 1 and month == 1:\n            assert week_year == year  # Check for first week of the year\n        elif week == 1:\n            # If it's the first week, the year can be year or year - 1\n            assert week_year in {year, year - 1}\n    except ValueError:\n        pass  # Ignore invalid date cases\n\n@given(st.integers(min_value=-9999, max_value=9999), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_end_of_year_week_property(year, month, day):\n    try:\n        week_year, week, day_of_week = isocalendar(year, month, day)\n        if month == 12 and day == 31:\n            # Check that the output week is either the last week of the year or the first week of the next year\n            assert week == 52 or week == 1  # Depending on the year transition\n    except ValueError:\n        pass  # Ignore invalid date cases\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_datetime_date_isocalendar_year_output_property(dt):\n    iso_calendar = dt.isocalendar()\n    year = iso_calendar[0]\n    if dt.month == 1 and dt.day <= 7 and dt.weekday() == 0:  # First week condition\n        assert year == dt.year\n    else:\n        assert year == dt.isocalendar()[0]\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_datetime_date_isocalendar_week_range_property(dt):\n    week = dt.isocalendar()[1]\n    assert 1 <= week <= 53\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_datetime_date_isocalendar_day_range_property(dt):\n    day = dt.isocalendar()[2]\n    assert 1 <= day <= 7\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_datetime_date_isocalendar_week_one_property(dt):\n    iso_calendar = dt.isocalendar()\n    year = iso_calendar[0]\n    if iso_calendar[1] == 1:  # First week\n        assert year == dt.year or year == dt.year - 1\n\n@given(st.datetimes(min_value=datetime.datetime(1, 1, 1), max_value=datetime.datetime(9999, 12, 31)))\ndef test_datetime_date_isocalendar_end_of_year_property(dt):\n    if dt.month == 12 and dt.day == 31:\n        week = dt.isocalendar()[1]\n        year = dt.isocalendar()[0]\n        if dt >= datetime.datetime(year + 1, 1, 1, 0, 0):  # After new year's day\n            assert week == 1 or week == 53\n        else:\n            assert week == 53\n# End program\n```"], "properties": ["1. The output year should be equal to the input year if the date falls within the first week of the year; otherwise, it should reflect the correct ISO year based on the week calculation.\n\n2. The output week should always be in the range of 1 to 53, inclusive, as per ISO week date standards.\n\n3. The output day should always be in the range of 1 to 7, corresponding to Monday through Sunday.\n\n4. For any given date, if the output week is 1, then the output year should be the same as the input year or one less if the date falls before the first Monday of the year.\n\n5. If the input date is the last day of the year (December 31), the output week should either be the last week of the year or the first week of the following year, depending on whether it falls before or after the first Monday of the new year."], "api_doc": "date.isocalendar()\nReturn a named tuple object with three components: year, week and weekday.\n\nThe ISO calendar is a widely used variant of the Gregorian calendar. [3]\n\nThe ISO year consists of 52 or 53 full weeks, and where a week starts on a Monday and ends on a Sunday. The first week of an ISO year is the first (Gregorian) calendar week of a year containing a Thursday. This is called week number 1, and the ISO year of that Thursday is the same as its Gregorian year.\n\nFor example, 2004 begins on a Thursday, so the first week of ISO year 2004 begins on Monday, 29 Dec 2003 and ends on Sunday, 4 Jan 2004:\n\n>>>\nfrom datetime import date\ndate(2003, 12, 29).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=1)\ndate(2004, 1, 4).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=7)\nChanged in version 3.9: Result changed from a tuple to a named tuple.", "api_code": "def isocalendar(self):\n    year = self._year\n    week1monday = _isoweek1monday(year)\n    today = _ymd2ord(self._year, self._month, self._day)\n    # Internally, week and day have origin 0\n    week, day = divmod(today - week1monday, 7)\n    if week < 0:\n        year -= 1\n        week1monday = _isoweek1monday(year)\n        week, day = divmod(today - week1monday, 7)\n    elif week >= 52:\n        if today >= _isoweek1monday(year+1):\n            year += 1\n            week = 0\n    return _IsoCalendarDate(year, week+1, day+1)"}
{"function_name": "dateutil.parser.isoparse", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport dateutil.parser\n\n@given(st.text())\ndef test_output_is_valid_datetime(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Expected for invalid inputs\n\n@given(st.text())\ndef test_output_for_24_hour_case(dt_str):\n    if \"24:00\" in dt_str:\n        result = dateutil.parser.isoparse(dt_str.replace(\"24:00\", \"00:00\"))\n        assert result == datetime.combine(result.date() + timedelta(days=1), datetime.min.time())\n\n@given(st.text())\ndef test_output_matches_input_components(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        components = dateutil.parser._parse_isodate(dt_str)\n        assert (result.year, result.month, result.day, result.hour, result.minute, result.second) == components\n    except ValueError:\n        pass  # Expected for invalid inputs\n\n@given(st.text())\ndef test_value_error_for_unknown_components(dt_str):\n    invalid_str = dt_str + \"XYZ\"  # Append unknown component\n    try:\n        dateutil.parser.isoparse(invalid_str)\n        assert False  # Should raise ValueError\n    except ValueError:\n        pass\n\n@given(st.text())\ndef test_timezone_information(dt_str):\n    if \"+00:00\" in dt_str or \"-00:00\" in dt_str:\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.tzinfo is not None\n    else:\n        try:\n            result = dateutil.parser.isoparse(dt_str)\n            assert result.tzinfo is None\n        except ValueError:\n            pass  # Expected for invalid inputs\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom dateutil.parser import isoparse\nfrom datetime import datetime, timedelta\nimport re\n\n@given(st.text())\ndef test_output_is_valid_datetime(dt_str):\n    try:\n        result = isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Expected for invalid input\n\n@given(st.text())\ndef test_output_next_day_for_24_hour(dt_str):\n    if re.match(r'^\\d{4}-\\d{2}-\\d{2}T24:00(:\\d{2}:\\d{2}(\\.\\d{1,6})?)([+-]\\d{2}:\\d{2}|Z)?$', dt_str):\n        result = isoparse(dt_str)\n        assert result.time() == datetime.min.time()  # should be 00:00\n        assert result.date() == (datetime.strptime(dt_str[:10], '%Y-%m-%d') + timedelta(days=1)).date()\n\n@given(st.text())\ndef test_output_matches_input_components(dt_str):\n    try:\n        result = isoparse(dt_str)\n        components = [result.year, result.month, result.day, result.hour, result.minute, result.second]\n        parsed_components = _parse_components(dt_str)  # Assuming a helper function to parse components\n        assert components[:3] == parsed_components[:3]  # Check year, month, day\n        assert components[3:] == parsed_components[3:]  # Check time components\n    except ValueError:\n        pass  # Expected for invalid input\n\n@given(st.text())\ndef test_raises_valueerror_for_unknown_components(dt_str):\n    invalid_str = dt_str + 'X'  # Append an invalid component\n    try:\n        isoparse(invalid_str)\n        assert False  # Should raise ValueError\n    except ValueError:\n        pass  # Expected behavior\n\n@given(st.text())\ndef test_output_timezone_information(dt_str):\n    if re.match(r'^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}([+-]\\d{2}:\\d{2}|Z)$', dt_str):\n        result = isoparse(dt_str)\n        assert isinstance(result.tzinfo, datetime.tzinfo)  # Ensure timezone info is present\n\n# End program\n``` \n\nNote: The `_parse_components` function is a placeholder in the `test_output_matches_input_components` test. You'd need to implement or replace it with a function that extracts components from the ISO string for comparison.", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport dateutil.parser\n\n@given(st.text())\ndef test_output_is_datetime_property(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Valid as long as it raises ValueError for invalid input\n\n@given(st.text())\ndef test_output_for_24_hour_string_property(dt_str):\n    if \"24:00\" in dt_str:\n        parts = dt_str.split(\"24:00\")\n        if len(parts) > 1:\n            date_part = parts[0]\n            result = dateutil.parser.isoparse(date_part + \"00:00\") + timedelta(days=1)\n            assert result.hour == 0 and result.minute == 0\n            assert result.date() == (datetime.fromisoformat(date_part).date() + timedelta(days=1))\n\n@given(st.text())\ndef test_output_matches_input_values_property(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        if result:\n            components = list(map(int, dt_str.replace(\":\", \"-\").replace(\"-\", \" \").split()))\n            assert result.year == components[0]\n            assert result.month == components[1]\n            assert result.day == components[2]\n            assert result.hour == components[3]\n            assert result.minute == components[4]\n            assert result.second == components[5]\n    except ValueError:\n        pass  # Valid as long as it raises ValueError for invalid input\n\n@given(st.text())\ndef test_output_for_unknown_components_property(dt_str):\n    try:\n        dateutil.parser.isoparse(dt_str)\n    except ValueError:\n        assert True  # Valid case as it should raise ValueError for unknown components\n    else:\n        assert False  # If no exception raised, it should be invalid input\n\n@given(st.text())\ndef test_output_timezone_information_property(dt_str):\n    # This test is more complex and may require additional setup for timezones\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.tzinfo is not None  # Assuming input string has timezone information\n    except ValueError:\n        pass  # Valid as long as it raises ValueError for invalid input\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport dateutil.parser\n\n@given(st.text())\ndef test_output_is_valid_datetime(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Expecting ValueError for invalid input\n\n@given(st.text())\ndef test_output_for_24_hour_case(dt_str):\n    if \"24:00\" in dt_str:\n        result = dateutil.parser.isoparse(dt_str.replace(\"24:00\", \"00:00\"))\n        assert result == datetime.strptime(dt_str.split(\"T\")[0], \"%Y-%m-%d\") + timedelta(days=1)\n\n@given(st.text())\ndef test_output_matches_input_values(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        components = result.timetuple()[:6]  # year, month, day, hour, minute, second\n        # Compare components with parsed values (this is a simplification)\n        assert components == (result.year, result.month, result.day, result.hour, result.minute, result.second)\n    except ValueError:\n        pass  # Expecting ValueError for invalid input\n\n@given(st.text())\ndef test_raises_valueerror_for_unknown_components(dt_str):\n    if \"unknown\" in dt_str:  # Just an example of generating invalid input\n        try:\n            dateutil.parser.isoparse(dt_str)\n            assert False  # Ensure we don't reach this line\n        except ValueError:\n            assert True  # Expecting ValueError\n\n@given(st.text())\ndef test_output_timezone_information(dt_str):\n    if \"+00:00\" in dt_str:  # Example of a string with timezone\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.tzinfo is not None  # Ensure timezone is set\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport dateutil.parser\n\n@given(st.text())\ndef test_output_is_valid_datetime_property(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Expected if the input string is invalid\n\n@given(st.text())\ndef test_output_for_24_hour_case_property(dt_str):\n    if \"24:00\" in dt_str:\n        result = dateutil.parser.isoparse(dt_str.replace(\"24:00\", \"00:00\"))\n        assert result == datetime(*dateutil.parser._parse_isodate(dt_str)[0]) + timedelta(days=1)\n\n@given(st.text())\ndef test_output_year_month_day_hour_minute_second_property(dt_str):\n    try:\n        components, pos = dateutil.parser._parse_isodate(dt_str)\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.year == components[0]\n        assert result.month == components[1]\n        assert result.day == components[2]\n        assert result.hour == components[3] % 24  # Adjust for 24 hour case\n    except ValueError:\n        pass  # Expected if the input string is invalid\n\n@given(st.text())\ndef test_value_error_for_unknown_iso_components_property(dt_str):\n    bad_str = dt_str + \"XYZ\"\n    try:\n        dateutil.parser.isoparse(bad_str)\n        assert False  # We expect a ValueError\n    except ValueError:\n        assert True  # Expected\n\n@given(st.text())\ndef test_timezone_information_property(dt_str):\n    if any(tz in dt_str for tz in ['Z', '+', '-']):\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.tzinfo is not None  # Ensure timezone info is present\n\n# End program\n```"], "properties": ["1. The output should always be a valid `datetime` object representing the parsed ISO date and time from the input string.\n\n2. If the input string represents a date and time that is exactly \"24:00\", the output should represent the next day at \"00:00\".\n\n3. The output `datetime` object should have the same year, month, day, hour, minute, and second values as specified in the input string, barring any adjustments for the \"24:00\" case.\n\n4. The function should raise a `ValueError` if the input string contains unknown ISO components, ensuring that the output is only produced for valid ISO 8601 formats.\n\n5. The output should maintain the timezone information if the input string includes a timezone offset, ensuring that the `datetime` object accurately reflects the specified timezone."], "api_doc": "parser.isoparse(dt_str)\uf0c1\nParse an ISO-8601 datetime string into a datetime.datetime.\n\nAn ISO-8601 datetime string consists of a date portion, followed optionally by a time portion - the date and time portions are separated by a single character separator, which is T in the official standard. Incomplete date formats (such as YYYY-MM) may not be combined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\nYYYY\n\nYYYY-MM\n\nYYYY-MM-DD or YYYYMMDD\n\nUncommon:\n\nYYYY-Www or YYYYWww - ISO week (day defaults to 0)\n\nYYYY-Www-D or YYYYWwwD - ISO week and day\n\nThe ISO week and day numbering follows the same logic as datetime.date.isocalendar().\n\nSupported time formats are:\n\nhh\n\nhh:mm or hhmm\n\nhh:mm:ss or hhmmss\n\nhh:mm:ss.ssssss (Up to 6 sub-second digits)\n\nMidnight is a special case for hh, as the standard supports both 00:00 and 24:00 as a representation. The decimal separator can be either a dot or a comma.\n\nCaution\n\nSupport for fractional components other than seconds is part of the ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\nZ (UTC)\n\n\u00b1HH:MM\n\n\u00b1HHMM\n\n\u00b1HH\n\nOffsets will be represented as dateutil.tz.tzoffset objects, with the exception of UTC, which will be represented as dateutil.tz.tzutc. Time zone offsets equivalent to UTC (such as +00:00) will also be represented as dateutil.tz.tzutc.\n\nParameters\n:\ndt_str \u2013 A string or stream containing only an ISO-8601 datetime string\n\nReturns\n:\nReturns a datetime.datetime representing the string. Unspecified components default to their lowest value.\n\nWarning\n\nAs of version 2.7.0, the strictness of the parser should not be considered a stable part of the contract. Any valid ISO-8601 string that parses correctly with the default settings will continue to parse correctly in future versions, but invalid strings that currently fail (e.g. 2017-01-01T00:00+00:00:00) are not guaranteed to continue failing in future versions if they encode a valid date.\n\nNew in version 2.7.0.", "api_code": "@_takes_ascii\ndef isoparse(self, dt_str):\n    components, pos = self._parse_isodate(dt_str)\n\n    if len(dt_str) > pos:\n        if self._sep is None or dt_str[pos:pos + 1] == self._sep:\n            components += self._parse_isotime(dt_str[pos + 1:])\n        else:\n            raise ValueError('String contains unknown ISO components')\n\n    if len(components) > 3 and components[3] == 24:\n        components[3] = 0\n        return datetime(*components) + timedelta(days=1)\n\n    return datetime(*components)"}
{"function_name": "statistics.mean", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_identical_elements_property(data):\n    if len(data) > 0:\n        identical_value = data[0]\n        mean_value = statistics.mean([identical_value] * len(data))\n        assert mean_value == identical_value\n\n@given(st.lists(st.floats(), max_size=100))\ndef test_mean_empty_list_property(data):\n    if len(data) == 0:\n        try:\n            statistics.mean(data)\n            assert False, \"Expected StatisticsError for empty list\"\n        except statistics.StatisticsError:\n            pass  # Expected exception\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_range_property(data):\n    mean_value = statistics.mean(data)\n    assert mean_value >= min(data) and mean_value <= max(data)\n\n@given(st.lists(st.floats(), max_size=100))\ndef test_mean_invariant_under_zero_property(data):\n    mean_value = statistics.mean(data)\n    mean_value_with_zero = statistics.mean(data + [0])\n    assert mean_value == mean_value_with_zero\n\n@given(st.lists(st.floats(), min_size=1), st.lists(st.floats(), min_size=1))\ndef test_mean_concatenated_lists_property(data1, data2):\n    mean1 = statistics.mean(data1)\n    mean2 = statistics.mean(data2)\n    combined_data = data1 + data2\n    if len(combined_data) > 0:\n        combined_mean = statistics.mean(combined_data)\n        weighted_mean = (mean1 * len(data1) + mean2 * len(data2)) / (len(data1) + len(data2))\n        assert combined_mean == pytest.approx(weighted_mean)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import mean, StatisticsError\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_identical_elements_property(data):\n    if len(data) > 0:\n        identical_value = data[0]\n        assert mean([identical_value] * len(data)) == identical_value\n\n@given()\ndef test_mean_empty_list_property():\n    with st.raises(StatisticsError):\n        mean([])\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_mean_range_property(data):\n    if data:\n        calculated_mean = mean(data)\n        assert min(data) <= calculated_mean <= max(data)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_zero_addition_property(data):\n    calculated_mean = mean(data)\n    assert mean(data + [0]) == calculated_mean\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)), st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_mean_concat_weighted_average_property(list1, list2):\n    if list1 and list2:\n        mean1 = mean(list1)\n        mean2 = mean(list2)\n        combined_list = list1 + list2\n        calculated_mean = mean(combined_list)\n        weighted_mean = (mean1 * len(list1) + mean2 * len(list2)) / (len(list1) + len(list2))\n        assert calculated_mean == weighted_mean\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import mean, StatisticsError\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_identical_elements_property(data):\n    if len(data) > 0:\n        identical_value = data[0]\n        assert mean([identical_value] * len(data)) == identical_value\n# End program\n\n@given(st.lists(st.floats(), max_size=100))\ndef test_mean_empty_list_property(data):\n    if len(data) == 0:\n        with st.raises(StatisticsError):\n            mean(data)\n# End program\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_range_property(data):\n    calculated_mean = mean(data)\n    assert min(data) <= calculated_mean <= max(data)\n# End program\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_invariance_under_zero_property(data):\n    original_mean = mean(data)\n    assert mean(data + [0]) == original_mean\n# End program\n\n@given(st.lists(st.floats(), min_size=1), st.lists(st.floats(), min_size=1))\ndef test_mean_concatenated_lists_property(list1, list2):\n    mean1 = mean(list1)\n    mean2 = mean(list2)\n    combined_mean = mean(list1 + list2)\n    weighted_mean = (mean1 * len(list1) + mean2 * len(list2)) / (len(list1) + len(list2))\n    assert combined_mean == weighted_mean\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import mean, StatisticsError\n\n@given(st.lists(st.floats()))\ndef test_mean_identical_elements_property(data):\n    if data:  # Ensure there are elements to test\n        identical_value = data[0]\n        identical_list = [identical_value] * len(data)\n        assert mean(identical_list) == identical_value\n\n@given()\ndef test_mean_empty_list_property():\n    with st.raises(StatisticsError):\n        mean([])\n\n@given(st.lists(st.floats()))\ndef test_mean_range_property(data):\n    if data:  # Ensure there are elements to test\n        calculated_mean = mean(data)\n        assert min(data) <= calculated_mean <= max(data)\n\n@given(st.lists(st.floats()))\ndef test_mean_invariance_under_zero_property(data):\n    zero_added_data = data + [0]\n    assert mean(data) == mean(zero_added_data)\n\n@given(st.lists(st.floats()), st.lists(st.floats()))\ndef test_mean_concat_lists_property(data1, data2):\n    if data1 and data2:  # Ensure both lists have elements to test\n        mean1 = mean(data1)\n        mean2 = mean(data2)\n        combined_list = data1 + data2\n        combined_mean = mean(combined_list)\n        assert combined_mean == (mean1 * len(data1) + mean2 * len(data2)) / (len(data1) + len(data2))\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import mean, StatisticsError\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_identical_elements_property(data):\n    if len(data) > 0:\n        identical_value = data[0]\n        assert mean([identical_value] * len(data)) == identical_value\n\n@given(st.lists(st.floats(), min_size=0))\ndef test_mean_empty_list_property(data):\n    if len(data) == 0:\n        try:\n            mean(data)\n            assert False, \"Expected StatisticsError for empty list\"\n        except StatisticsError:\n            pass\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_range_property(data):\n    calculated_mean = mean(data)\n    assert min(data) <= calculated_mean <= max(data)\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_invariant_zero_property(data):\n    original_mean = mean(data)\n    assert mean(data + [0]) == original_mean\n\n@given(st.lists(st.floats(), min_size=1), st.lists(st.floats(), min_size=1))\ndef test_mean_concatenated_lists_property(data1, data2):\n    combined_data = data1 + data2\n    if combined_data:\n        mean_combined = mean(combined_data)\n        mean_data1 = mean(data1)\n        mean_data2 = mean(data2)\n        weighted_mean = (mean_data1 * len(data1) + mean_data2 * len(data2)) / (len(data1) + len(data2))\n        assert mean_combined == weighted_mean\n\n# End program\n```"], "properties": ["1. The mean of a list containing identical elements should equal the value of those elements.\n2. The mean of an empty list should raise a StatisticsError.\n3. The mean of a list with negative and positive numbers should be within the range of the minimum and maximum values of the list.\n4. The mean of a list should be invariant under the addition of zero (i.e., adding zero to the list should not change the mean).\n5. The mean of a concatenated list (combining two lists) should equal the weighted average of the means of the individual lists, weighted by their sizes."], "api_doc": "statistics.mean(data)\nReturn the sample arithmetic mean of data which can be a sequence or iterable.\n\nThe arithmetic mean is the sum of the data divided by the number of data points. It is commonly called \u201cthe average\u201d, although it is only one of many different mathematical averages. It is a measure of the central location of the data.\n\nIf data is empty, StatisticsError will be raised.\n\nSome examples of use:\n\n>>>\nmean([1, 2, 3, 4, 4])\n2.8\nmean([-1.0, 2.5, 3.25, 5.75])\n2.625\n\nfrom fractions import Fraction as F\nmean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\nfrom decimal import Decimal as D\nmean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\nNote The mean is strongly affected by outliers and is not necessarily a typical example of the data points. For a more robust, although less efficient, measure of central tendency, see median().\nThe sample mean gives an unbiased estimate of the true population mean, so that when taken on average over all the possible samples, mean(sample) converges on the true mean of the entire population. If data represents the entire population rather than a sample, then mean(data) is equivalent to calculating the true population mean \u03bc.", "api_code": "def mean(data):\n    T, total, n = _sum(data)\n    if n < 1:\n        raise StatisticsError('mean requires at least one data point')\n    return _convert(total / n, T)"}
{"function_name": "networkx.algorithms.cycles.find_cycle", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_cycle_is_closed_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.DiGraph, st.lists(st.tuples(st.integers(), st.integers()))),\n        st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers())))\n    ))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.one_of(st.none(), st.just(\"original\"), st.just(\"reverse\"), st.just(\"ignore\")))\n\n    cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n    \n    if cycle:\n        assert cycle[0][0] == cycle[-1][1]\n\n@given(st.data())\ndef test_no_cycle_in_acyclic_graph_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()))))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.one_of(st.none(), st.just(\"original\"), st.just(\"reverse\"), st.just(\"ignore\")))\n\n    if nx.is_directed_acyclic_graph(G):\n        cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n        assert cycle == []\n\n@given(st.data())\ndef test_cycle_length_property(data):\n    G = data.draw(st.builds(nx.DiGraph, st.lists(st.tuples(st.integers(), st.integers()))))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.one_of(st.none(), st.just(\"original\"), st.just(\"reverse\"), st.just(\"ignore\")))\n\n    try:\n        cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n        if len(cycle) >= 3:\n            assert len(cycle) >= 3\n    except nx.exception.NetworkXNoCycle:\n        pass\n\n@given(st.data())\ndef test_cycle_edges_exist_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()))))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.one_of(st.none(), st.just(\"original\"), st.just(\"reverse\"), st.just(\"ignore\")))\n\n    cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n    \n    for edge in cycle:\n        assert edge in G.edges() or (edge[1], edge[0]) in G.edges()  # Check both directions for undirected graphs\n\n@given(st.data())\ndef test_cycle_orientation_property(data):\n    G = data.draw(st.builds(nx.DiGraph, st.lists(st.tuples(st.integers(), st.integers()))))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = \"reverse\"\n\n    cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n    \n    if cycle:\n        for edge in cycle:\n            assert (edge[1], edge[0]) in G.edges()  # Check if the reversed edge exists\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_cycle_forms_closed_loop_property(data):\n    G = data.draw(st.builds(nx.Graph))  # Generate a graph\n    source = data.draw(st.one_of([None, st.sampled_from(G.nodes())]))  # Generate a source node\n    cycle = nx.algorithms.cycles.find_cycle(G, source)\n    \n    if cycle:\n        first_edge = cycle[0]\n        last_edge = cycle[-1]\n        tail_first, head_first = first_edge\n        tail_last, head_last = last_edge\n        assert head_last == tail_first  # Check if the last node connects back to the first node\n\n@given(st.data())\ndef test_empty_list_for_acyclic_graph_property(data):\n    G = data.draw(st.builds(nx.Graph))  # Generate a graph\n    if nx.is_directed_acyclic_graph(G):\n        cycle = nx.algorithms.cycles.find_cycle(G)\n        assert cycle == []  # Check that the output is an empty list\n\n@given(st.data())\ndef test_cycle_length_greater_than_or_equal_to_three_property(data):\n    G = data.draw(st.builds(nx.Graph))  # Generate a graph\n    source = data.draw(st.one_of([None, st.sampled_from(G.nodes())]))  # Generate a source node\n    cycle = nx.algorithms.cycles.find_cycle(G, source)\n    \n    if cycle and len(cycle) < 3:\n        assert False, \"Cycle found with length less than 3.\"\n\n@given(st.data())\ndef test_output_cycle_edges_exist_in_input_graph_property(data):\n    G = data.draw(st.builds(nx.Graph))  # Generate a graph\n    source = data.draw(st.one_of([None, st.sampled_from(G.nodes())]))  # Generate a source node\n    cycle = nx.algorithms.cycles.find_cycle(G, source)\n    \n    for edge in cycle:\n        assert edge in G.edges()  # Check that each edge in the cycle exists in the graph\n\n@given(st.data())\ndef test_reverse_orientation_cycle_property(data):\n    G = data.draw(st.builds(nx.DiGraph))  # Generate a directed graph\n    source = data.draw(st.sampled_from(G.nodes()))  # Generate a source node\n    cycle = nx.algorithms.cycles.find_cycle(G, source, orientation=\"reverse\")\n    \n    if cycle:\n        for edge in cycle:\n            tail, head = edge\n            assert (head, tail) in G.edges()  # Check that the reversed edge is in the graph\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_cycle_edges_form_closed_loop_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)),\n        st.builds(nx.DiGraph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\n    ))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.sampled_from([None, \"original\", \"reverse\", \"ignore\"]))\n    \n    cycle = nx.algorithms.cycles.find_cycle(G, source=source, orientation=orientation)\n    \n    if cycle:\n        tail, head = cycle[0]\n        assert head in (e[0] for e in cycle[1:]), \"Cycle does not close.\"\n\n@given(st.data())\ndef test_no_cycle_in_acyclic_graph_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1, unique=True)))\n    G = nx.dag.transitive_closure(G)  # Ensure it's acyclic\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.sampled_from([None, \"original\", \"reverse\", \"ignore\"]))\n    \n    cycle = nx.algorithms.cycles.find_cycle(G, source=source, orientation=orientation)\n    \n    assert cycle == [], \"Expected an empty cycle for an acyclic graph.\"\n\n@given(st.data())\ndef test_cycle_length_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=3)))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.sampled_from([None, \"original\", \"reverse\", \"ignore\"]))\n    \n    cycle = nx.algorithms.cycles.find_cycle(G, source=source, orientation=orientation)\n    \n    if cycle:\n        assert len(cycle) >= 3, \"Cycle must have at least 3 edges.\"\n\n@given(st.data())\ndef test_cycle_edges_exist_in_graph_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.sampled_from([None, \"original\", \"reverse\", \"ignore\"]))\n    \n    cycle = nx.algorithms.cycles.find_cycle(G, source=source, orientation=orientation)\n    \n    if cycle:\n        for edge in cycle:\n            assert edge in G.edges, \"Cycle contains an edge not in the original graph.\"\n\n@given(st.data())\ndef test_reverse_orientation_cycle_property(data):\n    G = data.draw(st.builds(nx.DiGraph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = \"reverse\"\n    \n    cycle = nx.algorithms.cycles.find_cycle(G, source=source, orientation=orientation)\n    \n    if cycle:\n        for edge in cycle:\n            assert edge[::-1] in G.edges, \"The reverse cycle does not correspond to the edges in the original graph.\"\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_cycle_forms_closed_loop_property(data):\n    G = data.draw(st.one_of(st.graphs()))\n    source = data.draw(st.one_of([None, st.nodes(G)]))\n    orientation = data.draw(st.sampled_from([None, \"original\", \"reverse\", \"ignore\"]))\n    cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n    \n    if cycle:\n        first_edge = cycle[0]\n        last_edge = cycle[-1]\n        assert last_edge[1] == first_edge[0]  # last node connects back to the first node\n\n@given(st.data())\ndef test_acyclic_graph_returns_empty_list_property(data):\n    G = data.draw(st.graphs(directed=True, nodes=st.integers(min_value=0, max_value=100), edges=0))\n    source = data.draw(st.one_of([None, st.nodes(G)]))\n    orientation = data.draw(st.sampled_from([None, \"original\", \"reverse\", \"ignore\"]))\n    cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n    \n    assert cycle == []  # No cycle should be found\n\n@given(st.data())\ndef test_found_cycle_has_minimum_length_property(data):\n    G = data.draw(st.random_graphs())\n    source = data.draw(st.one_of([None, st.nodes(G)]))\n    orientation = data.draw(st.sampled_from([None, \"original\", \"reverse\", \"ignore\"]))\n    cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n    \n    if cycle:\n        assert len(cycle) >= 3  # A cycle must have at least 3 edges\n\n@given(st.data())\ndef test_cycle_edges_are_valid_edges_property(data):\n    G = data.draw(st.random_graphs())\n    source = data.draw(st.one_of([None, st.nodes(G)]))\n    orientation = data.draw(st.sampled_from([None, \"original\", \"reverse\", \"ignore\"]))\n    cycle = nx.algorithms.cycles.find_cycle(G, source, orientation)\n    \n    for edge in cycle:\n        assert edge in G.edges()  # All edges in the cycle must be valid edges of the graph\n\n@given(st.data())\ndef test_reverse_orientation_changes_cycle_direction_property(data):\n    G = data.draw(st.random_graphs())\n    source = data.draw(st.one_of([None, st.nodes(G)]))\n    cycle_original = nx.algorithms.cycles.find_cycle(G, source, orientation=None)\n    cycle_reverse = nx.algorithms.cycles.find_cycle(G, source, orientation=\"reverse\")\n    \n    if cycle_original:\n        assert len(cycle_original) == len(cycle_reverse)  # Both cycles should have the same length\n        assert all(edge[::-1] in cycle_reverse for edge in cycle_original)  # Reverse of each edge must be in the reverse cycle\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.cycles import find_cycle\n\n@given(st.data())\ndef test_cycle_is_closed_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.sets(st.tuples(st.integers(), st.integers()))))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.one_of(st.none(), st.sampled_from([\"original\", \"reverse\", \"ignore\"])))\n    \n    cycle = find_cycle(G, source=source, orientation=orientation)\n    \n    if cycle:\n        # Check if the first and last nodes of the cycle are connected\n        tail, head = cycle[0][0], cycle[-1][1]\n        assert tail == head\n\n@given(st.data())\ndef test_empty_cycle_for_acyclic_graph_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.sets(st.tuples(st.integers(), st.integers()))))\n    if nx.is_directed_acyclic_graph(G):\n        source = data.draw(st.one_of(st.none(), st.integers()))\n        orientation = data.draw(st.one_of(st.none(), st.sampled_from([\"original\", \"reverse\", \"ignore\"])))\n        \n        cycle = find_cycle(G, source=source, orientation=orientation)\n        assert cycle == []\n\n@given(st.data())\ndef test_cycle_length_property(data):\n    G = data.draw(st.builds(nx.DiGraph, edges=st.sets(st.tuples(st.integers(), st.integers()))))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.one_of(st.none(), st.sampled_from([\"original\", \"reverse\", \"ignore\"])))\n    \n    cycle = find_cycle(G, source=source, orientation=orientation)\n    \n    if cycle and len(cycle) >= 3:\n        # Check that the cycle length is at least 3\n        assert len(cycle) >= 3\n\n@given(st.data())\ndef test_edges_in_cycle_are_valid_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.sets(st.tuples(st.integers(), st.integers()))))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    orientation = data.draw(st.one_of(st.none(), st.sampled_from([\"original\", \"reverse\", \"ignore\"])))\n    \n    cycle = find_cycle(G, source=source, orientation=orientation)\n    \n    for edge in cycle:\n        assert edge in G.edges\n\n@given(st.data())\ndef test_reverse_orientation_property(data):\n    G = data.draw(st.builds(nx.DiGraph, edges=st.sets(st.tuples(st.integers(), st.integers()))))\n    source = data.draw(st.one_of(st.none(), st.integers()))\n    \n    cycle_original = find_cycle(G, source=source, orientation=\"original\")\n    cycle_reverse = find_cycle(G, source=source, orientation=\"reverse\")\n    \n    # Reverse the edges of the original cycle and check if it matches the reversed cycle\n    if cycle_original:\n        reversed_cycle = [(edge[1], edge[0]) for edge in cycle_original]\n        assert sorted(reversed_cycle) == sorted(cycle_reverse)\n\n# End program\n```"], "properties": ["1. The output cycle must consist of edges that form a closed loop, meaning the last node in the cycle must connect back to the first node in the cycle.\n\n2. If the input graph is acyclic (i.e., it contains no cycles), the output should be an empty list.\n\n3. The length of the output cycle must be greater than or equal to 3 if a cycle is found, as a cycle requires at least three nodes to connect back to the starting node.\n\n4. The output cycle must only include edges that exist in the input graph, ensuring that all edges in the cycle are valid edges of the graph.\n\n5. If the orientation is set to \"reverse\", the output cycle must reflect the reversed direction of the edges compared to the original graph's edges."], "api_doc": "find_cycle\nfind_cycle(G, source=None, orientation=None)[source]\nReturns a cycle found via depth-first traversal.\n\nThe cycle is a list of edges indicating the cyclic path. Orientation of directed edges is controlled by orientation.\n\nParameters\n:\nG\ngraph\nA directed/undirected graph/multigraph.\n\nsource\nnode, list of nodes\nThe node from which the traversal begins. If None, then a source is chosen arbitrarily and repeatedly until all edges from each node in the graph are searched.\n\norientation\nNone | \u2018original\u2019 | \u2018reverse\u2019 | \u2018ignore\u2019 (default: None)\nFor directed graphs and directed multigraphs, edge traversals need not respect the original orientation of the edges. When set to \u2018reverse\u2019 every edge is traversed in the reverse direction. When set to \u2018ignore\u2019, every edge is treated as undirected. When set to \u2018original\u2019, every edge is treated as directed. In all three cases, the yielded edge tuples add a last entry to indicate the direction in which that edge was traversed. If orientation is None, the yielded edge has no direction indicated. The direction is respected, but not reported.\n\nReturns\n:\nedges\ndirected edges\nA list of directed edges indicating the path taken for the loop. If no cycle is found, then an exception is raised. For graphs, an edge is of the form (u, v) where u and v are the tail and head of the edge as determined by the traversal. For multigraphs, an edge is of the form (u, v, key), where key is the key of the edge. When the graph is directed, then u and v are always in the order of the actual directed edge. If orientation is not None then the edge tuple is extended to include the direction of traversal (\u2018forward\u2019 or \u2018reverse\u2019) on that edge.\n\nRaises\n:\nNetworkXNoCycle\nIf no cycle was found.\n\nSee also\n\nsimple_cycles\nExamples\n\nIn this example, we construct a DAG and find, in the first call, that there are no directed cycles, and so an exception is raised. In the second call, we ignore edge orientations and find that there is an undirected cycle. Note that the second call finds a directed cycle while effectively traversing an undirected graph, and so, we found an \u201cundirected cycle\u201d. This means that this DAG structure does not form a directed tree (which is also known as a polytree).\n\nG = nx.DiGraph([(0, 1), (0, 2), (1, 2)])\nnx.find_cycle(G, orientation=\"original\")\nTraceback (most recent call last):\n    ...\nnetworkx.exception.NetworkXNoCycle: No cycle found.\nlist(nx.find_cycle(G, orientation=\"ignore\"))\n[(0, 1, 'forward'), (1, 2, 'forward'), (0, 2, 'reverse')]", "api_code": "@nx._dispatchable\ndef find_cycle(G, source=None, orientation=None):\n    if not G.is_directed() or orientation in (None, \"original\"):\n\n        def tailhead(edge):\n            return edge[:2]\n\n    elif orientation == \"reverse\":\n\n        def tailhead(edge):\n            return edge[1], edge[0]\n\n    elif orientation == \"ignore\":\n\n        def tailhead(edge):\n            if edge[-1] == \"reverse\":\n                return edge[1], edge[0]\n            return edge[:2]\n\n    explored = set()\n    cycle = []\n    final_node = None\n    for start_node in G.nbunch_iter(source):\n        if start_node in explored:\n            # No loop is possible.\n            continue\n\n        edges = []\n        # All nodes seen in this iteration of edge_dfs\n        seen = {start_node}\n        # Nodes in active path.\n        active_nodes = {start_node}\n        previous_head = None\n\n        for edge in nx.edge_dfs(G, start_node, orientation):\n            # Determine if this edge is a continuation of the active path.\n            tail, head = tailhead(edge)\n            if head in explored:\n                # Then we've already explored it. No loop is possible.\n                continue\n            if previous_head is not None and tail != previous_head:\n                # This edge results from backtracking.\n                # Pop until we get a node whose head equals the current tail.\n                # So for example, we might have:\n                #  (0, 1), (1, 2), (2, 3), (1, 4)\n                # which must become:\n                #  (0, 1), (1, 4)\n                while True:\n                    try:\n                        popped_edge = edges.pop()\n                    except IndexError:\n                        edges = []\n                        active_nodes = {tail}\n                        break\n                    else:\n                        popped_head = tailhead(popped_edge)[1]\n                        active_nodes.remove(popped_head)\n\n                    if edges:\n                        last_head = tailhead(edges[-1])[1]\n                        if tail == last_head:\n                            break\n            edges.append(edge)\n\n            if head in active_nodes:\n                # We have a loop!\n                cycle.extend(edges)\n                final_node = head\n                break\n            else:\n                seen.add(head)\n                active_nodes.add(head)\n                previous_head = head\n\n        if cycle:\n            break\n        else:\n            explored.update(seen)\n\n    else:\n        assert len(cycle) == 0\n        raise nx.exception.NetworkXNoCycle(\"No cycle found.\")\n\n    # We now have a list of edges which ends on a cycle.\n    # So we need to remove from the beginning edges that are not relevant.\n\n    for i, edge in enumerate(cycle):\n        tail, head = tailhead(edge)\n        if tail == final_node:\n            break\n\n    return cycle[i:]"}
{"function_name": "networkx.algorithms.approximation.dominating_set.min_weighted_dominating_set", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.lists(st.tuples(st.integers(), st.integers(), st.integers()), min_size=1).map(lambda edges: nx.Graph(edges)))\ndef test_output_covers_all_vertices_property(G):\n    dom_set = min_weighted_dominating_set(G)\n    assert all(v in dom_set or any(neighbor in dom_set for neighbor in G[v]) for v in G)\n\n@given(st.lists(st.tuples(st.integers(), st.integers(), st.integers()), min_size=1).map(lambda edges: nx.Graph(edges)))\ndef test_output_subset_of_input_vertices_property(G):\n    dom_set = min_weighted_dominating_set(G)\n    assert dom_set.issubset(G.nodes)\n\n@given(st.lists(st.tuples(st.integers(), st.integers(), st.integers()), min_size=1).map(lambda edges: nx.Graph(edges)))\ndef test_minimized_size_of_output_dominating_set_property(G):\n    dom_set = min_weighted_dominating_set(G)\n    for size in range(1, len(G.nodes) + 1):\n        smaller_set = set(list(G.nodes)[:size])\n        if all(v in smaller_set or any(neighbor in smaller_set for neighbor in G[v]) for v in G):\n            assert len(smaller_set) >= len(dom_set)\n\n@given(st.lists(st.tuples(st.integers(), st.integers(), st.integers()), min_size=1).map(lambda edges: nx.Graph(edges)))\ndef test_empty_graph_output_property():\n    G = nx.Graph()\n    dom_set = min_weighted_dominating_set(G)\n    assert dom_set == set()\n\n@given(st.lists(st.tuples(st.integers(), st.integers(), st.integers()), min_size=1).map(lambda edges: nx.Graph(edges)))\ndef test_non_increasing_cost_effectiveness_property(G):\n    dom_set = min_weighted_dominating_set(G)\n    neighborhoods = {v: {v} | set(G[v]) for v in G}\n    \n    costs = []\n    for node in dom_set:\n        neighborhood = neighborhoods[node]\n        cost = G.nodes[node].get('weight', 1) / len(neighborhood - dom_set)\n        costs.append(cost)\n    \n    assert all(costs[i] >= costs[i + 1] for i in range(len(costs) - 1))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_covers_all_vertices_property(data):\n    G = data.draw(st.builds(nx.graphs.random_graphs.gnm_random_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             m=st.integers(min_value=1, max_value=1000)))\n    weight_attr = st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1)))\n    weight = data.draw(weight_attr)\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight=weight)\n    \n    for v in G.nodes:\n        assert v in dom_set or any(neighbor in dom_set for neighbor in G[v])\n\n@given(st.data())\ndef test_output_is_subset_of_input_property(data):\n    G = data.draw(st.builds(nx.graphs.random_graphs.gnm_random_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             m=st.integers(min_value=1, max_value=1000)))\n    weight_attr = st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1)))\n    weight = data.draw(weight_attr)\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight=weight)\n    \n    assert dom_set.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_minimizes_size_property(data):\n    G = data.draw(st.builds(nx.graphs.random_graphs.gnm_random_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             m=st.integers(min_value=1, max_value=1000)))\n    weight_attr = st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1)))\n    weight = data.draw(weight_attr)\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight=weight)\n    \n    # Check that no smaller dominating set exists (this is a simplified check)\n    for node in G.nodes:\n        smaller_set = dom_set - {node}\n        if all(v in smaller_set or any(neighbor in smaller_set for neighbor in G[v]) for v in G.nodes):\n            assert False  # Found a smaller dominating set\n\n@given(st.data())\ndef test_empty_graph_output_property(data):\n    G = nx.Graph()  # Empty graph\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G)\n    \n    assert dom_set == set()\n\n@given(st.data())\ndef test_cost_effectiveness_non_increasing_property(data):\n    G = data.draw(st.builds(nx.graphs.random_graphs.gnm_random_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             m=st.integers(min_value=1, max_value=1000)))\n    weight_attr = st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1)))\n    weight = data.draw(weight_attr)\n    \n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight=weight)\n    \n    cost_effectiveness = []\n    neighborhoods = {v: {v} | set(G[v]) for v in G}\n    \n    for node in dom_set:\n        neighborhood = neighborhoods[node]\n        cost = G.nodes[node].get(weight, 1) / len(neighborhood - dom_set)\n        cost_effectiveness.append(cost)\n    \n    assert all(cost_effectiveness[i] >= cost_effectiveness[i + 1] for i in range(len(cost_effectiveness) - 1))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_min_weighted_dominating_set_covers_all_vertices_property():\n    G = st.builds(nx.empty_graph, n=st.integers(min_value=0, max_value=100)).example()\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G)\n    assert all(v in dom_set or any(adj in dom_set for adj in G[v]) for v in G)\n\n@given(st.data())\ndef test_min_weighted_dominating_set_subset_of_input_property():\n    G = st.builds(nx.empty_graph, n=st.integers(min_value=0, max_value=100)).example()\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G)\n    assert dom_set.issubset(G.nodes)\n\n@given(st.data())\ndef test_min_weighted_dominating_set_minimized_size_property():\n    G = st.builds(nx.empty_graph, n=st.integers(min_value=0, max_value=100)).example()\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G)\n    for potential_set in st.lists(st.sampled_from(G.nodes), min_size=len(dom_set)-1, max_size=len(G.nodes)).example():\n        if all(v in potential_set or any(adj in potential_set for adj in G[v]) for v in G):\n            assert len(potential_set) >= len(dom_set)\n\n@given(st.data())\ndef test_min_weighted_dominating_set_empty_graph_property():\n    G = nx.empty_graph(0)\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G)\n    assert dom_set == set()\n\n@given(st.data())\ndef test_min_weighted_dominating_set_non_increasing_cost_effectiveness_property():\n    G = st.builds(nx.empty_graph, n=st.integers(min_value=0, max_value=100)).example()\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G)\n    costs = []\n    for node in dom_set:\n        neighborhood = {node} | set(G[node])\n        cost = G.nodes[node].get('weight', 1) / len(neighborhood - dom_set)\n        costs.append(cost)\n    assert all(costs[i] >= costs[i+1] for i in range(len(costs) - 1))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_dominating_set_covers_all_vertices_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    weight_attr = st.one_of(st.none(), st.lists(st.integers(), min_size=len(G.nodes())))\n    if weight_attr is not None:\n        for idx, node in enumerate(G.nodes()):\n            G.nodes[node]['weight'] = weight_attr[idx] if idx < len(weight_attr) else 1\n    dom_set = min_weighted_dominating_set(G, weight='weight')\n    assert all(v in dom_set or any(adj in dom_set for adj in G[v]) for v in G)\n\n@given(st.data())\ndef test_dominating_set_subset_of_nodes_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    weight_attr = st.one_of(st.none(), st.lists(st.integers(), min_size=len(G.nodes())))\n    if weight_attr is not None:\n        for idx, node in enumerate(G.nodes()):\n            G.nodes[node]['weight'] = weight_attr[idx] if idx < len(weight_attr) else 1\n    dom_set = min_weighted_dominating_set(G, weight='weight')\n    assert dom_set.issubset(G.nodes())\n\n@given(st.data())\ndef test_dominating_set_minimized_size_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    weight_attr = st.one_of(st.none(), st.lists(st.integers(), min_size=len(G.nodes())))\n    if weight_attr is not None:\n        for idx, node in enumerate(G.nodes()):\n            G.nodes[node]['weight'] = weight_attr[idx] if idx < len(weight_attr) else 1\n    dom_set = min_weighted_dominating_set(G, weight='weight')\n    # Check against any smaller dominating set (not trivial, requires more context)\n    # Placeholder assertion: details depend on specific graph structure\n    assert len(dom_set) <= len(G.nodes())\n\n@given(st.data())\ndef test_empty_graph_yields_empty_dominating_set_property(data):\n    G = nx.Graph()  # Empty graph\n    dom_set = min_weighted_dominating_set(G)\n    assert dom_set == set()\n\n@given(st.data())\ndef test_non_increasing_cost_effectiveness_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    weight_attr = st.one_of(st.none(), st.lists(st.integers(), min_size=len(G.nodes())))\n    if weight_attr is not None:\n        for idx, node in enumerate(G.nodes()):\n            G.nodes[node]['weight'] = weight_attr[idx] if idx < len(weight_attr) else 1\n    dom_set = min_weighted_dominating_set(G, weight='weight')\n    costs = []\n    neighborhoods = {v: {v} | set(G[v]) for v in G}\n    \n    for node in dom_set:\n        neighborhood = neighborhoods[node]\n        cost = G.nodes[node].get('weight', 1) / len(neighborhood - dom_set)\n        costs.append(cost)\n    \n    assert all(costs[i] >= costs[i + 1] for i in range(len(costs) - 1))\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_covers_all_vertices_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    weight = data.draw(st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1))))\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight)\n    for node in G.nodes:\n        assert node in dom_set or any(neighbor in dom_set for neighbor in G.neighbors(node))\n\n@given(st.data())\ndef test_output_is_subset_of_input_vertices_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    weight = data.draw(st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1))))\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight)\n    assert dom_set.issubset(G.nodes)\n\n@given(st.data())\ndef test_size_of_output_is_minimized_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    weight = data.draw(st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1))))\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight)\n    for candidate in st.lists(st.integers(), min_size=1).example():\n        if all(node in G.nodes for node in candidate):\n            candidate_set = set(candidate)\n            if all(node in candidate_set or any(neighbor in candidate_set for neighbor in G.neighbors(node)) for node in G.nodes):\n                assert len(dom_set) <= len(candidate_set)\n\n@given(st.data())\ndef test_empty_graph_output_is_empty_set_property(data):\n    G = nx.Graph()  # Empty graph\n    weight = data.draw(st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1))))\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight)\n    assert dom_set == set()\n\n@given(st.data())\ndef test_non_increasing_cost_effectiveness_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    weight = data.draw(st.one_of(st.none(), st.dictionaries(st.integers(), st.integers(min_value=1))))\n    dom_set = nx.algorithms.approximation.dominating_set.min_weighted_dominating_set(G, weight)\n    \n    # Check cost-effectiveness\n    costs = []\n    for node in dom_set:\n        neighborhood = {node} | set(G.neighbors(node))\n        cost_effectiveness = G.nodes[node].get(weight, 1) / len(neighborhood - dom_set)\n        costs.append(cost_effectiveness)\n\n    assert all(costs[i] >= costs[i + 1] for i in range(len(costs) - 1))\n\n# End program\n```"], "properties": ["1. The output dominating set must cover all vertices in the graph, meaning every vertex should either be in the dominating set or adjacent to at least one vertex in the dominating set.\n\n2. The output dominating set should be a subset of the input graph's vertices, ensuring that all nodes in the dominating set are valid nodes from the input graph.\n\n3. The size of the output dominating set should be minimized, meaning that there should not exist a smaller dominating set that covers all vertices in the graph.\n\n4. If the input graph is empty, the output should also be an empty set, reflecting the fact that there are no vertices to dominate.\n\n5. The cost-effectiveness of nodes added to the dominating set should be non-increasing; that is, each subsequent node added should have a cost-effectiveness that is less than or equal to that of the previously added node, ensuring a greedy approach is maintained throughout the process."], "api_doc": "min_weighted_dominating_set\nmin_weighted_dominating_set(G, weight=None)[source]\nReturns a dominating set that approximates the minimum weight node dominating set.\n\nParameters\n:\nG\nNetworkX graph\nUndirected graph.\n\nweight\nstring\nThe node attribute storing the weight of an node. If provided, the node attribute with this key must be a number for each node. If not provided, each node is assumed to have weight one.\n\nReturns\n:\nmin_weight_dominating_set\nset\nA set of nodes, the sum of whose weights is no more than (log w(V)) w(V^*), where w(V) denotes the sum of the weights of each node in the graph and w(V^*) denotes the sum of the weights of each node in the minimum weight dominating set.\n\nRaises\n:\nNetworkXNotImplemented\nIf G is directed.\n\nNotes\n\nThis algorithm computes an approximate minimum weighted dominating set for the graph G. The returned solution has weight (log w(V)) w(V^*), where w(V) denotes the sum of the weights of each node in the graph and w(V^*) denotes the sum of the weights of each node in the minimum weight dominating set for the graph.\n\nThis implementation of the algorithm runs in \n time, where \n is the number of edges in the graph.\n\nReferences\n\n[1]\nVazirani, Vijay V. Approximation Algorithms. Springer Science & Business Media, 2001.\n\nExamples\n\nG = nx.Graph([(0, 1), (0, 4), (1, 4), (1, 2), (2, 3), (3, 4), (2, 5)])\nnx.approximation.min_weighted_dominating_set(G)\n{1, 2, 4}", "api_code": "@not_implemented_for(\"directed\")\n@nx._dispatchable(node_attrs=\"weight\")\ndef min_weighted_dominating_set(G, weight=None):\n    # The unique dominating set for the null graph is the empty set.\n    if len(G) == 0:\n        return set()\n\n    # This is the dominating set that will eventually be returned.\n    dom_set = set()\n\n    def _cost(node_and_neighborhood):\n        \"\"\"Returns the cost-effectiveness of greedily choosing the given\n        node.\n\n        `node_and_neighborhood` is a two-tuple comprising a node and its\n        closed neighborhood.\n\n        \"\"\"\n        v, neighborhood = node_and_neighborhood\n        return G.nodes[v].get(weight, 1) / len(neighborhood - dom_set)\n\n    # This is a set of all vertices not already covered by the\n    # dominating set.\n    vertices = set(G)\n    # This is a dictionary mapping each node to the closed neighborhood\n    # of that node.\n    neighborhoods = {v: {v} | set(G[v]) for v in G}\n\n    # Continue until all vertices are adjacent to some node in the\n    # dominating set.\n    while vertices:\n        # Find the most cost-effective node to add, along with its\n        # closed neighborhood.\n        dom_node, min_set = min(neighborhoods.items(), key=_cost)\n        # Add the node to the dominating set and reduce the remaining\n        # set of nodes to cover.\n        dom_set.add(dom_node)\n        del neighborhoods[dom_node]\n        vertices -= min_set\n\n    return dom_set"}
{"function_name": "datetime.timedelta.total_seconds", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.integers(min_value=0), st.integers(), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_non_negative_property(days, seconds, microseconds):\n    result = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds).total_seconds()\n    assert result >= 0\n\n@given(st.integers(min_value=0), st.integers(min_value=0, max_value=999999), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_zero_input_property(days, seconds, microseconds):\n    assert days == 0 and seconds == 0 and microseconds == 0 or \\\n           datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds).total_seconds() == 0\n\n@given(st.integers(min_value=0), st.integers(), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_proportionality_property(days, seconds, microseconds):\n    dt = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    result = dt.total_seconds()\n    assert result == dt.total_seconds() * 2 if (days * 2, seconds * 2, microseconds) else result\n\n@given(st.integers(), st.integers(), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_deterministic_property(days, seconds, microseconds):\n    dt = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    result1 = dt.total_seconds()\n    result2 = dt.total_seconds()\n    assert result1 == result2\n\n@given(st.integers(), st.integers(), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_correct_conversion_property(days, seconds, microseconds):\n    dt = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    expected_result = (days * 86400) + seconds + (microseconds / 10**6)\n    assert dt.total_seconds() == expected_result\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pytest\nfrom datetime import timedelta\n\n@given(days=st.integers(min_value=0), \n       seconds=st.integers(min_value=0, max_value=59), \n       microseconds=st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_non_negative_property(days, seconds, microseconds):\n    td = timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    assert td.total_seconds() >= 0\n\n@given(days=st.integers(min_value=0), \n       seconds=st.integers(min_value=0, max_value=59), \n       microseconds=st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_zero_property(days, seconds, microseconds):\n    td = timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    if days == 0 and seconds == 0 and microseconds == 0:\n        assert td.total_seconds() == 0\n\n@given(days=st.integers(min_value=0, max_value=1000), \n       seconds=st.integers(min_value=0, max_value=59), \n       microseconds=st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_proportionality_property(days, seconds, microseconds):\n    td = timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    td_double = timedelta(days=days * 2, seconds=seconds * 2, microseconds=microseconds * 2)\n    assert td_double.total_seconds() == 2 * td.total_seconds()\n\n@given(days=st.integers(), \n       seconds=st.integers(min_value=0, max_value=59), \n       microseconds=st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_deterministic_property(days, seconds, microseconds):\n    td = timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    assert td.total_seconds() == td.total_seconds()  # Should be consistent across calls\n\n@given(days=st.integers(min_value=0, max_value=1000), \n       seconds=st.integers(min_value=0, max_value=59), \n       microseconds=st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_correct_accounting_property(days, seconds, microseconds):\n    td = timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    expected_total_seconds = (days * 86400) + seconds + (microseconds / 10**6)\n    assert td.total_seconds() == expected_total_seconds\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.integers(min_value=0), st.integers(min_value=0, max_value=86400), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_non_negative_property(days, seconds, microseconds):\n    td = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    assert td.total_seconds() >= 0\n\n@given(st.integers(min_value=0))\ndef test_total_seconds_zero_property(value):\n    td = datetime.timedelta(days=0, seconds=0, microseconds=0)\n    assert td.total_seconds() == 0\n\n@given(st.integers(min_value=0), st.integers(min_value=0, max_value=86400), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_proportionality_property(days, seconds, microseconds):\n    td = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    assert td.total_seconds() == (days * 86400 + seconds + microseconds / 10**6)\n\n@given(st.integers())\ndef test_total_seconds_deterministic_property(value):\n    td = datetime.timedelta(days=value, seconds=value, microseconds=value)\n    assert td.total_seconds() == td.total_seconds()\n\n@given(st.integers(min_value=0), st.integers(min_value=0, max_value=86400), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_correct_conversion_property(days, seconds, microseconds):\n    td = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    expected_seconds = days * 86400 + seconds + microseconds / 10**6\n    assert td.total_seconds() == expected_seconds\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.integers(min_value=0, max_value=10**6), \n       st.integers(min_value=0, max_value=59), \n       st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_non_negative_property(days, seconds, microseconds):\n    td = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    assert td.total_seconds() >= 0\n\n@given(st.integers(min_value=0, max_value=10**6), \n       st.integers(min_value=0, max_value=59), \n       st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_zero_property(days, seconds, microseconds):\n    td = datetime.timedelta(days=0, seconds=0, microseconds=0)\n    assert td.total_seconds() == 0\n\n@given(st.integers(min_value=0, max_value=10**6), \n       st.integers(min_value=0, max_value=59), \n       st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_proportional_property(days, seconds, microseconds):\n    td = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    doubled_td = datetime.timedelta(days=days * 2, seconds=seconds * 2, microseconds=microseconds * 2)\n    assert td.total_seconds() * 2 == doubled_td.total_seconds()\n\n@given(st.integers(min_value=0, max_value=10**6), \n       st.integers(min_value=0, max_value=59), \n       st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_deterministic_property(days, seconds, microseconds):\n    td = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    assert td.total_seconds() == td.total_seconds()\n\n@given(st.integers(min_value=0, max_value=10**6), \n       st.integers(min_value=0, max_value=59), \n       st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_correct_accounting_property(days, seconds, microseconds):\n    td = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds)\n    expected_total_seconds = (days * 86400) + seconds + (microseconds / 10**6)\n    assert td.total_seconds() == expected_total_seconds\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.integers(min_value=0, max_value=10**6), st.integers(min_value=0, max_value=59), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_non_negative_property(days, seconds, microseconds):\n    total = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds).total_seconds()\n    assert total >= 0\n\n@given(st.integers())\ndef test_total_seconds_zero_property(zero_input):\n    total = datetime.timedelta(days=0, seconds=0, microseconds=0).total_seconds()\n    assert total == 0\n\n@given(st.integers(min_value=0, max_value=10**6), st.integers(min_value=0, max_value=59), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_proportionality_property(days, seconds, microseconds):\n    total_original = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds).total_seconds()\n    total_doubled = datetime.timedelta(days=days * 2, seconds=seconds * 2, microseconds=microseconds * 2).total_seconds()\n    assert total_doubled == total_original * 2\n\n@given(st.integers(min_value=0, max_value=10**6), st.integers(min_value=0, max_value=59), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_deterministic_property(days, seconds, microseconds):\n    total_first_call = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds).total_seconds()\n    total_second_call = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds).total_seconds()\n    assert total_first_call == total_second_call\n\n@given(st.integers(min_value=0, max_value=10**6), st.integers(min_value=0, max_value=59), st.integers(min_value=0, max_value=999999))\ndef test_total_seconds_correct_conversion_property(days, seconds, microseconds):\n    total = datetime.timedelta(days=days, seconds=seconds, microseconds=microseconds).total_seconds()\n    expected_total = days * 86400 + seconds + microseconds / 10**6\n    assert total == expected_total\n# End program\n```"], "properties": ["1. The output of `total_seconds` should always be a non-negative float, as it represents a duration of time.\n2. The output of `total_seconds` should equal zero if both `days`, `seconds`, and `microseconds` are all zero.\n3. The output of `total_seconds` should be proportional to the input values, meaning that doubling the input values (for `days`, `seconds`, and `microseconds`) should double the output.\n4. The output of `total_seconds` should be consistent across multiple calls with the same input values, ensuring that it is deterministic.\n5. The output of `total_seconds` should correctly account for the conversion of days to seconds, meaning that the output should equal the sum of `days * 86400`, `seconds`, and `microseconds / 10**6`."], "api_doc": "timedelta.total_seconds()\nReturn the total number of seconds contained in the duration. Equivalent to td / timedelta(seconds=1). For interval units other than seconds, use the division form directly (e.g. td / timedelta(microseconds=1)).\n\nNote that for very large time intervals (greater than 270 years on most platforms) this method will lose microsecond accuracy.\n\nAdded in version 3.2.\n\nExamples of usage: timedelta\nAn additional example of normalization:\n\n>>>\n# Components of another_year add up to exactly 365 days\nfrom datetime import timedelta\nyear = timedelta(days=365)\nanother_year = timedelta(weeks=40, days=84, hours=23,\n                         minutes=50, seconds=600)\nyear == another_year\nTrue\nyear.total_seconds()\n31536000.0", "api_code": "def total_seconds(self):\n        return ((self.days * 86400 + self.seconds) * 10**6 +\n                self.microseconds) / 10**6"}
{"function_name": "numpy.linalg.norm", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.floats(), max_size=1000))  # Testing vectors with float values\ndef test_norm_vector_non_negative_property(vector):\n    result = np.linalg.norm(vector, ord=2)\n    assert result >= 0\n    if all(v == 0 for v in vector):\n        assert result == 0\n\n@given(st.lists(st.lists(st.floats()), max_size=100, min_size=10, max_size=100))  # Testing matrices with float values\ndef test_norm_frobenius_non_negative_property(matrix):\n    result = np.linalg.norm(matrix, ord='fro')\n    assert result >= 0\n\n@given(st.lists(st.lists(st.floats()), max_size=100, min_size=10, max_size=100))  # Testing matrices with float values\ndef test_norm_inf_matrix_property(matrix):\n    result = np.linalg.norm(matrix, ord=np.inf)\n    assert result >= 0\n\n@given(st.lists(st.floats(), max_size=1000))  # Testing vectors with float values\ndef test_norm_zero_count_property(vector):\n    result = np.linalg.norm(vector, ord=0)\n    assert result == sum(1 for v in vector if v != 0)\n\n@given(st.lists(st.lists(st.floats()), max_size=100, min_size=10, max_size=100))  # Testing matrices with float values\ndef test_norm_unitary_invariance_property(matrix):\n    if len(matrix) == 0 or len(matrix[0]) == 0:\n        return  # Skip empty matrices\n    unitary_matrix = np.linalg.qr(np.random.rand(len(matrix), len(matrix[0])))[0]  # Generate a random unitary matrix\n    transformed_matrix = unitary_matrix @ matrix\n    original_norm = np.linalg.norm(matrix, ord=2)\n    transformed_norm = np.linalg.norm(transformed_matrix, ord=2)\n    assert np.isclose(original_norm, transformed_norm)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy.linalg import norm\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_vector_norm_non_negative_property(vec):\n    result = norm(vec, ord=2)\n    assert result >= 0\n    assert (result == 0) == all(v == 0 for v in vec)\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, max_size=100, max_len=100))\ndef test_frobenius_norm_property(mat):\n    mat_array = np.array(mat)\n    result = norm(mat_array, ord='fro')\n    assert result >= 0\n    expected = np.sqrt(np.sum(np.abs(mat_array)**2))\n    assert np.isclose(result, expected)\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, max_size=100, max_len=100))\ndef test_inf_norm_property(mat):\n    mat_array = np.array(mat)\n    result = norm(mat_array, ord=np.inf)\n    assert result >= 0\n    expected = np.max(np.sum(np.abs(mat_array), axis=1))\n    assert np.isclose(result, expected)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_zero_norm_property(vec):\n    result = norm(vec, ord=0)\n    count_non_zero = sum(v != 0 for v in vec)\n    assert result == count_non_zero\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, max_size=100, max_len=100))\ndef test_unitary_invariance_property(mat):\n    mat_array = np.array(mat)\n    # Generate a random unitary matrix\n    u, _, _ = np.linalg.svd(np.random.rand(2, 2))\n    transformed = u @ mat_array @ u.T\n    assert np.isclose(norm(mat_array), norm(transformed))\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy.linalg import norm\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_vector_norm_non_negative_property(vec):\n    result = norm(vec, ord=2)\n    assert result >= 0\n    assert (result == 0) == all(v == 0 for v in vec)\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, max_size=100, max_depth=2))\ndef test_frobenius_norm_non_negative_property(mat):\n    result = norm(mat, ord='fro')\n    assert result >= 0\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, max_size=100, max_depth=2))\ndef test_inf_norm_maximum_row_sum_property(mat):\n    result = norm(mat, ord=np.inf)\n    max_row_sum = max(np.sum(np.abs(row)) for row in mat)\n    assert result == max_row_sum\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_zero_norm_count_non_zero_elements_property(vec):\n    result = norm(vec, ord=0)\n    assert result == sum(1 for v in vec if v != 0)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_invariance_under_unitary_property(vec):\n    unitary_matrix = np.eye(len(vec))  # Identity matrix as a trivial unitary transformation\n    original_norm = norm(vec)\n    transformed_norm = norm(np.dot(unitary_matrix, vec))\n    assert original_norm == transformed_norm\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_vector_norm_non_negative_property(vector):\n    norm_value = np.linalg.norm(vector, ord=2)\n    assert norm_value >= 0\n    assert (norm_value == 0) == all(v == 0 for v in vector)\n\n@given(st.lists(st.lists(st.floats()), min_size=1, max_size=100, max_length=100), \n               st.booleans())\ndef test_frobenius_norm_non_negative_property(matrix, keepdims):\n    if len(matrix) == 0 or len(matrix[0]) == 0:\n        return  # Skip empty matrices\n    norm_value = np.linalg.norm(matrix, ord='fro', keepdims=keepdims)\n    assert norm_value >= 0\n\n@given(st.lists(st.lists(st.floats()), min_size=1, max_size=100, max_length=100), \n               st.booleans())\ndef test_matrix_inf_norm_non_negative_property(matrix, keepdims):\n    if len(matrix) == 0 or len(matrix[0]) == 0:\n        return  # Skip empty matrices\n    norm_value = np.linalg.norm(matrix, ord=np.inf, keepdims=keepdims)\n    assert norm_value >= 0\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_zero_norm_non_negative_integer_property(vector):\n    norm_value = np.linalg.norm(vector, ord=0)\n    assert isinstance(norm_value, int)\n    assert norm_value >= 0\n\n@given(st.lists(st.lists(st.floats(), min_size=1, max_size=100, max_length=100)),\n               st.lists(st.lists(st.floats()), min_size=1, max_size=100, max_length=100)))\ndef test_unitary_invariance_property(matrix1, matrix2):\n    if len(matrix1) == 0 or len(matrix1[0]) == 0 or len(matrix2) == 0 or len(matrix2[0]) == 0:\n        return  # Skip empty matrices\n    norm_before = np.linalg.norm(matrix1)\n    norm_after = np.linalg.norm(np.dot(matrix1, matrix2))\n    assert norm_before == norm_after\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy.linalg import norm\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_vector_norm_non_negative_property(vec):\n    result = norm(vec, ord=2)\n    assert result >= 0\n    if all(v == 0 for v in vec):\n        assert result == 0\n\n@given(st.matrices(st.floats(), min_rows=1, min_cols=1, max_rows=100, max_cols=100))\ndef test_matrix_frobenius_norm_property(mat):\n    result = norm(mat, ord='fro')\n    assert result >= 0\n    expected = np.sqrt(np.sum(np.square(mat)))\n    assert np.isclose(result, expected)\n\n@given(st.matrices(st.floats(), min_rows=1, min_cols=1, max_rows=100, max_cols=100))\ndef test_matrix_inf_norm_property(mat):\n    result = norm(mat, ord=np.inf)\n    assert result >= 0\n    expected = np.max(np.sum(np.abs(mat), axis=1))\n    assert np.isclose(result, expected)\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_vector_zero_norm_property(vec):\n    result = norm(vec, ord=0)\n    assert isinstance(result, int)\n    assert result >= 0\n    assert result == sum(1 for v in vec if v != 0)\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_unitary_transform_invariance_property(vec):\n    unitary_matrix = np.random.randn(len(vec), len(vec))\n    unitary_matrix, _ = np.linalg.qr(unitary_matrix)  # Ensure it's unitary\n    transformed_vec = unitary_matrix @ vec\n    result_original = norm(vec, ord=2)\n    result_transformed = norm(transformed_vec, ord=2)\n    assert np.isclose(result_original, result_transformed)\n# End program\n```"], "properties": ["1. The norm of a vector (1D array) with order 2 should always be non-negative and equal to zero if and only if the vector itself contains only zeros.\n\n2. The norm of a matrix (2D array) with order 'fro' (Frobenius norm) should be equal to the square root of the sum of the absolute squares of its elements, and should also be non-negative.\n\n3. The norm with order 'inf' for a matrix should return the maximum absolute row sum and should be non-negative.\n\n4. The norm with order 0 for a vector should return the count of non-zero elements and should be a non-negative integer.\n\n5. The norm should be invariant under unitary transformations, meaning that applying a unitary transformation to the input should not change the value of the norm."], "api_doc": "numpy.linalg.norm\nlinalg.norm(x, ord=None, axis=None, keepdims=False)[source]\nMatrix or vector norm.\n\nThis function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n\nParameters:\nxarray_like\nInput array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of x.ravel will be returned.\n\nord{non-zero int, inf, -inf, \u2018fro\u2019, \u2018nuc\u2019}, optional\nOrder of the norm (see table under Notes). inf means numpy\u2019s inf object. The default is None.\n\naxis{None, int, 2-tuple of ints}, optional.\nIf axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.\n\nNew in version 1.8.0.\n\nkeepdimsbool, optional\nIf this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n\nNew in version 1.10.0.\n\nReturns:\nn\nfloat or ndarray\nNorm of the matrix or vector(s).\n\nSee also\n\nscipy.linalg.norm\nSimilar function in SciPy.\n\nNotes\n\nFor values of ord < 1, the result is, strictly speaking, not a mathematical \u2018norm\u2019, but it may still be useful for various numerical purposes.\n\nThe following norms can be calculated:\n\nord\n\nnorm for matrices\n\nnorm for vectors\n\nNone\n\nFrobenius norm\n\n2-norm\n\n\u2018fro\u2019\n\nFrobenius norm\n\n\u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013\n\ninf\n\nmax(sum(abs(x), axis=1))\n\nmax(abs(x))\n\n-inf\n\nmin(sum(abs(x), axis=1))\n\nmin(abs(x))\n\n0\n\n\u2013\n\nsum(x != 0)\n\n1\n\nmax(sum(abs(x), axis=0))\n\nas below\n\n-1\n\nmin(sum(abs(x), axis=0))\n\nas below\n\n2\n\n2-norm (largest sing. value)\n\nas below\n\n-2\n\nsmallest singular value\n\nas below\n\nother\n\n\u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nThe Frobenius norm is given by [1]:\n\n\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when x.ndim != 2.\n\nReferences\n\n[1]\nG. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n\nExamples\n\nimport numpy as np\nfrom numpy import linalg as LA\na = np.arange(9) - 4\na\narray([-4, -3, -2, ...,  2,  3,  4])\nb = a.reshape((3, 3))\nb\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\nLA.norm(a)\n7.745966692414834\nLA.norm(b)\n7.745966692414834\nLA.norm(b, 'fro')\n7.745966692414834\nLA.norm(a, np.inf)\n4.0\nLA.norm(b, np.inf)\n9.0\nLA.norm(a, -np.inf)\n0.0\nLA.norm(b, -np.inf)\n2.0\nLA.norm(a, 1)\n20.0\nLA.norm(b, 1)\n7.0\nLA.norm(a, -1)\n-4.6566128774142013e-010\nLA.norm(b, -1)\n6.0\nLA.norm(a, 2)\n7.745966692414834\nLA.norm(b, 2)\n7.3484692283495345\nLA.norm(a, -2)\n0.0\nLA.norm(b, -2)\n1.8570331885190563e-016 # may vary\nLA.norm(a, 3)\n5.8480354764257312 # may vary\nLA.norm(a, -3)\n0.0\nUsing the axis argument to compute vector norms:\n\nc = np.array([[ 1, 2, 3],\n              [-1, 1, 4]])\nLA.norm(c, axis=0)\narray([ 1.41421356,  2.23606798,  5.        ])\nLA.norm(c, axis=1)\narray([ 3.74165739,  4.24264069])\nLA.norm(c, ord=1, axis=1)\narray([ 6.,  6.])\nUsing the axis argument to compute matrix norms:\n\nm = np.arange(8).reshape(2,2,2)\nLA.norm(m, axis=(1,2))\narray([  3.74165739,  11.22497216])\nLA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(3.7416573867739413, 11.224972160321824)", "api_code": "@array_function_dispatch(_norm_dispatcher)\ndef norm(x, ord=None, axis=None, keepdims=False):\n    x = asarray(x)\n\n    if not issubclass(x.dtype.type, (inexact, object_)):\n        x = x.astype(float)\n\n    # Immediately handle some default, simple, fast, and common cases.\n    if axis is None:\n        ndim = x.ndim\n        if (\n            (ord is None) or\n            (ord in ('f', 'fro') and ndim == 2) or\n            (ord == 2 and ndim == 1)\n        ):\n            x = x.ravel(order='K')\n            if isComplexType(x.dtype.type):\n                x_real = x.real\n                x_imag = x.imag\n                sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n            else:\n                sqnorm = x.dot(x)\n            ret = sqrt(sqnorm)\n            if keepdims:\n                ret = ret.reshape(ndim*[1])\n            return ret\n\n    # Normalize the `axis` argument to a tuple.\n    nd = x.ndim\n    if axis is None:\n        axis = tuple(range(nd))\n    elif not isinstance(axis, tuple):\n        try:\n            axis = int(axis)\n        except Exception as e:\n            raise TypeError(\n                \"'axis' must be None, an integer or a tuple of integers\"\n            ) from e\n        axis = (axis,)\n\n    if len(axis) == 1:\n        if ord == inf:\n            return abs(x).max(axis=axis, keepdims=keepdims)\n        elif ord == -inf:\n            return abs(x).min(axis=axis, keepdims=keepdims)\n        elif ord == 0:\n            # Zero norm\n            return (\n                (x != 0)\n                .astype(x.real.dtype)\n                .sum(axis=axis, keepdims=keepdims)\n            )\n        elif ord == 1:\n            # special case for speedup\n            return add.reduce(abs(x), axis=axis, keepdims=keepdims)\n        elif ord is None or ord == 2:\n            # special case for speedup\n            s = (x.conj() * x).real\n            return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n        # None of the str-type keywords for ord ('fro', 'nuc')\n        # are valid for vectors\n        elif isinstance(ord, str):\n            raise ValueError(f\"Invalid norm order '{ord}' for vectors\")\n        else:\n            absx = abs(x)\n            absx **= ord\n            ret = add.reduce(absx, axis=axis, keepdims=keepdims)\n            ret **= reciprocal(ord, dtype=ret.dtype)\n            return ret\n    elif len(axis) == 2:\n        row_axis, col_axis = axis\n        row_axis = normalize_axis_index(row_axis, nd)\n        col_axis = normalize_axis_index(col_axis, nd)\n        if row_axis == col_axis:\n            raise ValueError('Duplicate axes given.')\n        if ord == 2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amax)\n        elif ord == -2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amin)\n        elif ord == 1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).max(axis=col_axis)\n        elif ord == inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).max(axis=row_axis)\n        elif ord == -1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).min(axis=col_axis)\n        elif ord == -inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).min(axis=row_axis)\n        elif ord in [None, 'fro', 'f']:\n            ret = sqrt(add.reduce((x.conj() * x).real, axis=axis))\n        elif ord == 'nuc':\n            ret = _multi_svd_norm(x, row_axis, col_axis, sum)\n        else:\n            raise ValueError(\"Invalid norm order for matrices.\")\n        if keepdims:\n            ret_shape = list(x.shape)\n            ret_shape[axis[0]] = 1\n            ret_shape[axis[1]] = 1\n            ret = ret.reshape(ret_shape)\n        return ret\n    else:\n        raise ValueError(\"Improper number of dimensions to norm.\")"}
{"function_name": "decimal.Decimal.adjusted", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.data())\ndef test_output_is_integer_property(data):\n    @st.composite\n    def decimal_strategy(draw):\n        int_part = draw(st.text(min_size=1))\n        exp = draw(st.integers())\n        return Decimal(f\"{int_part}E{exp}\")\n    \n    decimal_value = data.draw(decimal_strategy())\n    result = decimal_value.adjusted()\n    assert isinstance(result, int)\n\n@given(st.data())\ndef test_adjusted_for_finite_values_property(data):\n    @st.composite\n    def finite_decimal_strategy(draw):\n        int_part = draw(st.text(min_size=1))\n        exp = draw(st.integers())\n        return Decimal(f\"{int_part}E{exp}\")\n    \n    decimal_value = data.draw(finite_decimal_strategy())\n    expected_result = decimal_value._exp + len(decimal_value._int) - 1\n    assert decimal_value.adjusted() == expected_result\n\n@given(st.data())\ndef test_adjusted_for_nan_infinity_property(data):\n    @st.sampled_from([Decimal('NaN'), Decimal('Infinity'), Decimal('-Infinity')])\n    def nan_infinity_strategy():\n        return data.draw(st.sampled_from([Decimal('NaN'), Decimal('Infinity'), Decimal('-Infinity')]))\n\n    decimal_value = nan_infinity_strategy()\n    assert decimal_value.adjusted() == 0\n\n@given(st.data())\ndef test_output_non_negative_for_finite_property(data):\n    @st.composite\n    def finite_decimal_strategy(draw):\n        int_part = draw(st.text(min_size=1))\n        exp = draw(st.integers())\n        return Decimal(f\"{int_part}E{exp}\")\n    \n    decimal_value = data.draw(finite_decimal_strategy())\n    result = decimal_value.adjusted()\n    assert result >= 0\n\n@given(st.data())\ndef test_output_consistency_with_integer_length_property(data):\n    @st.composite\n    def finite_decimal_strategy(draw):\n        int_part = draw(st.text(min_size=1))\n        exp = draw(st.integers())\n        return Decimal(f\"{int_part}E{exp}\")\n    \n    decimal_value = data.draw(finite_decimal_strategy())\n    \n    original_length = len(decimal_value._int)\n    new_int_part = '1' + decimal_value._int\n    decimal_value._int = new_int_part\n    \n    assert decimal_value.adjusted() == (decimal_value._exp + len(new_int_part) - 1)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals(allow_nan=False, allow_infinite=False))\ndef test_decimal_adjusted_output_is_integer_property(decimal_value):\n    result = decimal_value.adjusted()\n    assert isinstance(result, int)\n\n@given(st.decimals(allow_nan=False, allow_infinite=False))\ndef test_decimal_adjusted_finite_calculation_property(decimal_value):\n    if decimal_value.is_finite():\n        int_part_length = len(decimal_value._int)\n        result = decimal_value.adjusted()\n        assert result == (decimal_value._exp + int_part_length - 1)\n\n@given(st.decimals(allow_nan=True, allow_infinite=True))\ndef test_decimal_adjusted_nan_infinity_property(decimal_value):\n    if decimal_value.is_nan() or decimal_value.is_infinite():\n        result = decimal_value.adjusted()\n        assert result == 0\n\n@given(st.decimals(allow_nan=False, allow_infinite=False))\ndef test_decimal_adjusted_output_is_non_negative_property(decimal_value):\n    if decimal_value.is_finite():\n        result = decimal_value.adjusted()\n        assert result >= 0\n\n@given(st.decimals(allow_nan=False, allow_infinite=False))\ndef test_decimal_adjusted_consistent_with_changes_property(decimal_value):\n    original_exp = decimal_value._exp\n    original_int_length = len(decimal_value._int)\n    result_before = decimal_value.adjusted()\n\n    # Change the decimal's value to induce changes in exponent or integer part\n    decimal_value *= Decimal('10')  # Example of scaling\n    result_after = decimal_value.adjusted()\n\n    assert result_after != result_before  # Expect change due to the multiplication\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals(allow_nan=False, allow_infinity=False, max_value=Decimal('1e1000')))\ndef test_decimal_Decimal_adjusted_output_is_integer(decimal_value):\n    result = decimal_value.adjusted()\n    assert isinstance(result, int)\n# End program\n\n@given(st.decimals(allow_nan=False, allow_infinity=False, max_value=Decimal('1e1000')))\ndef test_decimal_Decimal_adjusted_finite_output_calculation(decimal_value):\n    if decimal_value.is_finite():\n        expected = decimal_value._exp + len(decimal_value._int) - 1\n        assert decimal_value.adjusted() == expected\n# End program\n\n@given(st.decimals(allow_nan=True, allow_infinity=True))\ndef test_decimal_Decimal_adjusted_nan_infinity_output(decimal_value):\n    if decimal_value.is_nan() or decimal_value.is_infinite():\n        assert decimal_value.adjusted() == 0\n# End program\n\n@given(st.decimals(allow_nan=False, allow_infinity=False, max_value=Decimal('1e1000')))\ndef test_decimal_Decimal_adjusted_finite_output_not_negative(decimal_value):\n    if decimal_value.is_finite():\n        result = decimal_value.adjusted()\n        assert result >= 0\n# End program\n\n@given(st.decimals(allow_nan=False, allow_infinity=False, max_value=Decimal('1e1000')))\ndef test_decimal_Decimal_adjusted_output_consistency(decimal_value):\n    initial_exp = decimal_value._exp\n    initial_int_length = len(decimal_value._int)\n    result_initial = decimal_value.adjusted()\n    \n    # Change the integer part\n    decimal_value._int += '1'\n    \n    result_updated = decimal_value.adjusted()\n    expected_updated = decimal_value._exp + len(decimal_value._int) - 1\n    assert result_updated == expected_updated\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_output_is_integer_property(decimal_value):\n    result = decimal_value.adjusted()\n    assert isinstance(result, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_correctly_calculates_finite_property(decimal_value):\n    int_part = str(decimal_value).split('.')[0]  # Get the integer part\n    exp = decimal_value._exp\n    expected_result = exp + len(int_part) - 1\n    assert decimal_value.adjusted() == expected_result\n\n@given(st.decimals(allow_nan=True, allow_infinity=True))\ndef test_decimal_Decimal_adjusted_nan_infinity_property(decimal_value):\n    if decimal_value.is_nan() or decimal_value.is_infinite():\n        assert decimal_value.adjusted() == 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_non_negative_property(decimal_value):\n    result = decimal_value.adjusted()\n    assert result >= 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_consistency_with_changes_property(decimal_value):\n    original_length = len(str(decimal_value).split('.')[0])\n    original_exp = decimal_value._exp\n    adjusted_before = decimal_value.adjusted()\n    \n    # Change the integer part by multiplying by 10\n    modified_decimal = decimal_value * Decimal(10)\n    adjusted_after = modified_decimal.adjusted()\n    \n    assert adjusted_after == adjusted_before + 1  # As the integer part length increases by 1\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_output_is_integer_property(decimal_value):\n    result = decimal_value.adjusted()\n    assert isinstance(result, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_finite_output_calculation_property(decimal_value):\n    exp = decimal_value._exp\n    int_part = len(decimal_value._int)\n    expected_result = exp + int_part - 1\n    assert decimal_value.adjusted() == expected_result\n\n@given(st.decimals(allow_nan=True, allow_infinity=True))\ndef test_decimal_Decimal_adjusted_nan_infinity_output_property(decimal_value):\n    if decimal_value.is_nan() or decimal_value.is_infinite():\n        assert decimal_value.adjusted() == 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_non_negative_output_property(decimal_value):\n    result = decimal_value.adjusted()\n    assert result >= 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_output_reflects_changes_property(decimal_value):\n    original_exp = decimal_value._exp\n    original_int_length = len(decimal_value._int)\n    result_before = decimal_value.adjusted()\n    \n    # Simulate a change in the integer part or exponent\n    decimal_value._int = '1' * (original_int_length + 1)  # Increase integer part length\n    decimal_value._exp += 1  # Change the exponent\n    result_after = decimal_value.adjusted()\n    \n    assert result_after != result_before  # Expect a change in the output\n\n# End program\n```"], "properties": ["1. The output of the `adjusted` function should be an integer value representing the adjusted exponent of the decimal number.\n\n2. If the decimal number is a finite value (not NaN or Infinity), the output should be equal to the exponent (`self._exp`) plus the length of the integer part (`self._int`) minus one.\n\n3. If the decimal number is NaN or Infinity, the output should always be 0, regardless of the values of `self._exp` and `self._int`.\n\n4. The output should never be negative when the decimal number is finite, since the exponent is adjusted based on the integer part's length.\n\n5. The output should consistently reflect changes in the integer part's length or the exponent for finite decimal numbers, meaning that if the integer part increases or the exponent changes, the output should update accordingly."], "api_doc": "adjusted()\nReturn the adjusted exponent after shifting out the coefficient\u2019s rightmost digits until only the lead digit remains: Decimal('321e+5').adjusted() returns seven. Used for determining the position of the most significant digit with respect to the decimal point.", "api_code": "def adjusted(self):\n    try:\n        return self._exp + len(self._int) - 1\n    # If NaN or Infinity, self._exp is string\n    except TypeError:\n        return 0"}
{"function_name": "html.escape", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_should_not_contain_special_characters_property(s):\n    result = html.escape(s, quote=True)\n    assert '&' not in result\n    assert '<' not in result\n    assert '>' not in result\n    assert '\"' not in result\n    assert \"'\" not in result\n\n@given(st.text())\ndef test_output_length_property(s):\n    result = html.escape(s, quote=True)\n    assert len(result) >= len(s)\n\n@given(st.text())\ndef test_output_is_html_safe_property(s):\n    result = html.escape(s, quote=True)\n    for char in ['&', '<', '>', '\"', \"'\"]:\n        assert result.count(char) == 0\n\n@given(st.text())\ndef test_empty_string_property(s):\n    if s == \"\":\n        result = html.escape(s, quote=True)\n        assert result == \"\"\n\n@given(st.text())\ndef test_order_of_characters_property(s):\n    result = html.escape(s, quote=True)\n    index = 0\n    for char in s:\n        if char in ['&', '<', '>', '\"', \"'\"]:\n            index += 1  # Skip replaced characters\n        assert result[index] == html.escape(char, quote=True)[0]\n        index += 1\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_should_not_contain_special_characters_property(input_str):\n    result = html.escape(input_str, quote=True)\n    assert \"&\" not in result\n    assert \"<\" not in result\n    assert \">\" not in result\n    assert '\"' not in result\n    assert \"'\" not in result\n\n@given(st.text())\ndef test_output_length_property(input_str):\n    result = html.escape(input_str, quote=True)\n    assert len(result) >= len(input_str)\n\n@given(st.text())\ndef test_output_should_be_html_safe_property(input_str):\n    result = html.escape(input_str, quote=True)\n    # Check if the output is a valid HTML-safe representation\n    assert all(char not in result for char in [\"&\", \"<\", \">\", '\"', \"'\"])\n\n@given(st.text())\ndef test_empty_input_should_produce_empty_output_property():\n    result = html.escape(\"\", quote=True)\n    assert result == \"\"\n\n@given(st.text())\ndef test_output_order_property(input_str):\n    result = html.escape(input_str, quote=True)\n    for original_char, escaped_char in zip(input_str, result):\n        if original_char in [\"&\", \"<\", \">\", '\"', \"'\"]:\n            assert escaped_char != original_char  # Ensure replaced\n        else:\n            assert escaped_char == original_char  # Ensure order maintained\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text(max_size=1000))  # Adjust max_size as needed for large inputs\ndef test_output_no_special_characters_property(input_str):\n    result = html.escape(input_str, quote=True)\n    assert '&' not in result\n    assert '<' not in result\n    assert '>' not in result\n    assert '\"' not in result\n    assert \"'\" not in result\n\n@given(st.text(max_size=1000))\ndef test_output_length_property(input_str):\n    result = html.escape(input_str, quote=True)\n    expected_length = len(input_str) + sum(input_str.count(char) for char in ['&', '<', '>', '\"', \"'\"])\n    assert len(result) == expected_length\n\n@given(st.text(max_size=1000))\ndef test_output_valid_html_property(input_str):\n    result = html.escape(input_str, quote=True)\n    assert result == html.escape(input_str, quote=True)  # Should match the escape output\n\n@given(st.text())\ndef test_empty_input_property(input_str):\n    result = html.escape(\"\", quote=True)\n    assert result == \"\"\n\n@given(st.text(max_size=1000))\ndef test_output_order_property(input_str):\n    result = html.escape(input_str, quote=True)\n    for original_char, escaped_char in zip(input_str, result):\n        if original_char in ['&', '<', '>', '\"', \"'\"]:\n            assert result[result.index(escaped_char):].startswith(escaped_char)\n        else:\n            assert original_char in result\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_no_special_characters_property(s):\n    quoted = True\n    result = html.escape(s, quote=quoted)\n    assert '&' not in result\n    assert '<' not in result\n    assert '>' not in result\n    assert '\"' not in result\n    assert \"'\" not in result\n\n@given(st.text())\ndef test_output_length_property(s):\n    quoted = True\n    result = html.escape(s, quote=quoted)\n    original_length = len(s)\n    transformed_length = len(result)\n    # The output length should be equal to the input length plus replacements.\n    assert transformed_length >= original_length\n\n@given(st.text())\ndef test_output_html_safe_property(s):\n    quoted = True\n    result = html.escape(s, quote=quoted)\n    for char in s:\n        if char in '&<>':\n            assert result.count(html.escape(char)) == 1\n        elif quoted and char in '\"\\'':\n            assert result.count(html.escape(char)) == 1\n\n@given(st.text())\ndef test_empty_string_property(s):\n    quoted = True\n    result = html.escape(\"\", quote=quoted)\n    assert result == \"\"\n\n@given(st.text())\ndef test_order_of_characters_property(s):\n    quoted = True\n    result = html.escape(s, quote=quoted)\n    for original_char, escaped_char in zip(s, result):\n        if original_char in '&<>\\\"\\'': \n            assert escaped_char == html.escape(original_char)\n        else:\n            assert escaped_char == original_char\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_should_not_contain_special_characters_property(input_string):\n    result = html.escape(input_string, quote=True)\n    assert '&' not in result\n    assert '<' not in result\n    assert '>' not in result\n    assert '\"' not in result\n    assert \"'\" not in result\n\n@given(st.text())\ndef test_output_length_property(input_string):\n    result = html.escape(input_string, quote=True)\n    num_replacements = sum(1 for char in input_string if char in '&<>\"\\'')\n    assert len(result) == len(input_string) + num_replacements * (len(html.escape(\"\", quote=True)) - 1)\n\n@given(st.text())\ndef test_output_should_be_html_safe_property(input_string):\n    result = html.escape(input_string, quote=True)\n    safe_string = html.escape(input_string, quote=True)\n    assert result == safe_string\n\n@given(st.text())\ndef test_empty_input_should_return_empty_output_property(input_string):\n    result = html.escape(\"\", quote=True)\n    assert result == \"\"\n\n@given(st.text())\ndef test_output_order_property(input_string):\n    result = html.escape(input_string, quote=True)\n    for i, char in enumerate(input_string):\n        if char in '&<>\"\\'':\n            assert result[i] != char\n        else:\n            assert result[i] == char\n\n# End program\n```"], "properties": ["1. The output string should not contain any of the characters `&`, `<`, `>`, `\"`, or `'` if `quote` is set to `True`.\n2. The output string should contain the same number of characters as the input string plus any necessary replacements for special characters.\n3. The output string should be a valid HTML-safe representation of the input string, meaning it should not introduce any additional characters outside of the specified replacements.\n4. If the input string is empty, the output should also be an empty string regardless of the value of the `quote` parameter.\n5. The output should maintain the order of characters from the input string, with only the specified characters being replaced."], "api_doc": "html.escape(s, quote=True)\nConvert the characters &, < and > in string s to HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the optional flag quote is true, the characters (\") and (') are also translated; this helps for inclusion in an HTML attribute value delimited by quotes, as in <a href=\"...\">.\n\nAdded in version 3.2.", "api_code": "def escape(s, quote=True):\n    s = s.replace(\"&\", \"&amp;\") # Must be done first!\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace(\">\", \"&gt;\")\n    if quote:\n        s = s.replace('\"', \"&quot;\")\n        s = s.replace('\\'', \"&#x27;\")\n    return s"}
{"function_name": "networkx.algorithms.bipartite.basic.color", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_contains_two_distinct_colors_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.complete_bipartite_graph, st.integers(min_value=1, max_value=10), st.integers(min_value=1, max_value=10)),\n        st.builds(nx.empty_graph, st.integers(min_value=0, max_value=10)),\n    ))\n    color = nx.algorithms.bipartite.basic.color(G)\n    assert set(color.values()) <= {0, 1}\n\n@given(st.data())\ndef test_empty_graph_output_property(data):\n    G = nx.empty_graph()\n    color = nx.algorithms.bipartite.basic.color(G)\n    assert color == {}\n\n@given(st.data())\ndef test_isolated_nodes_color_property(data):\n    isolated_node_count = data.draw(st.integers(min_value=1, max_value=10))\n    G = nx.Graph()\n    G.add_nodes_from(range(isolated_node_count))  # Add isolated nodes\n    color = nx.algorithms.bipartite.basic.color(G)\n    assert all(color[node] == 0 for node in G.nodes())\n\n@given(st.data())\ndef test_one_to_one_mapping_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.complete_bipartite_graph, st.integers(min_value=1, max_value=10), st.integers(min_value=1, max_value=10)),\n        st.builds(nx.empty_graph, st.integers(min_value=0, max_value=10)),\n    ))\n    color = nx.algorithms.bipartite.basic.color(G)\n    assert len(color) == G.number_of_nodes()\n\n@given(st.data())\ndef test_non_bipartite_graph_error_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.complete_graph, st.integers(min_value=3, max_value=10)),\n        st.builds(nx.cycle_graph, st.integers(min_value=4, max_value=10), directed=False)\n    ))\n    try:\n        nx.algorithms.bipartite.basic.color(G)\n        assert False  # Should raise an error\n    except nx.NetworkXError:\n        pass  # Expected behavior\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_color_distinct_colors_property(data):\n    # Generate a bipartite graph\n    G = nx.bipartite.random_graph(10, 10)\n    color_dict = nx.algorithms.bipartite.basic.color(G)\n    assert set(color_dict.values()) == {0, 1}\n\n@given(st.data())\ndef test_color_empty_graph_property(data):\n    # Test with an empty graph\n    G = nx.Graph()\n    color_dict = nx.algorithms.bipartite.basic.color(G)\n    assert color_dict == {}\n\n@given(st.data())\ndef test_color_isolated_nodes_property(data):\n    # Create a graph with isolated nodes\n    G = nx.Graph()\n    G.add_nodes_from(range(5))  # Five isolated nodes\n    color_dict = nx.algorithms.bipartite.basic.color(G)\n    assert all(color_dict[node] == 0 for node in G.nodes())\n\n@given(st.data())\ndef test_color_one_to_one_mapping_property(data):\n    # Generate a bipartite graph\n    G = nx.bipartite.random_graph(5, 5)\n    color_dict = nx.algorithms.bipartite.basic.color(G)\n    assert len(color_dict) == len(G.nodes())\n\n@given(st.data())\ndef test_color_non_bipartite_graph_property(data):\n    # Generate a non-bipartite graph\n    G = nx.cycle_graph(5)  # A cycle with an odd number of nodes (non-bipartite)\n    try:\n        nx.algorithms.bipartite.basic.color(G)\n        assert False, \"Expected NetworkXError for non-bipartite graph\"\n    except nx.NetworkXError:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_contains_two_colors_property(data):\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\n    color_result = nx.algorithms.bipartite.basic.color(G)\n    assert set(color_result.values()) == {0, 1}\n\n@given(st.data())\ndef test_empty_graph_output_property(data):\n    G = nx.Graph()  # empty graph\n    color_result = nx.algorithms.bipartite.basic.color(G)\n    assert color_result == {}\n\n@given(st.data())\ndef test_isolated_nodes_colored_zero_property(data):\n    G = nx.Graph()\n    G.add_nodes_from(range(5))  # adding isolated nodes\n    color_result = nx.algorithms.bipartite.basic.color(G)\n    for node in G.nodes():\n        assert color_result[node] == 0\n\n@given(st.data())\ndef test_one_to_one_mapping_property(data):\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\n    color_result = nx.algorithms.bipartite.basic.color(G)\n    assert len(color_result) == len(G.nodes())\n\n@given(st.data())\ndef test_not_bipartite_graph_raises_error_property(data):\n    G = data.draw(st.builds(nx.generators.random_graphs.erdos_renyi_graph, st.integers(min_value=1, max_value=100), st.floats(min_value=0, max_value=1)))\n    if not nx.is_bipartite(G):\n        try:\n            nx.algorithms.bipartite.basic.color(G)\n            assert False, \"Expected NetworkXError for non-bipartite graph\"\n        except nx.NetworkXError:\n            pass  # expected error\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.bipartite.basic import color\n\n@given(st.data())\ndef test_output_contains_two_colors_property(data):\n    # Generate a bipartite graph\n    G = nx.bipartite.random_graph(10, 10)\n    result = color(G)\n    assert set(result.values()) == {0, 1}\n\n@given(st.data())\ndef test_empty_graph_output_property(data):\n    # Test for an empty graph\n    G = nx.Graph()\n    result = color(G)\n    assert result == {}\n\n@given(st.data())\ndef test_isolated_nodes_colored_zero_property(data):\n    # Generate a graph with isolated nodes\n    G = nx.Graph()\n    G.add_nodes_from(range(5))\n    G.add_edges_from([(0, 1), (1, 2)])  # 3 is isolated\n    result = color(G)\n    assert result[3] == 0\n\n@given(st.data())\ndef test_one_to_one_mapping_property(data):\n    # Generate a random bipartite graph\n    G = nx.bipartite.random_graph(10, 10)\n    result = color(G)\n    assert len(result) == G.number_of_nodes()\n\n@given(st.data())\ndef test_not_bipartite_graph_raises_error_property(data):\n    # Generate a non-bipartite graph\n    G = nx.Graph()\n    G.add_edges_from([(0, 1), (1, 2), (2, 0)])  # Triangle (not bipartite)\n    try:\n        color(G)\n        assert False, \"Expected NetworkXError not raised\"\n    except nx.NetworkXError:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_contains_exactly_two_colors_property(data):\n    G = data.draw(st.one_of(st.graphs(), st.empty_graphs()))\n    color_result = nx.algorithms.bipartite.basic.color(G)\n    unique_colors = set(color_result.values())\n    assert unique_colors.issubset({0, 1}) and len(unique_colors) <= 2\n\n@given(st.data())\ndef test_empty_graph_output_property(data):\n    G = nx.Graph()  # explicitly create an empty graph\n    color_result = nx.algorithms.bipartite.basic.color(G)\n    assert color_result == {}\n\n@given(st.data())\ndef test_isolated_nodes_color_property(data):\n    G = data.draw(st.graphs())\n    G.add_node('isolated_node')  # add an isolated node\n    color_result = nx.algorithms.bipartite.basic.color(G)\n    assert color_result['isolated_node'] == 0\n\n@given(st.data())\ndef test_one_to_one_mapping_property(data):\n    G = data.draw(st.graphs())\n    color_result = nx.algorithms.bipartite.basic.color(G)\n    assert all(node in color_result for node in G.nodes())\n\n@given(st.data())\ndef test_non_bipartite_graph_raises_error_property(data):\n    G = data.draw(st.non_bipartite_graphs())  # assuming you have a strategy for non-bipartite graphs\n    try:\n        nx.algorithms.bipartite.basic.color(G)\n        assert False, \"Expected a NetworkXError for non-bipartite graph\"\n    except nx.NetworkXError:\n        pass  # Expected behavior\n# End program\n```"], "properties": ["1. The output dictionary should contain exactly two distinct colors (0 and 1) for all nodes in a bipartite graph, ensuring that no two adjacent nodes share the same color.\n\n2. If the input graph is empty, the output should also be an empty dictionary, indicating that there are no nodes to color.\n\n3. For any isolated nodes in the input graph, the output should assign them the color 0, as they are not connected to any other nodes.\n\n4. The output should maintain a one-to-one mapping between nodes in the input graph and the keys in the output dictionary, meaning every node in the graph should have a corresponding entry in the color dictionary.\n\n5. If the input graph is not bipartite, the function should raise a `NetworkXError`, indicating that it cannot assign colors without violating the bipartite condition."], "api_doc": "color\ncolor(G)[source]\nReturns a two-coloring of the graph.\n\nRaises an exception if the graph is not bipartite.\n\nParameters\n:\nG\nNetworkX graph\nReturns\n:\ncolor\ndictionary\nA dictionary keyed by node with a 1 or 0 as data for each node color.\n\nRaises\n:\nNetworkXError\nIf the graph is not two-colorable.\n\nExamples\n\nfrom networkx.algorithms import bipartite\nG = nx.path_graph(4)\nc = bipartite.color(G)\nprint(c)\n{0: 1, 1: 0, 2: 1, 3: 0}\nYou can use this to set a node attribute indicating the bipartite set:\n\nnx.set_node_attributes(G, c, \"bipartite\")\nprint(G.nodes[0][\"bipartite\"])\n1\nprint(G.nodes[1][\"bipartite\"])\n0", "api_code": "@nx._dispatchable\ndef color(G):\n    if G.is_directed():\n        import itertools\n\n        def neighbors(v):\n            return itertools.chain.from_iterable([G.predecessors(v), G.successors(v)])\n\n    else:\n        neighbors = G.neighbors\n\n    color = {}\n    for n in G:  # handle disconnected graphs\n        if n in color or len(G[n]) == 0:  # skip isolates\n            continue\n        queue = [n]\n        color[n] = 1  # nodes seen with color (1 or 0)\n        while queue:\n            v = queue.pop()\n            c = 1 - color[v]  # opposite color of node v\n            for w in neighbors(v):\n                if w in color:\n                    if color[w] == color[v]:\n                        raise nx.NetworkXError(\"Graph is not bipartite.\")\n                else:\n                    color[w] = c\n                    queue.append(w)\n    # color isolates with 0\n    color.update(dict.fromkeys(nx.isolates(G), 0))\n    return color"}
