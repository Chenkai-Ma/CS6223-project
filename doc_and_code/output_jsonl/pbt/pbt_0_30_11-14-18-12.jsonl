{"function_name": "numpy.resize", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.integers()), st.tuples(st.integers(min_value=1)))\ndef test_output_shape_property(a, new_shape):\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.size == np.prod(new_shape)\n\n@given(st.lists(st.integers()), st.tuples(st.integers(min_value=1)))\ndef test_empty_input_property(a, new_shape):\n    if not a:\n        resized_array = np.resize(a, new_shape)\n        assert resized_array.size == 0\n\n@given(st.lists(st.integers()), st.tuples(st.integers(min_value=1, max_value=100)))\ndef test_repeated_elements_property(a, new_shape):\n    resized_array = np.resize(a, new_shape)\n    expected_length = np.prod(new_shape)\n    assert len(resized_array) == expected_length\n    assert all(resized_array[i] == a[i % len(a)] for i in range(expected_length))\n\n@given(st.lists(st.integers()), st.tuples(st.integers(min_value=1, max_value=100)))\ndef test_truncation_property(a, new_shape):\n    resized_array = np.resize(a, new_shape)\n    expected_length = np.prod(new_shape)\n    if len(a) > expected_length:\n        assert np.array_equal(resized_array, np.array(a)[:expected_length])\n\n@given(st.lists(st.integers()), st.tuples(st.integers(min_value=1)))\ndef test_data_type_property(a, new_shape):\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.dtype == np.array(a).dtype\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\ndef test_output_shape_property(input_array, new_shape):\n    a = np.array(input_array)\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.size == np.prod(new_shape)\n\n@given(st.lists(st.integers()), st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\ndef test_empty_input_property(input_array, new_shape):\n    a = np.array(input_array)\n    resized_array = np.resize(a, new_shape)\n    if a.size == 0:\n        assert resized_array.size == 0\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\ndef test_repeated_elements_property(input_array, new_shape):\n    a = np.array(input_array)\n    resized_array = np.resize(a, new_shape)\n    original_size = a.size\n    new_size = np.prod(new_shape)\n    if new_size > original_size:\n        assert all(resized_array[i] == a[i % original_size] for i in range(new_size))\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\ndef test_cropped_elements_property(input_array, new_shape):\n    a = np.array(input_array)\n    resized_array = np.resize(a, new_shape)\n    original_size = a.size\n    new_size = np.prod(new_shape)\n    if new_size < original_size:\n        assert np.array_equal(resized_array, a[:new_size])\n\n@given(st.lists(st.integers()), st.tuples(st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\ndef test_same_dtype_property(input_array, new_shape):\n    a = np.array(input_array)\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.dtype == a.dtype\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(a=st.lists(st.integers(), min_size=0), new_shape=st.tuples(st.integers(min_value=0), st.integers(min_value=0)))\ndef test_output_elements_count_property(a, new_shape):\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.size == np.prod(new_shape)\n    \n@given(new_shape=st.tuples(st.integers(min_value=0), st.integers(min_value=0)))\ndef test_empty_input_array_property(new_shape):\n    resized_array = np.resize([], new_shape)\n    assert resized_array.size == 0\n\n@given(a=st.lists(st.integers()), new_shape=st.tuples(st.integers(min_value=1), st.integers()))\ndef test_repeated_elements_property(a, new_shape):\n    if a:  # Ensure that a is not empty\n        resized_array = np.resize(a, new_shape)\n        assert all(resized_array[i] == a[i % len(a)] for i in range(resized_array.size))\n\n@given(a=st.lists(st.integers()), new_shape=st.tuples(st.integers(max_value=0), st.integers()))\ndef test_cropped_elements_property(a, new_shape):\n    if a:  # Ensure that a is not empty\n        resized_array = np.resize(a, new_shape)\n        assert np.array_equal(resized_array, a[:np.prod(new_shape)])\n\n@given(a=st.lists(st.integers()), new_shape=st.tuples(st.integers(min_value=1), st.integers()))\ndef test_data_type_property(a, new_shape):\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.dtype == np.array(a).dtype\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.integers(), min_size=1).filter(lambda x: len(x) > 0), \n               st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_output_shape_property(arr, new_shape):\n    a = np.array(arr)\n    resized = np.resize(a, new_shape)\n    assert resized.size == np.prod(new_shape)\n\n@given(st.lists(st.integers(), min_size=0), \n               st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_empty_input_property(arr, new_shape):\n    a = np.array(arr)\n    resized = np.resize(a, new_shape)\n    if a.size == 0:\n        assert resized.size == 0\n\n@given(st.lists(st.integers(), min_size=1).filter(lambda x: len(x) > 0), \n               st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_repeated_elements_property(arr, new_shape):\n    a = np.array(arr)\n    resized = np.resize(a, new_shape)\n    expected_size = np.prod(new_shape)\n    assert len(resized) == expected_size\n    repeats = expected_size // len(a)\n    for i in range(expected_size):\n        assert resized[i] == a[i % len(a)]\n\n@given(st.lists(st.integers(), min_size=1).filter(lambda x: len(x) > 0), \n               st.tuples(st.integers(max_value=1), st.integers(max_value=1)))\ndef test_trimmed_elements_property(arr, new_shape):\n    a = np.array(arr)\n    new_shape = (1, np.prod(new_shape))  # Ensure new shape is large enough to access elements\n    resized = np.resize(a, new_shape)\n    expected_size = np.prod(new_shape)\n    assert len(resized) == expected_size\n    assert np.all(resized[:len(a)] == a)\n\n@given(st.lists(st.integers(), min_size=1), \n               st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_data_type_property(arr, new_shape):\n    a = np.array(arr)\n    resized = np.resize(a, new_shape)\n    assert resized.dtype == a.dtype\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1)))\ndef test_output_shape_property(input_array, new_shape):\n    resized_array = np.resize(np.array(input_array), new_shape)\n    assert resized_array.size == new_shape[0] * new_shape[1]\n\n@given(st.lists(st.integers(), min_size=0), st.tuples(st.integers(min_value=1)))\ndef test_empty_input_property(input_array, new_shape):\n    resized_array = np.resize(np.array(input_array), new_shape)\n    assert resized_array.size == 0\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1, max_value=100)))\ndef test_repeat_elements_property(input_array, new_shape):\n    input_array_np = np.array(input_array)\n    resized_array = np.resize(input_array_np, new_shape)\n    expected_size = new_shape[0] * new_shape[1]\n    repeated_elements = (input_array_np.tolist() * ((expected_size // input_array_np.size) + 1))[:expected_size]\n    assert np.array_equal(resized_array.tolist(), repeated_elements)\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1, max_value=100)))\ndef test_truncate_elements_property(input_array, new_shape):\n    input_array_np = np.array(input_array)\n    resized_array = np.resize(input_array_np, new_shape)\n    expected_size = new_shape[0] * new_shape[1]\n    assert resized_array.size == expected_size\n    assert len(resized_array) <= len(input_array_np)\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1, max_value=100)))\ndef test_output_dtype_property(input_array, new_shape):\n    input_array_np = np.array(input_array)\n    resized_array = np.resize(input_array_np, new_shape)\n    assert resized_array.dtype == input_array_np.dtype\n# End program\n```"], "properties": ["1. The output array's total number of elements must match the product of the dimensions specified in `new_shape`.\n\n2. If the input array `a` is empty, the output array must also be empty, regardless of the value of `new_shape`.\n\n3. When the `new_shape` is larger than the size of the input array `a`, the output array must contain repeated copies of the original array's elements, cycling through them if necessary.\n\n4. If the `new_shape` is smaller than the size of the input array `a`, the output array must contain only the first `new_size` elements of the input array, where `new_size` is the product of the dimensions in `new_shape`.\n\n5. The output array must maintain the same data type as the input array `a`."], "api_doc": "numpy.resize\nnumpy.resize(a, new_shape)[source]\nReturn a new array with the specified shape.\n\nIf the new array is larger than the original array, then the new array is filled with repeated copies of a. Note that this behavior is different from a.resize(new_shape) which fills with zeros instead of repeated copies of a.\n\nParameters\n:\na\narray_like\nArray to be resized.\n\nnew_shape\nint or tuple of int\nShape of resized array.\n\nReturns\n:\nreshaped_array\nndarray\nThe new array is formed from the data in the old array, repeated if necessary to fill out the required number of elements. The data are repeated iterating over the array in C-order.\n\nSee also\n\nnumpy.reshape\nReshape an array without changing the total size.\n\nnumpy.pad\nEnlarge and pad an array.\n\nnumpy.repeat\nRepeat elements of an array.\n\nndarray.resize\nresize an array in-place.\n\nNotes\n\nWhen the total size of the array does not change reshape should be used. In most other cases either indexing (to reduce the size) or padding (to increase the size) may be a more appropriate solution.\n\nWarning: This functionality does not consider axes separately, i.e. it does not apply interpolation/extrapolation. It fills the return array with the required number of elements, iterating over a in C-order, disregarding axes (and cycling back from the start if the new shape is larger). This functionality is therefore not suitable to resize images, or data where each axis represents a separate and distinct entity.\n\nExamples\n\nimport numpy as np\na = np.array([[0,1],[2,3]])\nnp.resize(a,(2,3))\narray([[0, 1, 2],\n       [3, 0, 1]])\nnp.resize(a,(1,4))\narray([[0, 1, 2, 3]])\nnp.resize(a,(2,4))\narray([[0, 1, 2, 3],\n       [0, 1, 2, 3]])", "api_code": "@array_function_dispatch(_resize_dispatcher)\ndef resize(a, new_shape):\n    if isinstance(new_shape, (int, nt.integer)):\n        new_shape = (new_shape,)\n\n    a = ravel(a)\n\n    new_size = 1\n    for dim_length in new_shape:\n        new_size *= dim_length\n        if dim_length < 0:\n            raise ValueError(\n                'all elements of `new_shape` must be non-negative'\n            )\n\n    if a.size == 0 or new_size == 0:\n        # First case must zero fill. The second would have repeats == 0.\n        return np.zeros_like(a, shape=new_shape)\n\n    repeats = -(-new_size // a.size)  # ceil division\n    a = concatenate((a,) * repeats)[:new_size]\n\n    return reshape(a, new_shape)"}
{"function_name": "networkx.algorithms.clique.find_cliques", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.clique import find_cliques\n\n@given(st.data())\ndef test_maximal_cliques_property(data):\n    G = nx.generators.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        for node in clique:\n            # Check if the clique is maximal\n            neighbors = set(G.neighbors(node))\n            if neighbors.intersection(set(clique)) != set(clique):\n                assert False  # Not maximal\n    assert True  # All cliques are maximal\n# End program\n\n@given(st.data())\ndef test_cliques_contain_given_nodes_property(data):\n    G = nx.generators.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    nodes = data.draw(st.lists(st.integers(min_value=0, max_value=G.number_of_nodes()-1), min_size=1, max_size=5))\n    if all(node in G.nodes for node in nodes):\n        cliques = list(find_cliques(G, nodes=nodes))\n        for clique in cliques:\n            assert all(node in clique for node in nodes)\n    else:\n        # If nodes isn't a clique, test that ValueError is raised\n        try:\n            list(find_cliques(G, nodes=nodes))\n            assert False  # Should have raised ValueError\n        except ValueError:\n            assert True  # Correct behavior\n# End program\n\n@given(st.data())\ndef test_unique_cliques_property(data):\n    G = nx.generators.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    cliques = list(find_cliques(G))\n    assert len(cliques) == len(set(map(tuple, cliques)))  # Ensure uniqueness\n# End program\n\n@given(st.data())\ndef test_cliques_length_property(data):\n    G = nx.generators.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        assert len(clique) >= 1  # Each clique must have at least one node\n# End program\n\n@given(st.data())\ndef test_arbitrary_order_property(data):\n    G = nx.generators.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    cliques1 = list(find_cliques(G))\n    cliques2 = list(find_cliques(G))\n    assert sorted(map(sorted, cliques1)) == sorted(map(sorted, cliques2))  # Same cliques, arbitrary order\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.clique import find_cliques\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1))\ndef test_maximal_cliques_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    G.add_edges_from([(graph_nodes[i], graph_nodes[j]) for i in range(len(graph_nodes)) for j in range(i + 1, len(graph_nodes))])\n    \n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        for node in clique:\n            assert node in graph_nodes\n            # Check that no additional node can be added to the clique\n            for additional_node in graph_nodes:\n                if additional_node not in clique and not G.has_edge(clique[0], additional_node):\n                    assert additional_node not in clique\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1))\ndef test_nodes_in_cliques_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    # Create edges to form cliques\n    for i in range(len(graph_nodes)):\n        for j in range(i + 1, len(graph_nodes)):\n            if i % 2 == j % 2:  # Connect nodes with the same parity\n                G.add_edge(graph_nodes[i], graph_nodes[j])\n    \n    nodes_subset = st.sampled_from(graph_nodes)\n    cliques = list(find_cliques(G))\n    \n    for clique in cliques:\n        assert all(node in clique for node in nodes_subset)\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1))\ndef test_unique_cliques_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    for i in range(len(graph_nodes)):\n        for j in range(i + 1, len(graph_nodes)):\n            G.add_edge(graph_nodes[i], graph_nodes[j])\n    \n    cliques = list(find_cliques(G))\n    assert len(cliques) == len(set(tuple(sorted(clique)) for clique in cliques))\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1))\ndef test_clique_length_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    for i in range(len(graph_nodes)):\n        for j in range(i + 1, len(graph_nodes)):\n            G.add_edge(graph_nodes[i], graph_nodes[j])\n    \n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        assert len(clique) > 0\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1))\ndef test_arbitrary_order_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    for i in range(len(graph_nodes)):\n        for j in range(i + 1, len(graph_nodes)):\n            G.add_edge(graph_nodes[i], graph_nodes[j])\n    \n    cliques1 = list(find_cliques(G))\n    cliques2 = list(find_cliques(G))\n    \n    # Check that the same cliques are present regardless of order\n    assert sorted(cliques1) == sorted(cliques2)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom hypothesis import example\n\n@given(st.lists(st.integers(min_value=0, max_value=100), unique=True).map(nx.Graph.from_edges))\ndef test_find_cliques_maximal_property(G):\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        for node in clique:\n            # Check if the clique is maximal\n            assert all(node in G[n] for n in clique)  # must be a complete subgraph\n            assert all(node not in G[n] for n in G if n not in clique)  # can't add any node\n\n@given(st.lists(st.integers(min_value=0, max_value=100), unique=True).map(nx.Graph.from_edges),\n               st.lists(st.integers(min_value=0, max_value=100), unique=True))\ndef test_find_cliques_nodes_property(G, nodes):\n    if len(nodes) > 0:\n        try:\n            cliques = list(nx.find_cliques(G, nodes))\n            for clique in cliques:\n                assert all(node in clique for node in nodes)  # must contain all nodes\n        except ValueError:\n            # If nodes are not a clique, it's expected to raise ValueError\n            pass\n\n@given(st.lists(st.integers(min_value=0, max_value=100), unique=True).map(nx.Graph.from_edges))\ndef test_find_cliques_unique_cliques_property(G):\n    cliques = list(nx.find_cliques(G))\n    assert len(cliques) == len(set(map(tuple, cliques)))  # all cliques must be unique\n\n@given(st.lists(st.integers(min_value=0, max_value=100), unique=True).map(nx.Graph.from_edges))\ndef test_find_cliques_non_empty_cliques_property(G):\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        assert len(clique) >= 1  # each clique must have at least one node\n\n@given(st.lists(st.integers(min_value=0, max_value=100), unique=True).map(nx.Graph.from_edges))\ndef test_find_cliques_arbitrary_order_property(G):\n    cliques_1 = list(nx.find_cliques(G))\n    cliques_2 = list(nx.find_cliques(G))\n    assert sorted(cliques_1) == sorted(cliques_2)  # the same cliques should be present regardless of order\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.clique import find_cliques\n\n@given(st.data())\ndef test_cliques_are_maximal_property(data):\n    G = nx.gnm_random_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                              data.draw(st.integers(min_value=1, max_value=100)))\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        for node in clique:\n            neighbors = set(G[node])\n            assert all(neighbor in clique for neighbor in neighbors if neighbor in clique)\n\n@given(st.data())\ndef test_nodes_in_cliques_property(data):\n    G = nx.gnm_random_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                              data.draw(st.integers(min_value=1, max_value=100)))\n    nodes = data.draw(st.lists(st.integers(min_value=0, max_value=len(G.nodes)-1), min_size=1, max_size=5))\n                      if len(G.nodes) > 0 else st.lists(st.integers()))\n    if all(n in G.nodes for n in nodes) and len(nodes) > 1:\n        cliques = list(find_cliques(G, nodes))\n        for clique in cliques:\n            assert all(n in clique for n in nodes)\n    else:\n        try:\n            list(find_cliques(G, nodes))\n        except ValueError:\n            pass\n\n@given(st.data())\ndef test_unique_cliques_property(data):\n    G = nx.gnm_random_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                              data.draw(st.integers(min_value=1, max_value=100)))\n    cliques = list(find_cliques(G))\n    assert len(cliques) == len(set(tuple(sorted(clique)) for clique in cliques))\n\n@given(st.data())\ndef test_clique_length_property(data):\n    G = nx.gnm_random_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                              data.draw(st.integers(min_value=1, max_value=100)))\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        assert len(clique) >= 1\n\n@given(st.data())\ndef test_arbitrary_order_property(data):\n    G = nx.gnm_random_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                              data.draw(st.integers(min_value=1, max_value=100)))\n    cliques1 = list(find_cliques(G))\n    cliques2 = list(find_cliques(G))\n    assert set(tuple(sorted(clique)) for clique in cliques1) == set(tuple(sorted(clique)) for clique in cliques2)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.clique import find_cliques\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, unique=True).map(set))\ndef test_output_cliques_are_maximal_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    # Add edges to create cliques\n    for i in graph_nodes:\n        for j in graph_nodes:\n            if i != j:\n                G.add_edge(i, j)\n    \n    cliques = list(find_cliques(G))\n    \n    for clique in cliques:\n        for node in clique:\n            # Check if adding any other node from G would still be a clique\n            for other in G.nodes():\n                if other not in clique and not G.has_edge(node, other):\n                    assert len(clique) == len(set(clique))  # Must be maximal\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, unique=True).map(set))\ndef test_nodes_included_if_provided_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    # Add edges to create a complete graph\n    for i in graph_nodes:\n        for j in graph_nodes:\n            if i != j:\n                G.add_edge(i, j)\n    \n    # Test with a subset of nodes\n    subset = list(graph_nodes)[:len(graph_nodes)//2]\n    cliques = list(find_cliques(G, nodes=subset))\n    \n    for clique in cliques:\n        for node in subset:\n            assert node in clique  # Each clique must include all nodes in subset\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, unique=True).map(set))\ndef test_output_cliques_are_unique_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    # Add edges to create cliques\n    for i in graph_nodes:\n        for j in graph_nodes:\n            if i != j:\n                G.add_edge(i, j)\n\n    cliques = list(find_cliques(G))\n    assert len(cliques) == len(set(tuple(sorted(clique)) for clique in cliques))  # Must be unique\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, unique=True).map(set))\ndef test_length_of_cliques_is_at_least_one_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    # Add edges to create cliques\n    for i in graph_nodes:\n        for j in graph_nodes:\n            if i != j:\n                G.add_edge(i, j)\n    \n    cliques = list(find_cliques(G))\n    \n    for clique in cliques:\n        assert len(clique) >= 1  # Each clique must have at least one node\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, unique=True).map(set))\ndef test_order_of_cliques_is_arbitrary_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    # Add edges to create cliques\n    for i in graph_nodes:\n        for j in graph_nodes:\n            if i != j:\n                G.add_edge(i, j)\n\n    cliques1 = list(find_cliques(G))\n    cliques2 = list(find_cliques(G))\n    \n    # Check that the same cliques can be found, but order may differ\n    assert set(map(tuple, cliques1)) == set(map(tuple, cliques2))  # Must contain the same cliques\n\n# End program\n```"], "properties": ["1. The output cliques are maximal, meaning that no additional nodes can be added to any of the cliques without violating the completeness of the subgraph.\n\n2. If the `nodes` parameter is provided, every returned clique must contain all nodes specified in the `nodes` list, and if `nodes` is not a clique itself, a ValueError should be raised.\n\n3. The output should not include any duplicate cliques, ensuring that each maximal clique is unique within the iterator.\n\n4. The length of each returned clique should be at least 1, as a maximal clique must contain at least one node from the graph.\n\n5. The order of the cliques in the output is arbitrary, meaning that different invocations of the function should not guarantee the same order of cliques, but the same cliques should be present in any valid output for the same input graph."], "api_doc": "find_cliques\nfind_cliques(G, nodes=None)[source]\nReturns all maximal cliques in an undirected graph.\n\nFor each node n, a maximal clique for n is a largest complete subgraph containing n. The largest maximal clique is sometimes called the maximum clique.\n\nThis function returns an iterator over cliques, each of which is a list of nodes. It is an iterative implementation, so should not suffer from recursion depth issues.\n\nThis function accepts a list of nodes and only the maximal cliques containing all of these nodes are returned. It can considerably speed up the running time if some specific cliques are desired.\n\nParameters\n:\nG\nNetworkX graph\nAn undirected graph.\n\nnodes\nlist, optional (default=None)\nIf provided, only yield maximal cliques containing all nodes in nodes. If nodes isn\u2019t a clique itself, a ValueError is raised.\n\nReturns\n:\niterator\nAn iterator over maximal cliques, each of which is a list of nodes in G. If nodes is provided, only the maximal cliques containing all the nodes in nodes are returned. The order of cliques is arbitrary.\n\nRaises\n:\nValueError\nIf nodes is not a clique.\n\nSee also\n\nfind_cliques_recursive\nA recursive version of the same algorithm.\n\nNotes\n\nTo obtain a list of all maximal cliques, use list(find_cliques(G)). However, be aware that in the worst-case, the length of this list can be exponential in the number of nodes in the graph. This function avoids storing all cliques in memory by only keeping current candidate node lists in memory during its search.\n\nThis implementation is based on the algorithm published by Bron and Kerbosch (1973) [1], as adapted by Tomita, Tanaka and Takahashi (2006) [2] and discussed in Cazals and Karande (2008) [3]. It essentially unrolls the recursion used in the references to avoid issues of recursion stack depth (for a recursive implementation, see find_cliques_recursive()).\n\nThis algorithm ignores self-loops and parallel edges, since cliques are not conventionally defined with such edges.\n\nReferences\n\n[1]\nBron, C. and Kerbosch, J. \u201cAlgorithm 457: finding all cliques of an undirected graph\u201d. Communications of the ACM 16, 9 (Sep. 1973), 575\u2013577. <http://portal.acm.org/citation.cfm?doid=362342.362367>\n\n[2]\nEtsuji Tomita, Akira Tanaka, Haruhisa Takahashi, \u201cThe worst-case time complexity for generating all maximal cliques and computational experiments\u201d, Theoretical Computer Science, Volume 363, Issue 1, Computing and Combinatorics, 10th Annual International Conference on Computing and Combinatorics (COCOON 2004), 25 October 2006, Pages 28\u201342 <https://doi.org/10.1016/j.tcs.2006.06.015>\n\n[3]\nF. Cazals, C. Karande, \u201cA note on the problem of reporting maximal cliques\u201d, Theoretical Computer Science, Volume 407, Issues 1\u20133, 6 November 2008, Pages 564\u2013568, <https://doi.org/10.1016/j.tcs.2008.05.010>\n\nExamples\n\nfrom pprint import pprint  # For nice dict formatting\nG = nx.karate_club_graph()\nsum(1 for c in nx.find_cliques(G))  # The number of maximal cliques in G\n36\nmax(nx.find_cliques(G), key=len)  # The largest maximal clique in G\n[0, 1, 2, 3, 13]\nThe size of the largest maximal clique is known as the clique number of the graph, which can be found directly with:\n\nmax(len(c) for c in nx.find_cliques(G))\n5\nOne can also compute the number of maximal cliques in G that contain a given node. The following produces a dictionary keyed by node whose values are the number of maximal cliques in G that contain the node:\n\npprint({n: sum(1 for c in nx.find_cliques(G) if n in c) for n in G})\n{0: 13,\n 1: 6,\n 2: 7,\n 3: 3,\n 4: 2,\n 5: 3,\n 6: 3,\n 7: 1,\n 8: 3,\n 9: 2,\n 10: 2,\n 11: 1,\n 12: 1,\n 13: 2,\n 14: 1,\n 15: 1,\n 16: 1,\n 17: 1,\n 18: 1,\n 19: 2,\n 20: 1,\n 21: 1,\n 22: 1,\n 23: 3,\n 24: 2,\n 25: 2,\n 26: 1,\n 27: 3,\n 28: 2,\n 29: 2,\n 30: 2,\n 31: 4,\n 32: 9,\n 33: 14}\nOr, similarly, the maximal cliques in G that contain a given node. For example, the 4 maximal cliques that contain node 31:\n\n[c for c in nx.find_cliques(G) if 31 in c]\n[[0, 31], [33, 32, 31], [33, 28, 31], [24, 25, 31]]", "api_code": "@not_implemented_for(\"directed\")\n@nx._dispatchable\ndef find_cliques(G, nodes=None):\n    if len(G) == 0:\n        return\n\n    adj = {u: {v for v in G[u] if v != u} for u in G}\n\n    # Initialize Q with the given nodes and subg, cand with their nbrs\n    Q = nodes[:] if nodes is not None else []\n    cand = set(G)\n    for node in Q:\n        if node not in cand:\n            raise ValueError(f\"The given `nodes` {nodes} do not form a clique\")\n        cand &= adj[node]\n\n    if not cand:\n        yield Q[:]\n        return\n\n    subg = cand.copy()\n    stack = []\n    Q.append(None)\n\n    u = max(subg, key=lambda u: len(cand & adj[u]))\n    ext_u = cand - adj[u]\n\n    try:\n        while True:\n            if ext_u:\n                q = ext_u.pop()\n                cand.remove(q)\n                Q[-1] = q\n                adj_q = adj[q]\n                subg_q = subg & adj_q\n                if not subg_q:\n                    yield Q[:]\n                else:\n                    cand_q = cand & adj_q\n                    if cand_q:\n                        stack.append((subg, cand, ext_u))\n                        Q.append(None)\n                        subg = subg_q\n                        cand = cand_q\n                        u = max(subg, key=lambda u: len(cand & adj[u]))\n                        ext_u = cand - adj[u]\n            else:\n                Q.pop()\n                subg, cand, ext_u = stack.pop()\n    except IndexError:\n        pass"}
{"function_name": "statistics.correlation", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom statistics import StatisticsError\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1),\n                st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1))\n@given(st.lists(st.floats(min_value=1, max_value=1000), min_size=2, max_size=1000))\ndef test_correlation_output_range_property(x, y):\n    result = statistics.correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) == 1),\n               st.lists(st.floats(), min_size=2))\ndef test_correlation_constant_input_property(x, y):\n    try:\n        statistics.correlation(x, y)\n        assert False, \"Expected StatisticsError but function executed successfully.\"\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) > 1),\n               st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) > 1))\ndef test_correlation_linear_transformation_property(x, y):\n    # Scale and shift\n    scaled_y = [2 * val + 3 for val in y]\n    assert statistics.correlation(x, y) == statistics.correlation(x, scaled_y)\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1),\n                st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1))\ndef test_correlation_spearman_monotonic_property(x, y):\n    sorted_x = sorted(x)\n    sorted_y = sorted(y)\n    assert statistics.correlation(sorted_x, sorted_y, method='ranked') == 1.0\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1),\n                st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1))\ndef test_correlation_symmetry_property(x, y):\n    assert statistics.correlation(x, y) == statistics.correlation(y, x)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom statistics import StatisticsError\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda x: len(set(x)) > 1),\n                unique=True), st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda x: len(set(x)) > 1,\n                unique=True))\ndef test_output_within_bounds_property(x, y):\n    result = statistics.correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000, unique=True))\ndef test_constant_input_raises_property(x):\n    with st.raises(StatisticsError):\n        statistics.correlation(x, x)\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000), st.floats(), st.floats())\ndef test_linear_transformation_property(x, a, b):\n    y = [a * xi + b for xi in x]\n    result = statistics.correlation(x, y)\n    assert result == 1.0 or result == -1.0\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda x: len(set(x)) > 1),\n                unique=True), st.lists(st.floats(), min_size=2, max_size=1000).filter(lambda x: len(set(x)) > 1,\n                unique=True))\ndef test_spearman_monotonic_relationship_property(x, y):\n    result = statistics.correlation(x, y, method='ranked')\n    if sorted(x) == sorted(y):\n        assert result == 1.0\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000), st.lists(st.floats(), min_size=2, max_size=1000))\ndef test_symmetric_property(x, y):\n    result_xy = statistics.correlation(x, y)\n    result_yx = statistics.correlation(y, x)\n    assert result_xy == result_yx\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import correlation, StatisticsError\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000))\ndef test_output_range_property(data):\n    x, y = data, data  # Using the same list for simplicity\n    result = correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(min_value=0, max_value=1), min_size=2))\ndef test_constant_input_property(data):\n    constant_list = [data[0]] * len(data)\n    try:\n        correlation(data, constant_list)\n        assert False, \"Expected StatisticsError due to constant input\"\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats(), min_size=2), st.floats(), st.floats())\ndef test_linear_transformation_property(data, scale, shift):\n    x = data\n    y = [scale * xi + shift for xi in x]\n    result = correlation(x, y)\n    assert result == 1.0 or result == -1.0\n\n@given(st.lists(st.floats(), min_size=2))\ndef test_spearman_monotonic_relationship_property(data):\n    sorted_data = sorted(data)\n    result = correlation(sorted_data, sorted_data, method='ranked')\n    assert result == 1.0\n\n@given(st.lists(st.floats(), min_size=2), st.lists(st.floats(), min_size=2))\ndef test_symmetric_property(x_data, y_data):\n    if len(x_data) == len(y_data):\n        result_xy = correlation(x_data, y_data)\n        result_yx = correlation(y_data, x_data)\n        assert result_xy == result_yx\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import correlation, StatisticsError\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1),\n               st.lists(st.floats(), min_size=2).filter(lambda y: len(set(y)) > 1))\ndef test_output_range_property(x, y):\n    result = correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(), min_size=2).map(lambda x: [x[0]] * len(x)),\n               st.lists(st.floats(), min_size=2).map(lambda y: [y[0]] * len(y)))\ndef test_constant_input_property(x, y):\n    try:\n        correlation(x, y)\n        assert False, \"Expected StatisticsError for constant inputs\"\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1),\n               st.lists(st.floats(), min_size=2).filter(lambda y: len(set(y)) > 1))\ndef test_linear_transformation_property(x, y):\n    scale = st.floats(min_value=0.1, max_value=10.0).example()\n    shifted_y = [scale * val + 1 for val in y]\n    result = correlation(x, shifted_y)\n    assert result == 1.0 or result == -1.0\n\n@given(st.lists(st.floats(), min_size=2),\n       st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1))\ndef test_ranked_monotonic_property(x, y):\n    result = correlation(x, y, method='ranked')\n    assert result == 1.0 or result == -1.0\n\n@given(st.lists(st.floats(), min_size=2),\n       st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1))\ndef test_symmetric_property(x, y):\n    result_xy = correlation(x, y)\n    result_yx = correlation(y, x)\n    assert result_xy == result_yx\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom statistics import StatisticsError\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) > 1))\ndef test_output_range_property(x, y):\n    result = statistics.correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(), min_size=2).map(lambda lst: [lst[0]] * len(lst)))\ndef test_constant_input_property(x):\n    with st.raises(StatisticsError):\n        statistics.correlation(x, x)\n\n@given(st.lists(st.floats(), min_size=2), \n       st.floats(), \n       st.floats())\ndef test_linear_transformation_property(x, a, b):\n    transformed_x = [a * xi + b for xi in x]\n    result_original = statistics.correlation(x, transformed_x)\n    assert result_original == 1.0 or result_original == -1.0\n\n@given(st.lists(st.floats(), min_size=2), \n       st.lists(st.floats(), min_size=2).filter(lambda lst: len(set(lst)) > 1))\ndef test_spearman_monotonic_property(x, y):\n    assert statistics.correlation(x, y, method='ranked') == 1.0\n\n@given(st.lists(st.floats(), min_size=2), \n       st.lists(st.floats(), min_size=2))\ndef test_symmetry_property(x, y):\n    result_xy = statistics.correlation(x, y)\n    result_yx = statistics.correlation(y, x)\n    assert result_xy == result_yx\n# End program\n```"], "properties": ["1. The output of the function must always be a float value between -1 and +1, inclusive, representing the Pearson\u2019s or Spearman\u2019s correlation coefficient.\n\n2. If both input lists are constant (i.e., all elements are the same), the function should raise a StatisticsError indicating that at least one of the inputs is constant.\n\n3. For any two input lists of the same length, if one list is a linear transformation of the other (e.g., scaled or shifted), the output should reflect a correlation of 1.0 or -1.0, depending on the direction of the transformation.\n\n4. If the method is set to 'ranked', the output should be consistent with the properties of Spearman\u2019s rank correlation, indicating a value of 1.0 for perfectly monotonic relationships.\n\n5. The output should be symmetric; that is, the correlation of (x, y) should be equal to the correlation of (y, x) for any valid input lists x and y."], "api_doc": "statistics.correlation(x, y, /, *, method='linear')\nReturn the Pearson\u2019s correlation coefficient for two inputs. Pearson\u2019s correlation coefficient r takes values between -1 and +1. It measures the strength and direction of a linear relationship.\n\nIf method is \u201cranked\u201d, computes Spearman\u2019s rank correlation coefficient for two inputs. The data is replaced by ranks. Ties are averaged so that equal values receive the same rank. The resulting coefficient measures the strength of a monotonic relationship.\n\nSpearman\u2019s correlation coefficient is appropriate for ordinal data or for continuous data that doesn\u2019t meet the linear proportion requirement for Pearson\u2019s correlation coefficient.\n\nBoth inputs must be of the same length (no less than two), and need not to be constant, otherwise StatisticsError is raised.\n\nExample with Kepler\u2019s laws of planetary motion:\n\n>>>\n# Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and  Neptune\norbital_period = [88, 225, 365, 687, 4331, 10_756, 30_687, 60_190]    # days\ndist_from_sun = [58, 108, 150, 228, 778, 1_400, 2_900, 4_500] # million km\n\n# Show that a perfect monotonic relationship exists\ncorrelation(orbital_period, dist_from_sun, method='ranked')\n1.0\n\n# Observe that a linear relationship is imperfect\nround(correlation(orbital_period, dist_from_sun), 4)\n0.9882\n\n# Demonstrate Kepler's third law: There is a linear correlation\n# between the square of the orbital period and the cube of the\n# distance from the sun.\nperiod_squared = [p * p for p in orbital_period]\ndist_cubed = [d * d * d for d in dist_from_sun]\nround(correlation(period_squared, dist_cubed), 4)\n1.0\nAdded in version 3.10.\n\nChanged in version 3.12: Added support for Spearman\u2019s rank correlation coefficient.", "api_code": "def correlation(x, y, /, *, method='linear'):\n    n = len(x)\n    if len(y) != n:\n        raise StatisticsError('correlation requires that both inputs have same number of data points')\n    if n < 2:\n        raise StatisticsError('correlation requires at least two data points')\n    if method not in {'linear', 'ranked'}:\n        raise ValueError(f'Unknown method: {method!r}')\n    if method == 'ranked':\n        start = (n - 1) / -2            # Center rankings around zero\n        x = _rank(x, start=start)\n        y = _rank(y, start=start)\n    else:\n        xbar = fsum(x) / n\n        ybar = fsum(y) / n\n        x = [xi - xbar for xi in x]\n        y = [yi - ybar for yi in y]\n    sxy = sumprod(x, y)\n    sxx = sumprod(x, x)\n    syy = sumprod(y, y)\n    try:\n        return sxy / _sqrtprod(sxx, syy)\n    except ZeroDivisionError:\n        raise StatisticsError('at least one of the inputs is constant')"}
{"function_name": "networkx.algorithms.approximation.clustering_coefficient.average_clustering", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.data())\ndef test_output_between_0_and_1_property():\n    G = nx.erdos_renyi_graph(100, 0.1)  # limiting size to avoid overflow\n    result = approximation.average_clustering(G, trials=1000)\n    assert 0 <= result <= 1\n\n@given(st.data())\ndef test_output_zero_for_no_edges_property():\n    G = nx.Graph()  # an empty graph\n    result = approximation.average_clustering(G, trials=1000)\n    assert result == 0\n\n@given(st.data())\ndef test_reproducibility_with_fixed_seed_property():\n    G = nx.erdos_renyi_graph(100, 0.1)\n    seed = 42\n    result_1 = approximation.average_clustering(G, trials=1000, seed=seed)\n    result_2 = approximation.average_clustering(G, trials=1000, seed=seed)\n    assert result_1 == result_2\n\n@given(st.data())\ndef test_approximation_convergence_property():\n    G = nx.erdos_renyi_graph(1000, 0.1)  # larger graph for better approximation\n    result_approx = approximation.average_clustering(G, trials=10000)\n    result_exact = nx.average_clustering(G)  # deterministic calculation\n    assert abs(result_approx - result_exact) < 0.05  # check within a tolerance\n\n@given(st.data())\ndef test_directed_graph_raises_exception_property():\n    G = nx.DiGraph()  # creating a directed graph\n    try:\n        approximation.average_clustering(G, trials=1000)\n        assert False, \"Expected NetworkXNotImplemented exception\"\n    except nx.NetworkXNotImplemented:\n        pass  # expected exception\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.data())\ndef test_output_between_0_and_1_property(data):\n    G = nx.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), data.draw(st.floats(min_value=0, max_value=1)))\n    trials = data.draw(st.integers(min_value=1, max_value=1000))\n    result = approximation.average_clustering(G, trials=trials)\n    assert 0 <= result <= 1\n\n@given(st.data())\ndef test_output_zero_for_no_edges_property(data):\n    G = nx.Graph()\n    trials = data.draw(st.integers(min_value=1, max_value=1000))\n    result = approximation.average_clustering(G, trials=trials)\n    assert result == 0\n\n@given(st.data())\ndef test_output_consistency_with_fixed_seed_property(data):\n    G = nx.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), data.draw(st.floats(min_value=0, max_value=1)))\n    trials = data.draw(st.integers(min_value=1, max_value=1000))\n    seed = 42\n    result1 = approximation.average_clustering(G, trials=trials, seed=seed)\n    result2 = approximation.average_clustering(G, trials=trials, seed=seed)\n    assert result1 == result2\n\n@given(st.data())\ndef test_output_approximation_convergence_property(data):\n    G = nx.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), data.draw(st.floats(min_value=0, max_value=1)))\n    trials = data.draw(st.integers(min_value=1000, max_value=10000))\n    deterministic_result = nx.average_clustering(G)  # Using deterministic calculation\n    result = approximation.average_clustering(G, trials=trials)\n    assert abs(result - deterministic_result) < 0.1  # Allowing for a small margin of error\n\n@given(st.data())\ndef test_directed_graph_not_implemented_property(data):\n    G = nx.DiGraph()\n    trials = data.draw(st.integers(min_value=1, max_value=1000))\n    try:\n        approximation.average_clustering(G, trials=trials)\n        assert False, \"Expected NetworkXNotImplemented exception for directed graph\"\n    except nx.NetworkXNotImplemented:\n        pass  # Expected behavior, test passes\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1), min_size=1)\ndef test_output_between_0_and_1_property(graph_edges):\n    G = nx.Graph()\n    G.add_edges_from(graph_edges)\n    result = approximation.average_clustering(G, trials=1000)\n    assert 0 <= result <= 1\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1), min_size=1)\ndef test_output_zero_for_no_edges_property(graph_edges):\n    G = nx.Graph()\n    G.add_edges_from(graph_edges)\n    if len(G.edges) == 0 or all(len(list(G.neighbors(n))) < 2 for n in G.nodes):\n        result = approximation.average_clustering(G, trials=1000)\n        assert result == 0\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1), st.integers(min_value=0, max_value=100))\ndef test_reproducibility_with_fixed_seed_property(graph_edges, seed):\n    G = nx.Graph()\n    G.add_edges_from(graph_edges)\n    result1 = approximation.average_clustering(G, trials=1000, seed=seed)\n    result2 = approximation.average_clustering(G, trials=1000, seed=seed)\n    assert result1 == result2\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1), st.integers(min_value=0, max_value=100))\ndef test_approximation_convergence_property(graph_edges, trials):\n    G = nx.Graph()\n    G.add_edges_from(graph_edges)\n    deterministic_result = nx.average_clustering(G)\n    result = approximation.average_clustering(G, trials=trials)\n    assert abs(result - deterministic_result) < 0.1  # Allow some margin of error\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1), min_size=1)\ndef test_directed_graph_exception_property(graph_edges):\n    G = nx.DiGraph()\n    G.add_edges_from(graph_edges)\n    try:\n        approximation.average_clustering(G, trials=1000)\n        assert False, \"Expected NetworkXNotImplemented exception for directed graph\"\n    except nx.NetworkXNotImplemented:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.data())\ndef test_output_range_property():\n    G = nx.erdos_renyi_graph(10, 0.5)\n    trials = st.integers(min_value=1, max_value=1000).example()\n    result = approximation.average_clustering(G, trials=trials)\n    assert 0 <= result <= 1\n\n@given(st.data())\ndef test_zero_output_property():\n    G = nx.Graph()\n    trials = st.integers(min_value=1, max_value=1000).example()\n    result = approximation.average_clustering(G, trials=trials)\n    assert result == 0\n\n@given(st.data())\ndef test_reproducibility_property():\n    G = nx.erdos_renyi_graph(10, 0.5)\n    trials = 1000\n    seed = 42\n    result1 = approximation.average_clustering(G, trials=trials, seed=seed)\n    result2 = approximation.average_clustering(G, trials=trials, seed=seed)\n    assert result1 == result2\n\n@given(st.data())\ndef test_convergence_to_true_value_property():\n    G = nx.erdos_renyi_graph(100, 0.5)\n    trials = 10000\n    result_approx = approximation.average_clustering(G, trials=trials)\n    result_true = nx.average_clustering(G)\n    assert abs(result_approx - result_true) < 0.05\n\n@given(st.data())\ndef test_directed_graph_exception_property():\n    G = nx.DiGraph()\n    with pytest.raises(nx.NetworkXNotImplemented):\n        approximation.average_clustering(G)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st, settings\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1, max_size=100), unique=True))\ndef test_output_range_property(edges):\n    G = nx.Graph(edges)\n    result = approximation.average_clustering(G)\n    assert 0 <= result <= 1\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, max_size=100))\ndef test_output_zero_property(nodes):\n    G = nx.Graph()\n    G.add_nodes_from(nodes)\n    result = approximation.average_clustering(G)\n    assert result == 0\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1, max_size=100), st.integers(1, 1000))\ndef test_reproducibility_property(edges, trials):\n    G = nx.Graph(edges)\n    seed = 42\n    result1 = approximation.average_clustering(G, trials=trials, seed=seed)\n    result2 = approximation.average_clustering(G, trials=trials, seed=seed)\n    assert result1 == result2\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1, max_size=100), st.integers(1, 1000))\ndef test_approximation_convergence_property(edges, trials):\n    G = nx.Graph(edges)\n    approximate_result = approximation.average_clustering(G, trials=trials)\n    exact_result = nx.average_clustering(G)\n    assert abs(approximate_result - exact_result) < 0.1  # Allow some tolerance\n\n@given(st.lists(st.tuples(st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)), min_size=1, max_size=100))\ndef test_directed_graph_exception_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        approximation.average_clustering(G)\n        assert False, \"Expected NetworkXNotImplemented exception was not raised.\"\n    except nx.NetworkXNotImplemented:\n        pass  # Expected behavior, test passes\n\n# End program\n```"], "properties": ["1. The output value must be between 0 and 1, inclusive, since the average clustering coefficient represents a fraction of triangles and cannot exceed this range.\n\n2. The output should be 0 if the graph has no edges or if all nodes have fewer than 2 neighbors, as there can be no triangles formed in such cases.\n\n3. The output should be consistent across multiple runs with the same input parameters (graph and trials) when a fixed random seed is provided, ensuring reproducibility.\n\n4. The output should be approximately equal to the average clustering coefficient calculated using a deterministic method for large graphs, as the approximation should converge to the true value with an adequate number of trials.\n\n5. If the graph is directed, the function should raise a `NetworkXNotImplemented` exception, indicating that the average clustering coefficient cannot be computed for directed graphs."], "api_doc": "average_clustering\naverage_clustering(G, trials=1000, seed=None)[source]\nEstimates the average clustering coefficient of G.\n\nThe local clustering of each node in G is the fraction of triangles that actually exist over all possible triangles in its neighborhood. The average clustering coefficient of a graph G is the mean of local clusterings.\n\nThis function finds an approximate average clustering coefficient for G by repeating n times (defined in trials) the following experiment: choose a node at random, choose two of its neighbors at random, and check if they are connected. The approximate coefficient is the fraction of triangles found over the number of trials [1].\n\nParameters\n:\nG\nNetworkX graph\ntrials\ninteger\nNumber of trials to perform (default 1000).\n\nseed\ninteger, random_state, or None (default)\nIndicator of random number generation state. See Randomness.\n\nReturns\n:\nc\nfloat\nApproximated average clustering coefficient.\n\nRaises\n:\nNetworkXNotImplemented\nIf G is directed.\n\nReferences\n\n[1]\nSchank, Thomas, and Dorothea Wagner. Approximating clustering coefficient and transitivity. Universit\u00e4t Karlsruhe, Fakult\u00e4t f\u00fcr Informatik, 2004. https://doi.org/10.5445/IR/1000001239\n\nExamples\n\nfrom networkx.algorithms import approximation\nG = nx.erdos_renyi_graph(10, 0.2, seed=10)\napproximation.average_clustering(G, trials=1000, seed=10)\n0.214", "api_code": "@not_implemented_for(\"directed\")\n@py_random_state(2)\n@nx._dispatchable(name=\"approximate_average_clustering\")\ndef average_clustering(G, trials=1000, seed=None):\n    n = len(G)\n    triangles = 0\n    nodes = list(G)\n    for i in [int(seed.random() * n) for i in range(trials)]:\n        nbrs = list(G[nodes[i]])\n        if len(nbrs) < 2:\n            continue\n        u, v = seed.sample(nbrs, 2)\n        if u in G[v]:\n            triangles += 1\n    return triangles / trials"}
{"function_name": "statistics.pstdev", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom statistics import StatisticsError\nimport math\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_pstdev_non_negative_property(data):\n    result = statistics.pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_pstdev_single_element_property(data):\n    if len(data) == 1:\n        result = statistics.pstdev(data)\n        assert result == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_pstdev_variance_property(data):\n    variance = statistics.pvariance(data)\n    result = statistics.pstdev(data)\n    assert math.isclose(result, math.sqrt(variance))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_pstdev_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            statistics.pstdev(data)\n            assert False, \"Expected StatisticsError for empty data.\"\n        except StatisticsError:\n            pass\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_pstdev_repeatability_property(data):\n    result1 = statistics.pstdev(data)\n    result2 = statistics.pstdev(data)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_non_negative_output_property(data):\n    result = statistics.pstdev(data)\n    assert result >= 0\n# End program\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_single_element_zero_property(single_value):\n    result = statistics.pstdev([single_value])\n    assert result == 0\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_output_equals_sqrt_population_variance_property(data):\n    variance = statistics.pvariance(data)\n    result = statistics.pstdev(data)\n    assert result == variance ** 0.5\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_empty_data_raises_statistics_error_property(data):\n    if len(data) == 0:\n        try:\n            statistics.pstdev(data)\n            assert False, \"Expected StatisticsError not raised\"\n        except statistics.StatisticsError:\n            pass\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_consistent_output_property(data):\n    result1 = statistics.pstdev(data)\n    result2 = statistics.pstdev(data)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_output_non_negative_property(data):\n    result = statistics.pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_single_element_zero_property(data):\n    if len(data) == 1:\n        result = statistics.pstdev(data)\n        assert result == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_output_equals_sqrt_variance_property(data):\n    result = statistics.pstdev(data)\n    variance = statistics.pvariance(data)\n    assert result == variance ** 0.5\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_empty_input_raises_error_property(data):\n    if len(data) == 0:\n        try:\n            statistics.pstdev(data)\n            assert False  # Should not reach this line\n        except statistics.StatisticsError:\n            assert True  # Expected exception\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_consistent_output_property(data):\n    result1 = statistics.pstdev(data)\n    result2 = statistics.pstdev(data)\n    assert result1 == result2\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_output_non_negative_property(data):\n    result = statistics.pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_single_element_zero_property(data):\n    if len(data) == 1:\n        result = statistics.pstdev(data)\n        assert result == 0\n\n@given(st.lists(st.floats(), min_size=2))\ndef test_variance_relation_property(data):\n    variance = statistics.pvariance(data)\n    result = statistics.pstdev(data)\n    assert result**2 == variance\n\n@given(st.lists(st.floats(), min_size=0))\ndef test_empty_input_statistics_error_property(data):\n    if len(data) == 0:\n        try:\n            statistics.pstdev(data)\n            assert False, \"Expected StatisticsError for empty input\"\n        except statistics.StatisticsError:\n            pass\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_repeatability_property(data):\n    result1 = statistics.pstdev(data)\n    result2 = statistics.pstdev(data)\n    assert result1 == result2\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import pstdev, StatisticsError\nimport math\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_output_non_negative_property(data):\n    result = pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_single_element_zero_property(data):\n    if len(data) == 1:\n        result = pstdev(data)\n        assert result == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_output_equals_sqrt_variance_property(data):\n    result = pstdev(data)\n    variance = sum((x - sum(data) / len(data)) ** 2 for x in data) / len(data)\n    assert math.isclose(result, math.sqrt(variance))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_empty_data_raises_error_property(data):\n    if len(data) == 0:\n        try:\n            pstdev(data)\n            assert False, \"Expected StatisticsError for empty data\"\n        except StatisticsError:\n            pass\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_repeated_calls_consistent_property(data):\n    result1 = pstdev(data)\n    result2 = pstdev(data)\n    assert result1 == result2\n# End program\n```"], "properties": ["1. The output of `pstdev` should always be a non-negative number, as standard deviation cannot be negative.\n2. If the input data contains only one element, the output of `pstdev` should be zero, since there is no variability in a single data point.\n3. The output of `pstdev` should be equal to the square root of the population variance calculated from the same input data.\n4. If the input data is empty, `pstdev` should raise a `StatisticsError`, indicating that at least one data point is required.\n5. The output of `pstdev` should be consistent and repeatable for the same input data, meaning that calling the function multiple times with the same arguments should yield the same result."], "api_doc": "statistics.pstdev(data, mu=None)\nReturn the population standard deviation (the square root of the population variance). See pvariance() for arguments and other details.\n\n>>>\npstdev([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])\n0.986893273527251", "api_code": "def pstdev(data, mu=None):\n    T, ss, c, n = _ss(data, mu)\n    if n < 1:\n        raise StatisticsError('pstdev requires at least one data point')\n    mss = ss / n\n    if issubclass(T, Decimal):\n        return _decimal_sqrt_of_frac(mss.numerator, mss.denominator)\n    return _float_sqrt_of_frac(mss.numerator, mss.denominator)"}
{"function_name": "datetime.date.weekday", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_output_range_property(date):\n    result = date.weekday()\n    assert 0 <= result <= 6\n# End program\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_consistency_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n# End program\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_monday_property(date):\n    if date.weekday() == 0:  # If it's Monday\n        assert date.weekday() == 0\n# End program\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_sunday_property(date):\n    if date.weekday() == 6:  # If it's Sunday\n        assert date.weekday() == 6\n# End program\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_cyclic_pattern_property(date):\n    next_day = date + datetime.timedelta(days=1)\n    assert (date.weekday() + 1) % 7 == next_day.weekday()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    assert 0 <= date.weekday() <= 6\n\n@given(st.dates())\ndef test_output_is_consistent_property(date):\n    assert date.weekday() == date.weekday()\n\n@given(st.dates())\ndef test_monday_output_is_0_property(date):\n    if date.weekday() == 0:  # Monday\n        assert date.weekday() == 0\n\n@given(st.dates())\ndef test_sunday_output_is_6_property(date):\n    if date.weekday() == 6:  # Sunday\n        assert date.weekday() == 6\n\n@given(st.dates())\ndef test_cyclic_pattern_property(date):\n    if date.weekday() < 6:  # Not Sunday\n        assert date.weekday() + 1 == (date + datetime.timedelta(days=1)).weekday() % 7\n    else:  # Sunday\n        assert (date.weekday() + 1) % 7 == (date + datetime.timedelta(days=1)).weekday()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert 0 <= result <= 6\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_consistent_output_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_monday_output_is_0_property(date):\n    if date.weekday() == 0:  # If the date is a Monday\n        assert date.weekday() == 0\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_sunday_output_is_6_property(date):\n    if date.weekday() == 6:  # If the date is a Sunday\n        assert date.weekday() == 6\n\n@given(st.dates(min_value=datetime.date(1, 1, 1), max_value=datetime.date(9999, 12, 31)))\ndef test_cyclic_pattern_property(date):\n    if date.month < 12:  # Avoid overflow to the next year\n        next_day = date + datetime.timedelta(days=1)\n        assert (date.weekday() + 1) % 7 == next_day.weekday()\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int) and 0 <= result <= 6\n\n@given(st.dates())\ndef test_output_is_consistent_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n\n@given(st.dates())\ndef test_monday_output_is_0_property(date):\n    if date.weekday() == 0:  # If it's Monday\n        assert date.weekday() == 0\n\n@given(st.dates())\ndef test_sunday_output_is_6_property(date):\n    if date.weekday() == 6:  # If it's Sunday\n        assert date.weekday() == 6\n\n@given(st.dates())\ndef test_cyclic_pattern_property(date):\n    previous_date = date - datetime.timedelta(days=1)\n    next_date = date + datetime.timedelta(days=1)\n    assert (previous_date.weekday() + 1) % 7 == date.weekday()\n    assert (date.weekday() + 1) % 7 == next_date.weekday()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int) and 0 <= result <= 6\n\n@given(st.dates())\ndef test_output_is_consistent_property(date):\n    result1 = date.weekday()\n    result2 = date.weekday()\n    assert result1 == result2\n\n@given(st.dates())\ndef test_monday_output_is_0_property(date):\n    if date.weekday() == 0:  # If the date is a Monday\n        assert date.weekday() == 0\n\n@given(st.dates())\ndef test_sunday_output_is_6_property(date):\n    if date.weekday() == 6:  # If the date is a Sunday\n        assert date.weekday() == 6\n\n@given(st.dates())\ndef test_cyclic_pattern_property(date):\n    weekday_output = date.weekday()\n    next_day = date + datetime.timedelta(days=1)\n    next_weekday_output = next_day.weekday()\n    assert (next_weekday_output - weekday_output) % 7 == 1\n# End program\n```"], "properties": ["1. The output of the `weekday()` function is always an integer between 0 and 6, inclusive.\n2. The output for any given date remains consistent across multiple calls to `weekday()` for the same date.\n3. If the input date corresponds to a Monday, the output of `weekday()` should be 0.\n4. If the input date corresponds to a Sunday, the output of `weekday()` should be 6.\n5. The output of `weekday()` should exhibit a cyclic pattern, where consecutive days differ by 1, and the transition from Sunday (6) to Monday (0) wraps around."], "api_doc": "date.weekday()\nReturn the day of the week as an integer, where Monday is 0 and Sunday is 6. For example, date(2002, 12, 4).weekday() == 2, a Wednesday. See also isoweekday().", "api_code": "def weekday(self):\n    return (self.toordinal() + 6) % 7"}
{"function_name": "decimal.Decimal.from_float", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_is_instance_of_Decimal_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_represents_nearest_decimal_equivalent_property(f):\n    if f != 0:\n        result = Decimal.from_float(f)\n        expected = Decimal(str(f))\n        assert result == expected\n\n@given(st.one_of(st.just(float('nan')), st.just(float('inf')), st.just(float('-inf'))))\ndef test_output_handles_special_float_values_property(f):\n    result = Decimal.from_float(f)\n    if math.isnan(f):\n        assert result == Decimal('NaN')\n    elif math.isinf(f):\n        assert result == Decimal('Infinity') if f > 0 else Decimal('-Infinity')\n\n@given(st.integers())\ndef test_output_equivalent_to_decimal_of_integer_property(f):\n    result = Decimal.from_float(f)\n    expected = Decimal(str(abs(f))) if f >= 0 else -Decimal(str(abs(f)))\n    assert result == expected\n\n@given(st.floats())\ndef test_output_maintains_sign_of_input_property(f):\n    result = Decimal.from_float(f)\n    if f < 0:\n        assert result < 0\n    elif f > 0:\n        assert result > 0\n    else:\n        assert result == Decimal('0')\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_is_instance_of_Decimal_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_correctly_represents_float_property(f):\n    result = Decimal.from_float(f)\n    expected = Decimal(str(f))\n    assert result == expected\n\n@given(st.one_of(st.just(float('nan')), st.just(float('inf')), st.just(float('-inf'))))\ndef test_output_correctly_handles_special_float_values_property(f):\n    result = Decimal.from_float(f)\n    if math.isnan(f):\n        assert result == Decimal('NaN')\n    elif math.isinf(f):\n        assert result == Decimal('Infinity') if f > 0 else Decimal('-Infinity')\n\n@given(st.integers())\ndef test_output_equivalent_to_decimal_of_integer_property(i):\n    result = Decimal.from_float(float(i))\n    expected = Decimal(str(abs(i))) * (1 if i >= 0 else -1)\n    assert result == expected\n\n@given(st.floats())\ndef test_output_maintains_sign_of_input_property(f):\n    result = Decimal.from_float(f)\n    assert (result.is_signed() and f < 0) or (not result.is_signed() and f >= 0)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_from_float_instance_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats())\ndef test_decimal_from_float_nearest_decimal_property(f):\n    result = Decimal.from_float(f)\n    expected = Decimal(str(f))\n    assert result == expected\n\n@given(st.one_of(st.just(float('nan')), st.just(float('inf')), st.just(float('-inf'))))\ndef test_decimal_from_float_special_values_property(f):\n    result = Decimal.from_float(f)\n    if math.isnan(f):\n        assert result == Decimal('NaN')\n    elif f == float('inf'):\n        assert result == Decimal('Infinity')\n    elif f == float('-inf'):\n        assert result == Decimal('-Infinity')\n\n@given(st.integers())\ndef test_decimal_from_float_integer_conversion_property(i):\n    result = Decimal.from_float(i)\n    expected = Decimal(str(abs(i))) if i >= 0 else -Decimal(str(abs(i)))\n    assert result == expected\n\n@given(st.floats())\ndef test_decimal_from_float_sign_property(f):\n    result = Decimal.from_float(f)\n    assert (result.is_signed() and f < 0) or (not result.is_signed() and f >= 0)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_is_instance_of_decimal_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats())\ndef test_output_represents_nearest_decimal_equivalent_property(f):\n    result = Decimal.from_float(f)\n    expected = Decimal(str(f))\n    assert result == expected\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_handles_special_float_values_property(f):\n    result = Decimal.from_float(f)\n    if math.isnan(f):\n        assert result == Decimal('NaN')\n    elif math.isinf(f):\n        assert result == Decimal('Infinity') if f > 0 else Decimal('-Infinity')\n\n@given(st.integers())\ndef test_output_equivalent_to_decimal_of_absolute_integer_property(i):\n    result = Decimal.from_float(i)\n    expected = Decimal(str(abs(i))) * (1 if i >= 0 else -1)\n    assert result == expected\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_maintains_sign_of_input_property(f):\n    result = Decimal.from_float(f)\n    assert (result.is_signed() and f < 0) or (not result.is_signed() and f >= 0)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_is_instance_of_Decimal_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_represents_nearest_decimal_equivalent_property(f):\n    result = Decimal.from_float(f)\n    expected = Decimal(str(f))\n    assert result == expected\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_special_float_values_property(f):\n    if math.isnan(f):\n        assert Decimal.from_float(f) == Decimal('NaN')\n    elif math.isinf(f):\n        assert Decimal.from_float(f) == Decimal('Infinity') if f > 0 else Decimal('-Infinity')\n\n@given(st.integers())\ndef test_integer_conversion_property(i):\n    result = Decimal.from_float(i)\n    expected = Decimal(str(abs(i)))\n    assert result == expected\n    assert result.is_signed() == (i < 0)\n\n@given(st.floats())\ndef test_sign_of_output_matches_input_property(f):\n    result = Decimal.from_float(f)\n    assert (result.is_signed() == (f < 0))\n# End program\n```"], "properties": ["1. The output must be an instance of the `Decimal` class.\n2. The output must accurately represent the nearest decimal equivalent of the input float value, particularly for floating-point numbers that cannot be exactly represented (e.g., `0.1`).\n3. The output must correctly handle special float values, returning `Decimal('NaN')` for `float('nan')`, `Decimal('Infinity')` for `float('inf')`, and `Decimal('-Infinity')` for `float('-inf')`.\n4. For integer inputs, the output must be equivalent to `Decimal(str(abs(f)))` with the appropriate sign, confirming that the integer is correctly converted to a decimal representation.\n5. The output must maintain the same sign as the input float or integer, ensuring that positive inputs yield positive `Decimal` instances and negative inputs yield negative `Decimal` instances."], "api_doc": "classmethod from_float(f)\nAlternative constructor that only accepts instances of float or int.\n\nNote Decimal.from_float(0.1) is not the same as Decimal('0.1'). Since 0.1 is not exactly representable in binary floating point, the value is stored as the nearest representable value which is 0x1.999999999999ap-4. That equivalent value in decimal is 0.1000000000000000055511151231257827021181583404541015625.\n\nNote From Python 3.2 onwards, a Decimal instance can also be constructed directly from a float.\n>>>\nDecimal.from_float(0.1)\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\nDecimal.from_float(float('nan'))\nDecimal('NaN')\nDecimal.from_float(float('inf'))\nDecimal('Infinity')\nDecimal.from_float(float('-inf'))\nDecimal('-Infinity')\nAdded in version 3.1.", "api_code": "@classmethod\ndef from_float(cls, f):\n    if isinstance(f, int):                # handle integer inputs\n        sign = 0 if f >= 0 else 1\n        k = 0\n        coeff = str(abs(f))\n    elif isinstance(f, float):\n        if _math.isinf(f) or _math.isnan(f):\n            return cls(repr(f))\n        if _math.copysign(1.0, f) == 1.0:\n            sign = 0\n        else:\n            sign = 1\n        n, d = abs(f).as_integer_ratio()\n        k = d.bit_length() - 1\n        coeff = str(n*5**k)\n    else:\n        raise TypeError(\"argument must be int or float.\")\n\n    result = _dec_from_triple(sign, coeff, -k)\n    if cls is Decimal:\n        return result\n    else:\n        return cls(result)"}
{"function_name": "decimal.Decimal.shift", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_output_sign_remains_unchanged_property(d, shift_amount):\n    result = d.shift(shift_amount)\n    assert result.sign() == d.sign()\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_output_exponent_remains_unchanged_property(d, shift_amount):\n    result = d.shift(shift_amount)\n    assert result._exp == d._exp\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_shifting_by_zero_returns_original_operand_property(d):\n    result = d.shift(0)\n    assert result == d\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.integers())\ndef test_shifting_left_increases_coefficient_length_property(d, shift_amount):\n    if shift_amount > 0:\n        result = d.shift(shift_amount)\n        assert len(result._int) > len(d._int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.integers())\ndef test_shifting_right_decreases_coefficient_length_property(d, shift_amount):\n    if shift_amount < 0:\n        result = d.shift(shift_amount)\n        assert len(result._int) < len(d._int)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_shift_output_sign_property(decimal, shift_amount):\n    result = decimal.shift(shift_amount)\n    assert result.sign() == decimal.sign()\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_shift_output_exponent_property(decimal, shift_amount):\n    result = decimal.shift(shift_amount)\n    assert result._exp == decimal._exp\n\n@given(st.decimals())\ndef test_shift_zero_property(decimal):\n    result = decimal.shift(0)\n    assert result == decimal\n\n@given(st.decimals(), st.integers())\ndef test_shift_left_right_property(decimal, shift_amount):\n    if shift_amount > 0:\n        result = decimal.shift(shift_amount)\n        assert len(result._int) > len(decimal._int)  # Coefficient should increase\n    elif shift_amount < 0:\n        result = decimal.shift(shift_amount)\n        assert len(result._int) < len(decimal._int)  # Coefficient should decrease\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_shift_precision_property(decimal, shift_amount):\n    result = decimal.shift(shift_amount)\n    assert len(result._int) <= getcontext().prec  # Coefficient should not exceed precision\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n# Set a reasonable precision for testing\ngetcontext().prec = 50\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_shift_sign_property(value, shift_amount):\n    result = value.shift(shift_amount)\n    assert result._sign == value._sign\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_shift_exponent_property(value, shift_amount):\n    result = value.shift(shift_amount)\n    assert result._exp == value._exp\n\n@given(st.decimals(), st.integers())\ndef test_shift_zero_property(value, shift_amount):\n    result = value.shift(0)\n    assert result == value\n\n@given(st.decimals(), st.integers())\ndef test_shift_left_right_property(value, shift_amount):\n    result = value.shift(shift_amount)\n    if shift_amount < 0:\n        assert len(str(result)) <= len(str(value))\n    else:\n        assert len(str(result)) >= len(str(value))\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_shift_precision_property(value, shift_amount):\n    result = value.shift(shift_amount)\n    assert len(str(result)) <= getcontext().prec\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_output_sign_same_as_input_sign_property(x, shift_amount):\n    result = x.shift(shift_amount)\n    assert result._sign == x._sign\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_exponent_unchanged_property(x, shift_amount):\n    result = x.shift(shift_amount)\n    assert result._exp == x._exp\n\n@given(st.decimals(), st.integers())\ndef test_shift_by_zero_returns_original_property(x, any_integer):\n    result = x.shift(0)\n    assert result == x\n\n@given(st.decimals(), st.integers())\ndef test_shifting_left_increases_digits_property(x, shift_amount):\n    if shift_amount > 0:\n        result = x.shift(shift_amount)\n        assert len(result._int) > len(x._int) or result == Decimal('0')\n\n@given(st.decimals(), st.integers())\ndef test_shifting_right_decreases_digits_property(x, shift_amount):\n    if shift_amount < 0:\n        result = x.shift(shift_amount)\n        assert len(result._int) < len(x._int) or result == Decimal('0')\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_output_precision_property(x, shift_amount):\n    result = x.shift(shift_amount)\n    assert len(result._int) <= getcontext().prec\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_sign_of_output_property(x, shift_amount):\n    result = x.shift(shift_amount)\n    assert result._sign == x._sign\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_exponent_of_output_property(x, shift_amount):\n    result = x.shift(shift_amount)\n    assert result._exp == x._exp\n\n@given(st.decimals())\ndef test_shift_by_zero_property(x):\n    result = x.shift(0)\n    assert result == x\n\n@given(st.decimals(), st.integers())\ndef test_left_and_right_shift_property(x, shift_amount):\n    if shift_amount > 0:\n        result = x.shift(shift_amount)\n        assert len(result._int) > len(x._int)  # Left shift increases digits\n    elif shift_amount < 0:\n        result = x.shift(shift_amount)\n        assert len(result._int) < len(x._int) or result == Decimal(0)  # Right shift decreases digits or results in zero\n\n@given(st.decimals(), st.integers(min_value=-getcontext().prec, max_value=getcontext().prec))\ndef test_precision_limit_property(x, shift_amount):\n    result = x.shift(shift_amount)\n    assert len(result._int) <= getcontext().prec  # Output should not exceed context precision\n# End program\n```"], "properties": ["1. The output's sign should be the same as the input's sign, meaning the sign of the first operand remains unchanged after the shift operation.\n\n2. The output should have an exponent that remains the same as the input's exponent, indicating that shifting does not affect the exponent of the first operand.\n\n3. Shifting by zero should return the original operand, confirming that no changes occur when the shift amount is zero.\n\n4. Shifting left (positive integer) should increase the number of digits in the coefficient, while shifting right (negative integer) should decrease the number of digits, except when the result is zero.\n\n5. The output's coefficient should not exceed the defined precision in the context, ensuring that the result adheres to the maximum allowable digits as specified by the context's precision."], "api_doc": "shift(other, context=None)\nReturn the result of shifting the digits of the first operand by an amount specified by the second operand. The second operand must be an integer in the range -precision through precision. The absolute value of the second operand gives the number of places to shift. If the second operand is positive then the shift is to the left; otherwise the shift is to the right. Digits shifted into the coefficient are zeros. The sign and exponent of the first operand are unchanged.", "api_code": "def shift(self, other, context=None):\n    if context is None:\n        context = getcontext()\n\n    other = _convert_other(other, raiseit=True)\n\n    ans = self._check_nans(other, context)\n    if ans:\n        return ans\n\n    if other._exp != 0:\n        return context._raise_error(InvalidOperation)\n    if not (-context.prec <= int(other) <= context.prec):\n        return context._raise_error(InvalidOperation)\n\n    if self._isinfinity():\n        return Decimal(self)\n\n    # get values, pad if necessary\n    torot = int(other)\n    rotdig = self._int\n    topad = context.prec - len(rotdig)\n    if topad > 0:\n        rotdig = '0'*topad + rotdig\n    elif topad < 0:\n        rotdig = rotdig[-topad:]\n\n    # let's shift!\n    if torot < 0:\n        shifted = rotdig[:torot]\n    else:\n        shifted = rotdig + '0'*torot\n        shifted = shifted[-context.prec:]\n\n    return _dec_from_triple(self._sign,\n                                shifted.lstrip('0') or '0', self._exp)"}
{"function_name": "decimal.Decimal.exp", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_exp_positive_output_property(x):\n    result = Decimal(x).exp()\n    assert result > 0\n\n@given(st.just(0))\ndef test_decimal_Decimal_exp_zero_property(x):\n    result = Decimal(x).exp()\n    assert result == Decimal(1)\n\n@given(st.floats(allow_nan=False, allow_infinity=True, min_value=float('-inf'), max_value=-1e+10))\ndef test_decimal_Decimal_exp_negative_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result == Decimal(0)\n\n@given(st.floats(allow_nan=False, allow_infinity=True, min_value=1e+10, max_value=float('inf')))\ndef test_decimal_Decimal_exp_positive_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result == Decimal('Infinity')\n\n@given(st.floats())\ndef test_decimal_Decimal_exp_rounding_mode_property(x):\n    result = Decimal(x).exp()\n    assert result == result.quantize(result, rounding='ROUND_HALF_EVEN')\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n# Set a high precision for Decimal calculations\ngetcontext().prec = 50\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_decimal_Decimal_exp_positive_output_property(x):\n    result = Decimal(x).exp()\n    if x == float('inf'):\n        assert result == Decimal('Infinity')\n    elif x == float('-inf'):\n        assert result == Decimal(0)\n    elif x != float('nan'):\n        assert result > 0\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_exp_zero_input_property(x):\n    result = Decimal(0).exp()\n    assert result == Decimal(1)\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=-1e-1))\ndef test_decimal_Decimal_exp_negative_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result < 1\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=1e100, max_value=1e200))\ndef test_decimal_Decimal_exp_positive_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result == Decimal('Infinity')\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_exp_rounding_property(x):\n    result = Decimal(x).exp()\n    # Since we can't directly check the rounding mode, we will check if the output is a Decimal\n    # and if it is rounded to the expected precision by verifying the format.\n    assert isinstance(result, Decimal)\n    assert result == result.quantize(Decimal('1.0000000000000000000000000000000000000000000000000'), rounding='ROUND_HALF_EVEN')\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.floats(allow_nan=False, allow_infinity=False))  # Generate finite floats\ndef test_decimal_Decimal_exp_positive_output_property(x):\n    result = Decimal(x).exp()\n    assert result > 0  # Exponential function should always return a positive value\n\n@given(st.just(0.0))  # Test only the input 0\ndef test_decimal_Decimal_exp_zero_property():\n    result = Decimal(0).exp()\n    assert result == Decimal(1)  # Exponential function should return 1 for input 0\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=0))  # Generate negative floats\ndef test_decimal_Decimal_exp_negative_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result < 1  # Exponential function should approach 0 as input approaches negative infinity\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=0, max_value=1e100))  # Generate positive floats\ndef test_decimal_Decimal_exp_positive_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result > 1  # Exponential function should approach infinity as input approaches positive infinity\n\n@given(st.floats())\ndef test_decimal_Decimal_exp_rounding_property(x):\n    result = Decimal(x).exp()\n    # Check if the result is correctly rounded using ROUND_HALF_EVEN\n    context = getcontext()\n    context.rounding = 'ROUND_HALF_EVEN'\n    expected = result.quantize(Decimal('1.0'))  # Example quantization for checking rounding\n    assert result == expected  # Ensure the result is correctly rounded\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e300, max_value=1e300))\ndef test_decimal_exp_positive_output_property(x):\n    result = Decimal(x).exp()\n    assert result > 0\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e300, max_value=1e300))\ndef test_decimal_exp_zero_input_property(x):\n    if x == 0:\n        result = Decimal(x).exp()\n        assert result == Decimal(1)\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=float('-inf'), max_value=-1e300))\ndef test_decimal_exp_negative_infinity_property(x):\n    if x == float('-inf'):\n        result = Decimal(x).exp()\n        assert result == Decimal(0)\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=1e300, max_value=float('inf')))\ndef test_decimal_exp_positive_infinity_property(x):\n    if x == float('inf'):\n        result = Decimal(x).exp()\n        assert result == Decimal('Infinity')\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e300, max_value=1e300))\ndef test_decimal_exp_rounding_property(x):\n    result = Decimal(x).exp()\n    expected = result.quantize(Decimal('1e-15'))  # Example rounding for comparison\n    assert result == expected\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.floats(allow_nan=False, allow_infinity=False, width=32))\ndef test_decimal_Decimal_exp_positive_output_property(value):\n    result = Decimal(value).exp()\n    assert result > 0  # The output should always be positive\n\n@given(st.integers())\ndef test_decimal_Decimal_exp_zero_input_property(value):\n    if value == 0:\n        result = Decimal(value).exp()\n        assert result == Decimal(1)  # The output should be 1 when input is 0\n\n@given(st.floats(allow_nan=False, allow_infinity=False, width=32))\ndef test_decimal_Decimal_exp_negative_infinity_property(value):\n    if value < 0:\n        result = Decimal(value).exp()\n        assert result < 1  # The output should be close to 0 for large negative inputs\n\n@given(st.floats(allow_nan=False, allow_infinity=True, width=32))\ndef test_decimal_Decimal_exp_infinity_property(value):\n    if value == float('inf'):\n        result = Decimal(value).exp()\n        assert result == Decimal('Infinity')  # The output should be Infinity for positive infinity\n    elif value == float('-inf'):\n        result = Decimal(value).exp()\n        assert result == Decimal(0)  # The output should be 0 for negative infinity\n\n@given(st.floats(allow_nan=False, allow_infinity=False, width=32))\ndef test_decimal_Decimal_exp_rounding_property(value):\n    result = Decimal(value).exp()\n    # Check that the result is rounded correctly (this can be more complex in a real scenario)\n    expected = Decimal(value).exp().quantize(Decimal('1.00000000000000000000'))  # Example precision\n    assert result == expected  # The output should be correctly rounded\n# End program\n```"], "properties": ["1. The output of the exponential function for any real number input should always be a positive Decimal value, except when the input is negative infinity or NaN.\n\n2. The exponential function should return 1 when the input is 0.\n\n3. The output of the exponential function should approach 0 as the input approaches negative infinity.\n\n4. The output of the exponential function should approach positive infinity as the input approaches positive infinity.\n\n5. The output of the exponential function for any input should be correctly rounded using the ROUND_HALF_EVEN rounding mode."], "api_doc": "exp(context=None)\nReturn the value of the (natural) exponential function e**x at the given number. The result is correctly rounded using the ROUND_HALF_EVEN rounding mode.\n\n>>>\nDecimal(1).exp()\nDecimal('2.718281828459045235360287471')\nDecimal(321).exp()\nDecimal('2.561702493119680037517373933E+139')", "api_code": "def exp(self, context=None):\n\n    if context is None:\n        context = getcontext()\n\n    # exp(NaN) = NaN\n    ans = self._check_nans(context=context)\n    if ans:\n        return ans\n\n    # exp(-Infinity) = 0\n    if self._isinfinity() == -1:\n        return _Zero\n\n    # exp(0) = 1\n    if not self:\n        return _One\n\n    # exp(Infinity) = Infinity\n    if self._isinfinity() == 1:\n        return Decimal(self)"}
{"function_name": "decimal.Decimal.as_integer_ratio", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, Overflow, InvalidOperation\nimport math\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_lowest_terms_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert math.gcd(n, d) == 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_positive_denominator_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert d > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_accuracy_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert Decimal(n) / Decimal(d) == decimal_value\n\n@given(st.one_of(st.decimals(allow_infinity=True), st.decimals(allow_nan=True)))\ndef test_as_integer_ratio_overflow_error_property(decimal_value):\n    if decimal_value.is_infinite():\n        try:\n            decimal_value.as_integer_ratio()\n            assert False, \"Expected OverflowError for infinity\"\n        except OverflowError:\n            pass\n\n@given(st.decimals(allow_infinity=False))\ndef test_as_integer_ratio_nan_property(decimal_value):\n    if decimal_value.is_nan():\n        try:\n            decimal_value.as_integer_ratio()\n            assert False, \"Expected ValueError for NaN\"\n        except ValueError:\n            pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, Overflow, InvalidOperation\nimport math\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_lowest_terms_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert math.gcd(n, d) == 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_positive_denominator_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert d > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_equivalence_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert Decimal(n) / Decimal(d) == decimal_value\n\n@given(st.one_of(st.decimals(allow_nan=False, allow_infinity=True), st.floats(allow_nan=False, allow_infinity=True)))\ndef test_decimal_as_integer_ratio_overflow_property(decimal_value):\n    if decimal_value == Decimal('Infinity') or decimal_value == Decimal('-Infinity'):\n        try:\n            decimal_value.as_integer_ratio()\n            assert False, \"Expected OverflowError for infinity\"\n        except OverflowError:\n            pass\n\n@given(st.decimals(allow_nan=True, allow_infinity=False))\ndef test_decimal_as_integer_ratio_nan_property(decimal_value):\n    if decimal_value.is_nan():\n        try:\n            decimal_value.as_integer_ratio()\n            assert False, \"Expected ValueError for NaN\"\n        except ValueError:\n            pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport decimal\nimport math\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_lowest_terms_property(d):\n    n, d = d.as_integer_ratio()\n    assert math.gcd(n, d) == 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_positive_denominator_property(d):\n    n, d = d.as_integer_ratio()\n    assert d > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_equivalence_property(d):\n    n, d = d.as_integer_ratio()\n    assert d != 0  # Ensure denominator is not zero\n    assert d == 1 or d == -1 or n / d == float(d)\n\n@given(st.one_of(st.decimals(allow_nan=True), st.decimals(allow_infinity=True)))\ndef test_decimal_as_integer_ratio_overflow_property(d):\n    if d.is_nan() or d.is_infinite():\n        try:\n            d.as_integer_ratio()\n        except (ValueError, OverflowError):\n            pass  # Expected behavior\n\n@given(st.decimals(allow_nan=True, allow_infinity=True))\ndef test_decimal_as_integer_ratio_nan_property(d):\n    if d.is_nan():\n        try:\n            d.as_integer_ratio()\n        except ValueError:\n            pass  # Expected behavior\n    elif d.is_infinite():\n        try:\n            d.as_integer_ratio()\n        except OverflowError:\n            pass  # Expected behavior\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, OverflowError, InvalidOperation\nfrom math import gcd\n\n@given(st.decimals())\ndef test_output_fraction_lowest_terms_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert gcd(n, d) == 1  # Check if n and d are coprime\n\n@given(st.decimals())\ndef test_output_denominator_positive_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert d > 0  # Check if the denominator is positive\n\n@given(st.decimals())\ndef test_output_fraction_equals_decimal_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert Decimal(n) / Decimal(d) == decimal_value  # Check if n/d equals the original Decimal\n\n@given(st.one_of(st.decimals().filter(lambda x: x.is_infinite()), st.decimals().filter(lambda x: x.is_nan())))\ndef test_overflow_and_nan_property(decimal_value):\n    try:\n        decimal_value.as_integer_ratio()\n        if decimal_value.is_infinite():\n            assert False, \"Expected OverflowError for infinity\"\n        elif decimal_value.is_nan():\n            assert False, \"Expected ValueError for NaN\"\n    except OverflowError:\n        if not decimal_value.is_infinite():\n            raise\n    except ValueError:\n        if not decimal_value.is_nan():\n            raise\n\n@given(st.decimals())\ndef test_output_fraction_correctness_property(decimal_value):\n    if decimal_value.is_finite():\n        n, d = decimal_value.as_integer_ratio()\n        assert Decimal(n) / Decimal(d) == decimal_value  # Check if n/d equals the original Decimal\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport decimal\nimport math\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_lowest_terms_property(d):\n    n, d = d.as_integer_ratio()\n    assert math.gcd(n, d) == 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_positive_denominator_property(d):\n    n, d = d.as_integer_ratio()\n    assert d > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_as_integer_ratio_accuracy_property(d):\n    n, d = d.as_integer_ratio()\n    assert decimal.Decimal(n) / decimal.Decimal(d) == d\n\n@given(st.one_of(st.decimals(allow_nan=False, allow_infinity=True), st.integers()))\ndef test_decimal_as_integer_ratio_overflow_property(val):\n    if isinstance(val, decimal.Decimal) and val.is_infinite():\n        try:\n            val.as_integer_ratio()\n            assert False, \"Expected OverflowError for infinity\"\n        except OverflowError:\n            pass\n\n@given(st.decimals(allow_nan=True, allow_infinity=False))\ndef test_decimal_as_integer_ratio_nan_property(d):\n    if d.is_nan():\n        try:\n            d.as_integer_ratio()\n            assert False, \"Expected ValueError for NaN\"\n        except ValueError:\n            pass\n# End program\n```"], "properties": ["1. The output pair (n, d) should represent the Decimal instance as a fraction in lowest terms, meaning that the greatest common divisor (GCD) of n and d must be 1.\n\n2. The denominator d in the output pair (n, d) must always be a positive integer.\n\n3. If the Decimal instance is a finite number, the output pair (n, d) should accurately represent the value of the Decimal instance when evaluated as a float (i.e., n / d should equal the Decimal value).\n\n4. The function should raise an OverflowError if the Decimal instance represents positive or negative infinity.\n\n5. The function should raise a ValueError if the Decimal instance is NaN (not a number)."], "api_doc": "as_integer_ratio()\nReturn a pair (n, d) of integers that represent the given Decimal instance as a fraction, in lowest terms and with a positive denominator:\n\n>>>\nDecimal('-3.14').as_integer_ratio()\n(-157, 50)\nThe conversion is exact. Raise OverflowError on infinities and ValueError on NaNs.\n\nAdded in version 3.6.", "api_code": "def as_integer_ratio(self):\n    return (self._numerator, self._denominator)"}
{"function_name": "statistics.median", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_median_returns_middle_value_for_odd_length_property(data):\n    sorted_data = sorted(data)\n    n = len(sorted_data)\n    if n % 2 == 1:\n        expected = sorted_data[n // 2]\n        assert statistics.median(data) == expected\n\n@given(st.lists(st.integers(), min_size=2))\ndef test_median_returns_average_of_middle_values_for_even_length_property(data):\n    sorted_data = sorted(data)\n    n = len(sorted_data)\n    if n % 2 == 0:\n        i = n // 2\n        expected = (sorted_data[i - 1] + sorted_data[i]) / 2\n        assert statistics.median(data) == expected\n\n@given(st.lists(st.integers(), min_size=0))\ndef test_median_raises_statistics_error_for_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            statistics.median(data)\n            assert False  # Should not reach this line\n        except statistics.StatisticsError:\n            assert True  # Expected error raised\n\n@given(st.lists(st.integers()))\ndef test_median_is_robust_against_outliers_property(data):\n    outlier_value = max(data) * 10 if data else 1000\n    data_with_outlier = data + [outlier_value]\n    median_without_outlier = statistics.median(data)\n    median_with_outlier = statistics.median(data_with_outlier)\n    assert median_with_outlier == median_without_outlier or median_with_outlier == statistics.median(sorted(data_with_outlier))\n\n@given(st.lists(st.integers()))\ndef test_median_is_order_invariant_property(data):\n    sorted_data = sorted(data)\n    assert statistics.median(data) == statistics.median(sorted_data)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.integers(), max_size=1000))\ndef test_median_middle_value_property(data):\n    if len(data) % 2 == 1:  # Odd length\n        sorted_data = sorted(data)\n        expected = sorted_data[len(data) // 2]\n        assert median(data) == expected\n\n@given(st.lists(st.integers(), min_size=2, max_size=1000))\ndef test_median_average_of_middle_values_property(data):\n    if len(data) % 2 == 0:  # Even length\n        sorted_data = sorted(data)\n        expected = (sorted_data[len(data) // 2 - 1] + sorted_data[len(data) // 2]) / 2\n        assert median(data) == expected\n\n@given(st.lists(st.integers(), max_size=1000))\ndef test_median_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            median(data)\n            assert False, \"Expected StatisticsError for empty data\"\n        except StatisticsError:\n            pass  # This is the expected outcome\n\n@given(st.lists(st.integers(), max_size=1000))\ndef test_median_outlier_property(data):\n    # The presence of outliers should not affect the median compared to the mean\n    mean_value = sum(data) / len(data) if data else 0\n    sorted_data = sorted(data)\n    median_value = median(data)\n    assert abs(median_value - mean_value) < 1e6  # Arbitrary large difference threshold\n\n@given(st.lists(st.integers(), max_size=1000))\ndef test_median_order_invariance_property(data):\n    assert median(data) == median(sorted(data))  # The median should be the same regardless of order\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.integers(), max_size=1000))  # Test with lists of integers up to size 1000\ndef test_median_middle_value_property(data):\n    if len(data) % 2 == 1:\n        sorted_data = sorted(data)\n        assert median(data) == sorted_data[len(sorted_data) // 2]\n\n@given(st.lists(st.integers(), min_size=2, max_size=1000))  # Test with lists of integers of even size\ndef test_median_average_of_middle_values_property(data):\n    sorted_data = sorted(data)\n    assert median(data) == (sorted_data[len(sorted_data) // 2 - 1] + sorted_data[len(sorted_data) // 2]) / 2\n\n@given(st.lists(st.integers(), max_size=1000))  # Test with lists of integers\ndef test_median_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            median(data)\n            assert False  # Should not reach here\n        except StatisticsError:\n            assert True  # Expected exception raised\n\n@given(st.lists(st.integers(), max_size=1000))  # Test with lists of integers\ndef test_median_robustness_against_outliers_property(data):\n    outlier_data = data + [max(data) + 1]  # Adding an outlier\n    regular_median = median(data)\n    outlier_median = median(outlier_data)\n    assert outlier_median != (regular_median + 1)  # Outlier shouldn't affect median significantly\n\n@given(st.lists(st.integers(), max_size=1000))  # Test with lists of integers\ndef test_median_order_independence_property(data):\n    sorted_data = sorted(data)\n    assert median(data) == median(sorted_data)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_median_middle_value_property(data):\n    sorted_data = sorted(data)\n    n = len(sorted_data)\n    expected_output = sorted_data[n // 2] if n % 2 == 1 else (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2\n    assert median(data) == expected_output\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_median_average_of_middle_values_property(data):\n    if len(data) == 0:\n        try:\n            median(data)\n            assert False  # should raise exception\n        except StatisticsError:\n            pass\n    else:\n        sorted_data = sorted(data)\n        n = len(sorted_data)\n        if n % 2 == 0:\n            expected_output = (sorted_data[n // 2 - 1] + sorted_data[n // 2]) / 2\n            assert median(data) == expected_output\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_median_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            median(data)\n            assert False  # should raise exception\n        except StatisticsError:\n            pass\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_median_outlier_robustness_property(data):\n    sorted_data = sorted(data)\n    n = len(sorted_data)\n    if n == 0:\n        return  # Skip empty lists\n    mean_value = sum(sorted_data) / n\n    median_value = median(data)\n    assert abs(median_value - mean_value) <= abs(median_value - mean(sorted_data))  # Median should be less affected by outliers\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_median_order_independence_property(data):\n    assert median(data) == median(sorted(data))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import median, StatisticsError\n\n@given(st.lists(st.integers(min_value=-10**6, max_value=10**6), min_size=1))\ndef test_median_odd_length_property(data):\n    sorted_data = sorted(data)\n    result = median(data)\n    assert result == sorted_data[len(sorted_data) // 2]\n\n@given(st.lists(st.integers(min_value=-10**6, max_value=10**6), min_size=2))\ndef test_median_even_length_property(data):\n    sorted_data = sorted(data)\n    result = median(data)\n    mid_index = len(sorted_data) // 2\n    assert result == (sorted_data[mid_index - 1] + sorted_data[mid_index]) / 2\n\n@given(st.lists(st.integers(min_value=-10**6, max_value=10**6), min_size=0))\ndef test_median_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            median(data)\n            assert False, \"Expected StatisticsError for empty data\"\n        except StatisticsError:\n            pass  # Expected behavior\n\n@given(st.lists(st.integers(min_value=-10**6, max_value=10**6)))\ndef test_median_robustness_property(data):\n    mean_value = sum(data) / len(data) if data else float('nan')\n    result = median(data)\n    assert abs(result - mean_value) > 1e-6  # Median should differ from mean if outliers are present\n\n@given(st.lists(st.integers(min_value=-10**6, max_value=10**6)))\ndef test_median_order_invariance_property(data):\n    result1 = median(data)\n    result2 = median(reversed(data))\n    assert result1 == result2  # Median should be the same regardless of order\n\n# End program\n```"], "properties": ["1. The output is the middle value of the sorted input data when the number of elements is odd.\n2. The output is the average of the two middle values of the sorted input data when the number of elements is even.\n3. The function raises a StatisticsError when the input data is empty.\n4. The output is unaffected by the presence of outliers compared to the mean of the dataset.\n5. The output remains consistent regardless of the order of the input data, as the function sorts the data before processing."], "api_doc": "statistics.median(data)\nReturn the median (middle value) of numeric data, using the common \u201cmean of middle two\u201d method. If data is empty, StatisticsError is raised. data can be a sequence or iterable.\n\nThe median is a robust measure of central location and is less affected by the presence of outliers. When the number of data points is odd, the middle data point is returned:\n\n>>>\nmedian([1, 3, 5])\n3\nWhen the number of data points is even, the median is interpolated by taking the average of the two middle values:\n\n>>>\nmedian([1, 3, 5, 7])\n4.0\nThis is suited for when your data is discrete, and you don\u2019t mind that the median may not be an actual data point.\n\nIf the data is ordinal (supports order operations) but not numeric (doesn\u2019t support addition), consider using median_low() or median_high() instead.", "api_code": "def median(data):\n    data = sorted(data)\n    n = len(data)\n    if n == 0:\n        raise StatisticsError(\"no median for empty data\")\n    if n % 2 == 1:\n        return data[n // 2]\n    else:\n        i = n // 2\n        return (data[i - 1] + data[i]) / 2"}
{"function_name": "statistics.linear_regression", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x))))\ndef test_slope_is_float_property(x, y):\n    slope, intercept = linear_regression(x, y)\n    assert isinstance(slope, float)\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x))))\ndef test_intercept_is_float_property(x, y):\n    slope, intercept = linear_regression(x, y)\n    assert isinstance(intercept, float)\n\n@given(st.lists(st.floats(min_value=1, max_value=1000), min_size=3).filter(lambda x: len(set(x)) == 3),\n               st.lists(st.floats(min_value=1, max_value=1000), min_size=3).filter(lambda x: len(x) == len(x))))\ndef test_equal_values_slope_property(x, y):\n    slope, intercept = linear_regression(x, y)\n    assert slope == 1.0\n    assert intercept == 0.0  # This will hold if proportional is true\n\n@given(st.lists(st.floats(min_value=0, max_value=10), min_size=3).filter(lambda x: len(set(x)) == 1), \n               st.lists(st.floats(min_value=0, max_value=10), min_size=3).filter(lambda x: len(x) == len(x))))\ndef test_constant_x_raises_exception_property(x, y):\n    try:\n        linear_regression(x, y)\n        assert False  # Should not reach this line\n    except StatisticsError:\n        pass  # Expected behavior\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x)), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x)).filter(lambda y: len(y) == len(x))))\ndef test_equal_length_inputs_property(x, y):\n    if len(x) != len(y) or len(x) < 2:\n        try:\n            linear_regression(x, y)\n            assert False  # Should not reach this line\n        except StatisticsError:\n            pass  # Expected behavior\n    else:\n        slope, intercept = linear_regression(x, y)\n        assert isinstance(slope, float) and isinstance(intercept, float)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom statistics import StatisticsError\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000))\ndef test_slope_output_property(x, y):\n    slope, intercept = statistics.linear_regression(x, y)\n    assert isinstance(slope, float)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000))\ndef test_intercept_output_property(x, y):\n    slope, intercept = statistics.linear_regression(x, y)\n    assert isinstance(intercept, float)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000))\ndef test_constant_input_property(x, y):\n    constant_x = [5] * len(y)\n    try:\n        statistics.linear_regression(constant_x, y)\n        assert False, \"Expected StatisticsError for constant x\"\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000).filter(lambda lst: len(lst) == len(x)))\ndef test_equal_input_values_property(x, y):\n    same_values = [1] * len(x)\n    slope, intercept = statistics.linear_regression(same_values, same_values)\n    assert slope == 1\n    assert intercept == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000), \n                st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000).filter(lambda lst: len(lst) == len(x)))\ndef test_length_check_property(x, y):\n    if len(x) != len(y):\n        try:\n            statistics.linear_regression(x, y)\n            assert False, \"Expected StatisticsError due to unequal lengths\"\n        except StatisticsError:\n            pass\n    if len(x) < 2:\n        try:\n            statistics.linear_regression(x, y)\n            assert False, \"Expected StatisticsError due to less than two points\"\n        except StatisticsError:\n            pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_slope_is_float_property(x, y):\n    slope, _ = linear_regression(x, y)\n    assert isinstance(slope, float)\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_intercept_is_float_property(x, y):\n    _, intercept = linear_regression(x, y)\n    assert isinstance(intercept, float)\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) == 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_constant_x_raises_statistics_error_property(x, y):\n    try:\n        linear_regression(x, y)\n        assert False, \"Expected StatisticsError not raised\"\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats(), min_size=2), \n       st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_identical_x_and_y_yields_slope_one_property(x, y):\n    if x == y:\n        slope, _ = linear_regression(x, y)\n        assert slope == 1.0\n\n@given(st.lists(st.floats(), min_size=2), \n       st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_equal_length_inputs_property(x, y):\n    if len(x) != len(y):\n        try:\n            linear_regression(x, y)\n            assert False, \"Expected StatisticsError not raised\"\n        except StatisticsError:\n            pass\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_slope_is_float_property(x, y):\n    slope, _ = linear_regression(x, y)\n    assert isinstance(slope, float)\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_intercept_is_float_property(x, y):\n    _, intercept = linear_regression(x, y)\n    assert isinstance(intercept, float)\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_equal_values_slope_intercept_property(x, y):\n    if x == y:\n        slope, intercept = linear_regression(x, y)\n        assert slope == 1\n        assert intercept == 0\n\n@given(st.lists(st.floats(), min_size=2, unique=True).map(lambda x: [5] * len(x)), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_constant_x_raises_statistics_error_property(x, y):\n    try:\n        linear_regression(x, y)\n        assert False, \"Expected StatisticsError not raised\"\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats(), min_size=2), st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x[0]))))\ndef test_equal_length_inputs_property(x, y):\n    if len(x) != len(y) or len(x) < 2:\n        try:\n            linear_regression(x, y)\n            assert False, \"Expected StatisticsError not raised\"\n        except StatisticsError:\n            pass\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import linear_regression, StatisticsError\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1))\n)\ndef test_slope_property(x, y):\n    slope, intercept = linear_regression(x, y)\n    assert isinstance(slope, float)  # Slope should be a float.\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1))\n)\ndef test_intercept_property(x, y):\n    slope, intercept = linear_regression(x, y)\n    assert isinstance(intercept, float)  # Intercept should be a float.\n\n@given(st.lists(st.floats(), min_size=3).filter(lambda x: len(set(x)) == len(x)), \n               st.lists(st.floats(), min_size=3).filter(lambda x: len(set(x)) == len(x)))\n)\ndef test_equal_values_property(x, y):\n    # When x and y are equal, the slope should be 1 and intercept should be 0.\n    slope, intercept = linear_regression(x, x)\n    assert slope == 1\n    assert intercept == 0\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) == 1), \n               st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) > 1))\n)\ndef test_constant_x_property(y, constant_x):\n    # When x is constant, expect a StatisticsError\n    try:\n        linear_regression(constant_x, y)\n        assert False, \"Expected StatisticsError for constant x\"\n    except StatisticsError:\n        pass  # This is the expected outcome.\n\n@given(st.lists(st.floats(), min_size=2), \n       st.lists(st.floats(), min_size=2).filter(lambda x: len(x) == len(x)))\n)\ndef test_input_length_property(x, y):\n    # Check that both inputs have the same length\n    if len(x) != len(y):\n        try:\n            linear_regression(x, y)\n            assert False, \"Expected StatisticsError for unequal lengths\"\n        except StatisticsError:\n            pass  # This is the expected outcome.\n    else:\n        # If they are equal, we should not raise an error.\n        slope, intercept = linear_regression(x, y)\n        assert isinstance(slope, float)\n        assert isinstance(intercept, float)\n\n# End program\n```"], "properties": ["1. The slope of the linear regression line should be a float and represent the rate of change of the dependent variable with respect to the independent variable.\n\n2. The intercept of the linear regression line should be a float and represent the value of the dependent variable when the independent variable is zero (or zero when proportional is true).\n\n3. If both input lists have the same values (e.g., x = [1, 2, 3] and y = [1, 2, 3]), the slope should be equal to 1 and the intercept should be 0 (or undefined when proportional is true).\n\n4. If the input lists contain a constant value for the independent variable x (e.g., x = [5, 5, 5]), a StatisticsError should be raised, indicating that x cannot be constant.\n\n5. The output should handle input lists of varying lengths and should raise a StatisticsError if the lengths are not equal or if they contain fewer than two data points."], "api_doc": "statistics.linear_regression(x, y, /, *, proportional=False)\nReturn the slope and intercept of simple linear regression parameters estimated using ordinary least squares. Simple linear regression describes the relationship between an independent variable x and a dependent variable y in terms of this linear function:\n\ny = slope * x + intercept + noise\n\nwhere slope and intercept are the regression parameters that are estimated, and noise represents the variability of the data that was not explained by the linear regression (it is equal to the difference between predicted and actual values of the dependent variable).\n\nBoth inputs must be of the same length (no less than two), and the independent variable x cannot be constant; otherwise a StatisticsError is raised.\n\nFor example, we can use the release dates of the Monty Python films to predict the cumulative number of Monty Python films that would have been produced by 2019 assuming that they had kept the pace.\n\n>>>\nyear = [1971, 1975, 1979, 1982, 1983]\nfilms_total = [1, 2, 3, 4, 5]\nslope, intercept = linear_regression(year, films_total)\nround(slope * 2019 + intercept)\n16\nIf proportional is true, the independent variable x and the dependent variable y are assumed to be directly proportional. The data is fit to a line passing through the origin. Since the intercept will always be 0.0, the underlying linear function simplifies to:\n\ny = slope * x + noise\n\nContinuing the example from correlation(), we look to see how well a model based on major planets can predict the orbital distances for dwarf planets:\n\n>>>\nmodel = linear_regression(period_squared, dist_cubed, proportional=True)\nslope = model.slope\n\n# Dwarf planets:   Pluto,  Eris,    Makemake, Haumea, Ceres\norbital_periods = [90_560, 204_199, 111_845, 103_410, 1_680]  # days\npredicted_dist = [math.cbrt(slope * (p * p)) for p in orbital_periods]\nlist(map(round, predicted_dist))\n[5912, 10166, 6806, 6459, 414]\n\n[5_906, 10_152, 6_796, 6_450, 414]  # actual distance in million km\n[5906, 10152, 6796, 6450, 414]\nAdded in version 3.10.\n\nChanged in version 3.11: Added support for proportional.", "api_code": "def linear_regression(x, y, /, *, proportional=False):\n    n = len(x)\n    if len(y) != n:\n        raise StatisticsError('linear regression requires that both inputs have same number of data points')\n    if n < 2:\n        raise StatisticsError('linear regression requires at least two data points')\n    if not proportional:\n        xbar = fsum(x) / n\n        ybar = fsum(y) / n\n        x = [xi - xbar for xi in x]  # List because used three times below\n        y = (yi - ybar for yi in y)  # Generator because only used once below\n    sxy = sumprod(x, y) + 0.0        # Add zero to coerce result to a float\n    sxx = sumprod(x, x)\n    try:\n        slope = sxy / sxx   # equivalent to:  covariance(x, y) / variance(x)\n    except ZeroDivisionError:\n        raise StatisticsError('x is constant')\n    intercept = 0.0 if proportional else ybar - slope * xbar\n    return LinearRegression(slope=slope, intercept=intercept)"}
{"function_name": "networkx.algorithms.approximation.clique.large_clique_size", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom hypothesis import strategies as st\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=100).map(set).map(lambda nodes: nx.Graph(nx.path_graph(nodes))), \n                )\ndef test_large_clique_size_non_negative_integer_property(G):\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result >= 0\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=100).map(set).map(lambda nodes: nx.Graph(nx.path_graph(nodes))), \n                )\ndef test_large_clique_size_within_node_count_property(G):\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result <= G.number_of_nodes()\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=0).map(set).map(lambda nodes: nx.Graph(nx.path_graph(nodes))), \n                )\ndef test_large_clique_size_empty_graph_property(G):\n    result = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result == 0\n\n@given(st.lists(st.integers(min_value=1, max_value=100), min_size=1, max_size=100).map(set).map(lambda nodes: nx.Graph(nx.path_graph(nodes))), \n                )\ndef test_large_clique_size_at_least_one_edge_property(G):\n    if G.number_of_edges() > 0:\n        result = nx.algorithms.approximation.clique.large_clique_size(G)\n        assert result >= 1\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, max_size=100).map(set).map(lambda nodes: nx.Graph(nx.path_graph(nodes))), \n                )\ndef test_large_clique_size_consistent_output_property(G):\n    result1 = nx.algorithms.approximation.clique.large_clique_size(G)\n    result2 = nx.algorithms.approximation.clique.large_clique_size(G)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_large_clique_size_non_negative_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.path_graph, st.integers(min_value=0, max_value=100)),\n        st.builds(nx.complete_graph, st.integers(min_value=1, max_value=10))\n    ))\n    result = nx.approximation.large_clique_size(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_large_clique_size_not_exceed_node_count_property(data):\n    num_nodes = data.draw(st.integers(min_value=0, max_value=100))\n    G = nx.complete_graph(num_nodes)\n    result = nx.approximation.large_clique_size(G)\n    assert result <= num_nodes\n\n@given(st.data())\ndef test_large_clique_size_empty_graph_property(data):\n    G = nx.empty_graph(0)\n    result = nx.approximation.large_clique_size(G)\n    assert result == 0\n\n@given(st.data())\ndef test_large_clique_size_at_least_one_for_non_empty_graph_property(data):\n    num_nodes = data.draw(st.integers(min_value=1, max_value=100))\n    G = nx.complete_graph(num_nodes)\n    result = nx.approximation.large_clique_size(G)\n    assert result >= 1\n\n@given(st.data())\ndef test_large_clique_size_consistency_property(data):\n    num_nodes = data.draw(st.integers(min_value=1, max_value=100))\n    G = nx.complete_graph(num_nodes)\n    result1 = nx.approximation.large_clique_size(G)\n    result2 = nx.approximation.large_clique_size(G)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_large_clique_size_non_negative_property(data):\n    G = data.draw(st.sets(st.integers(min_size=1, max_size=100)).map(lambda nodes: nx.Graph(nx.complete_graph(nodes))))\n    result = nx.approximation.large_clique_size(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_large_clique_size_does_not_exceed_node_count_property(data):\n    G = data.draw(st.sets(st.integers(min_size=1, max_size=100)).map(lambda nodes: nx.Graph(nx.complete_graph(nodes))))\n    result = nx.approximation.large_clique_size(G)\n    assert result <= G.number_of_nodes()\n\n@given(st.data())\ndef test_large_clique_size_empty_graph_property(data):\n    G = nx.Graph()  # Empty graph\n    result = nx.approximation.large_clique_size(G)\n    assert result == 0\n\n@given(st.data())\ndef test_large_clique_size_at_least_one_edge_property(data):\n    G = data.draw(st.sets(st.integers(min_size=2, max_size=100)).map(lambda nodes: nx.Graph(nx.complete_graph(nodes))))\n    if G.number_of_edges() > 0:\n        result = nx.approximation.large_clique_size(G)\n        assert result >= 1\n\n@given(st.data())\ndef test_large_clique_size_consistency_property(data):\n    G = data.draw(st.sets(st.integers(min_size=1, max_size=100)).map(lambda nodes: nx.Graph(nx.complete_graph(nodes))))\n    result1 = nx.approximation.large_clique_size(G)\n    result2 = nx.approximation.large_clique_size(G)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_large_clique_size_non_negative_property(data):\n    G = data.draw(st.builds(nx.gnm_random_graph, st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)))\n    result = nx.approximation.large_clique_size(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_large_clique_size_not_exceed_node_count_property(data):\n    num_nodes = st.integers(min_value=0, max_value=100).example()\n    G = nx.empty_graph(num_nodes)\n    result = nx.approximation.large_clique_size(G)\n    assert result <= num_nodes\n\n@given(st.data())\ndef test_large_clique_size_empty_graph_property(data):\n    G = nx.empty_graph(0)\n    result = nx.approximation.large_clique_size(G)\n    assert result == 0\n\n@given(st.data())\ndef test_large_clique_size_at_least_one_for_non_empty_graph_property(data):\n    G = data.draw(st.builds(nx.gnm_random_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\n    result = nx.approximation.large_clique_size(G)\n    assert result >= 1\n\n@given(st.data())\ndef test_large_clique_size_consistency_property(data):\n    G = data.draw(st.builds(nx.gnm_random_graph, st.integers(min_value=0, max_value=100), st.integers(min_value=0, max_value=100)))\n    result1 = nx.approximation.large_clique_size(G)\n    result2 = nx.approximation.large_clique_size(G)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_large_clique_size_non_negative_property(data):\n    G = data.draw(st.one_of(nx.generators.random_graphs.random_graphs(), \n                             st.builds(nx.path_graph, st.integers(min_value=1, max_value=100)),\n                             st.builds(nx.complete_graph, st.integers(min_value=1, max_value=100))))\n    result = nx.approximation.large_clique_size(G)\n    assert result >= 0\n\n@given(st.data())\ndef test_large_clique_size_not_exceed_node_count_property(data):\n    num_nodes = data.draw(st.integers(min_value=1, max_value=100))\n    G = nx.complete_graph(num_nodes)\n    result = nx.approximation.large_clique_size(G)\n    assert result <= num_nodes\n\n@given(st.data())\ndef test_large_clique_size_empty_graph_property(data):\n    G = nx.empty_graph()\n    result = nx.approximation.large_clique_size(G)\n    assert result == 0\n\n@given(st.data())\ndef test_large_clique_size_at_least_one_edge_property(data):\n    G = data.draw(st.one_of(nx.generators.random_graphs.random_graphs(), \n                             st.builds(nx.path_graph, st.integers(min_value=2, max_value=100)),\n                             st.builds(nx.complete_graph, st.integers(min_value=2, max_value=100))))\n    if G.number_of_edges() > 0:\n        result = nx.approximation.large_clique_size(G)\n        assert result >= 1\n\n@given(st.data())\ndef test_large_clique_size_consistency_property(data):\n    G = data.draw(st.one_of(nx.generators.random_graphs.random_graphs(), \n                             st.builds(nx.path_graph, st.integers(min_value=1, max_value=100)),\n                             st.builds(nx.complete_graph, st.integers(min_value=1, max_value=100))))\n    result1 = nx.approximation.large_clique_size(G)\n    result2 = nx.approximation.large_clique_size(G)\n    assert result1 == result2\n# End program\n```"], "properties": ["1. The output size of the large clique must be a non-negative integer, as a clique cannot have negative size or be fractional.\n\n2. The size of the large clique returned should not exceed the total number of nodes in the graph, as it is not possible to have a clique larger than the number of available nodes.\n\n3. If the graph is empty (i.e., contains no nodes), the function should return a size of 0, indicating that there are no cliques in the graph.\n\n4. For any graph that contains at least one edge, the size of the large clique must be at least 1, since the smallest possible clique consists of a single node.\n\n5. The output should be consistent across multiple calls with the same input graph, meaning that calling the function multiple times with the same graph should yield the same size of the large clique, assuming the graph has not been modified."], "api_doc": "large_clique_size\nlarge_clique_size(G)[source]\nFind the size of a large clique in a graph.\n\nA clique is a subset of nodes in which each pair of nodes is adjacent. This function is a heuristic for finding the size of a large clique in the graph.\n\nParameters\n:\nG\nNetworkX graph\nReturns\n:\nk: integer\nThe size of a large clique in the graph.\n\nRaises\n:\nNetworkXNotImplemented\nIf the graph is directed or is a multigraph.\n\nSee also\n\nnetworkx.algorithms.approximation.clique.max_clique()\nA function that returns an approximate maximum clique with a guarantee on the approximation ratio.\n\nnetworkx.algorithms.clique\nFunctions for finding the exact maximum clique in a graph.\n\nNotes\n\nThis implementation is from [1]. Its worst case time complexity is \n, where n is the number of nodes in the graph and d is the maximum degree.\n\nThis function is a heuristic, which means it may work well in practice, but there is no rigorous mathematical guarantee on the ratio between the returned number and the actual largest clique size in the graph.\n\nReferences\n\n[1]\nPattabiraman, Bharath, et al. \u201cFast Algorithms for the Maximum Clique Problem on Massive Graphs with Applications to Overlapping Community Detection.\u201d Internet Mathematics 11.4-5 (2015): 421\u2013448. <https://doi.org/10.1080/15427951.2014.986778>\n\nExamples\n\nG = nx.path_graph(10)\nnx.approximation.large_clique_size(G)\n2", "api_code": "@not_implemented_for(\"directed\")\n@not_implemented_for(\"multigraph\")\n@nx._dispatchable\ndef large_clique_size(G):\n    degrees = G.degree\n\n    def _clique_heuristic(G, U, size, best_size):\n        if not U:\n            return max(best_size, size)\n        u = max(U, key=degrees)\n        U.remove(u)\n        N_prime = {v for v in G[u] if degrees[v] >= best_size}\n        return _clique_heuristic(G, U & N_prime, size + 1, best_size)\n\n    best_size = 0\n    nodes = (u for u in G if degrees[u] >= best_size)\n    for u in nodes:\n        neighbors = {v for v in G[u] if degrees[v] >= best_size}\n        best_size = _clique_heuristic(G, neighbors, 1, best_size)\n    return best_size"}
{"function_name": "datetime.datetime.combine", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, date, time\n\n@given(st.dates(), st.times(), st.booleans())\ndef test_year_month_day_property(input_date, input_time, tzinfo):\n    result = datetime.combine(input_date, input_time, tzinfo)\n    assert result.year == input_date.year\n    assert result.month == input_date.month\n    assert result.day == input_date.day\n\n@given(st.dates(), st.times(), st.booleans())\ndef test_hour_minute_second_microsecond_property(input_date, input_time, tzinfo):\n    result = datetime.combine(input_date, input_time, tzinfo)\n    assert result.hour == input_time.hour\n    assert result.minute == input_time.minute\n    assert result.second == input_time.second\n    assert result.microsecond == input_time.microsecond\n\n@given(st.dates(), st.times(), st.booleans())\ndef test_tzinfo_property(input_date, input_time, tzinfo):\n    result = datetime.combine(input_date, input_time, tzinfo)\n    expected_tzinfo = input_time.tzinfo if not tzinfo else tzinfo\n    assert result.tzinfo == expected_tzinfo\n\n@given(st.datetimes(), st.booleans())\ndef test_datetime_argument_property(input_datetime, tzinfo):\n    result = datetime.combine(input_datetime.date(), input_datetime.time(), tzinfo)\n    assert result == input_datetime.replace(hour=0, minute=0, second=0, microsecond=0)\n\n@given(st.dates(), st.times())\ndef test_fold_attribute_property(input_date, input_time):\n    result = datetime.combine(input_date, input_time)\n    assert result.fold == input_time.fold\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, date, time\n\n@given(st.datetimes(min_value=datetime(1, 1, 1), max_value=datetime(9999, 12, 31, 23, 59, 59, 999999)))\ndef test_year_month_day_property(dt):\n    d = dt.date()\n    t = dt.time()\n    combined = datetime.combine(d, t)\n    assert combined.year == d.year\n    assert combined.month == d.month\n    assert combined.day == d.day\n\n@given(st.datetimes(min_value=datetime(1, 1, 1), max_value=datetime(9999, 12, 31, 23, 59, 59, 999999)))\ndef test_time_components_property(dt):\n    d = dt.date()\n    t = dt.time()\n    combined = datetime.combine(d, t)\n    assert combined.hour == t.hour\n    assert combined.minute == t.minute\n    assert combined.second == t.second\n    assert combined.microsecond == t.microsecond\n\n@given(st.datetimes(min_value=datetime(1, 1, 1), max_value=datetime(9999, 12, 31, 23, 59, 59, 999999)),\n       st.one_of(st.none(), st.times()))\ndef test_tzinfo_property(dt, tzinfo):\n    d = dt.date()\n    t = dt.time(tzinfo=tzinfo)\n    combined = datetime.combine(d, t, tzinfo=tzinfo if tzinfo is not None else True)\n    assert combined.tzinfo == (tzinfo if tzinfo is not None else t.tzinfo)\n\n@given(st.datetimes(min_value=datetime(1, 1, 1), max_value=datetime(9999, 12, 31, 23, 59, 59, 999999)))\ndef test_datetime_input_property(dt):\n    combined = datetime.combine(dt, dt.time())\n    assert combined == dt\n\n@given(st.datetimes(min_value=datetime(1, 1, 1), max_value=datetime(9999, 12, 31, 23, 59, 59, 999999)))\ndef test_fold_attribute_property(dt):\n    d = dt.date()\n    t = dt.time(fold=0)\n    combined = datetime.combine(d, t)\n    assert combined.fold == t.fold\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n# Property 1: The year, month, and day of the output datetime object must match the year, month, and day of the input date object.\n@given(date=st.dates(), time=st.times())\ndef test_datetime_year_month_day_property(date, time):\n    combined = datetime.datetime.combine(date, time)\n    assert combined.year == date.year\n    assert combined.month == date.month\n    assert combined.day == date.day\n\n# Property 2: The hour, minute, second, and microsecond of the output datetime object must match the corresponding time components of the input time object.\n@given(date=st.dates(), time=st.times())\ndef test_datetime_hour_minute_second_microsecond_property(date, time):\n    combined = datetime.datetime.combine(date, time)\n    assert combined.hour == time.hour\n    assert combined.minute == time.minute\n    assert combined.second == time.second\n    assert combined.microsecond == time.microsecond\n\n# Property 3: If the tzinfo argument is provided, the tzinfo attribute of the output datetime object must match the provided tzinfo; otherwise, it must match the tzinfo attribute of the input time object.\n@given(date=st.dates(), time=st.times(), tzinfo=st.one_of(st.none(), st.builds(datetime.timezone, st.integers(min_value=-12, max_value=14))))\ndef test_datetime_tzinfo_property(date, time, tzinfo):\n    combined = datetime.datetime.combine(date, time, tzinfo)\n    expected_tzinfo = tzinfo if tzinfo is not None else time.tzinfo\n    assert combined.tzinfo == expected_tzinfo\n\n# Property 4: If the input date argument is a datetime object, the output datetime object must equal the input datetime object, ignoring its time components and tzinfo attributes.\n@given(dt=st.datetimes())\ndef test_datetime_combining_datetime_property(dt):\n    combined = datetime.datetime.combine(dt.date(), dt.time())\n    assert combined.date() == dt.date()\n    assert combined.time() == dt.time()\n\n# Property 5: The output datetime object's fold attribute must match the fold attribute of the input time object.\n@given(date=st.dates(), time=st.times())\ndef test_datetime_fold_property(date, time):\n    combined = datetime.datetime.combine(date, time)\n    assert combined.fold == time.fold\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, date, time, timedelta\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.times()), st.booleans())\ndef test_year_month_day_property(date_input, time_input, tzinfo_input, use_tzinfo):\n    result = datetime.combine(date_input, time_input, tzinfo=tzinfo_input if use_tzinfo else None)\n    assert result.year == date_input.year\n    assert result.month == date_input.month\n    assert result.day == date_input.day\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.times()), st.booleans())\ndef test_hour_minute_second_microsecond_property(date_input, time_input, tzinfo_input, use_tzinfo):\n    result = datetime.combine(date_input, time_input, tzinfo=tzinfo_input if use_tzinfo else None)\n    assert result.hour == time_input.hour\n    assert result.minute == time_input.minute\n    assert result.second == time_input.second\n    assert result.microsecond == time_input.microsecond\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.times()), st.booleans())\ndef test_tzinfo_property(date_input, time_input, tzinfo_input, use_tzinfo):\n    result = datetime.combine(date_input, time_input, tzinfo=tzinfo_input if use_tzinfo else None)\n    expected_tzinfo = tzinfo_input if use_tzinfo else time_input.tzinfo\n    assert result.tzinfo == expected_tzinfo\n\n@given(st.datetimes(), st.one_of(st.none(), st.times()), st.booleans())\ndef test_datetime_object_property(datetime_input, tzinfo_input, use_tzinfo):\n    date_input = datetime_input.date()\n    time_input = datetime_input.time()\n    result = datetime.combine(date_input, time_input, tzinfo=tzinfo_input if use_tzinfo else None)\n    assert result == datetime_input.replace(tzinfo=None)\n\n@given(st.dates(), st.times())\ndef test_fold_property(date_input, time_input):\n    result = datetime.combine(date_input, time_input)\n    assert result.fold == time_input.fold\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, date, time, timedelta\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.timedeltas()))\ndef test_date_components_match_property(date_arg, time_arg, tzinfo_arg):\n    result = datetime.combine(date_arg, time_arg, tzinfo=tzinfo_arg)\n    assert result.year == date_arg.year\n    assert result.month == date_arg.month\n    assert result.day == date_arg.day\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.timedeltas()))\ndef test_time_components_match_property(date_arg, time_arg, tzinfo_arg):\n    result = datetime.combine(date_arg, time_arg, tzinfo=tzinfo_arg)\n    assert result.hour == time_arg.hour\n    assert result.minute == time_arg.minute\n    assert result.second == time_arg.second\n    assert result.microsecond == time_arg.microsecond\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.timedeltas()))\ndef test_tzinfo_property(date_arg, time_arg, tzinfo_arg):\n    result = datetime.combine(date_arg, time_arg, tzinfo=tzinfo_arg)\n    expected_tzinfo = tzinfo_arg if tzinfo_arg is not None else time_arg.tzinfo\n    assert result.tzinfo == expected_tzinfo\n\n@given(st.datetimes(), st.one_of(st.times(), st.times().filter(lambda t: t.tzinfo is None)))\ndef test_datetime_equivalence_property(datetime_arg, time_arg):\n    result = datetime.combine(datetime_arg.date(), time_arg, datetime_arg.tzinfo)\n    assert result == datetime_arg.replace(hour=time_arg.hour, minute=time_arg.minute,\n                                          second=time_arg.second, microsecond=time_arg.microsecond)\n\n@given(st.dates(), st.times())\ndef test_fold_attribute_property(date_arg, time_arg):\n    result = datetime.combine(date_arg, time_arg)\n    assert result.fold == time_arg.fold\n\n# End program\n```"], "properties": ["1. The year, month, and day of the output datetime object must match the year, month, and day of the input date object.\n\n2. The hour, minute, second, and microsecond of the output datetime object must match the corresponding time components of the input time object.\n\n3. If the tzinfo argument is provided, the tzinfo attribute of the output datetime object must match the provided tzinfo; otherwise, it must match the tzinfo attribute of the input time object.\n\n4. If the input date argument is a datetime object, the output datetime object must equal the input datetime object, ignoring its time components and tzinfo attributes.\n\n5. The output datetime object's fold attribute must match the fold attribute of the input time object."], "api_doc": "classmethod datetime.combine(date, time, tzinfo=time.tzinfo)\nReturn a new datetime object whose date components are equal to the given date object\u2019s, and whose time components are equal to the given time object\u2019s. If the tzinfo argument is provided, its value is used to set the tzinfo attribute of the result, otherwise the tzinfo attribute of the time argument is used. If the date argument is a datetime object, its time components and tzinfo attributes are ignored.\n\nFor any datetime object d, d == datetime.combine(d.date(), d.time(), d.tzinfo).\n\nChanged in version 3.6: Added the tzinfo argument.", "api_code": "@classmethod\ndef combine(cls, date, time, tzinfo=True):\n    if not isinstance(date, _date_class):\n        raise TypeError(\"date argument must be a date instance\")\n    if not isinstance(time, _time_class):\n        raise TypeError(\"time argument must be a time instance\")\n    if tzinfo is True:\n        tzinfo = time.tzinfo\n    return cls(date.year, date.month, date.day,\n                time.hour, time.minute, time.second, time.microsecond,\n                tzinfo, fold=time.fold)"}
{"function_name": "decimal.Decimal.fma", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_output_equals_product_plus_third_property(self, other, third):\n    if self.is_nan() or other.is_nan():\n        return  # Skip testing if either is NaN\n    result = self.fma(other, third)\n    expected = self * other + third\n    assert result == expected\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_special_values_property(self, other, third):\n    context = getcontext()\n    if self.is_nan() or other.is_nan():\n        try:\n            result = self.fma(other, third)\n            assert result.is_nan()\n        except InvalidOperation:\n            pass  # Expected behavior, skip further checks\n    elif self.is_infinite() or other.is_infinite():\n        result = self.fma(other, third)\n        # Check that the result is either infinity or a valid number\n        assert result.is_infinite() or isinstance(result, Decimal)\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_sign_property(self, other, third):\n    if self.is_zero() or other.is_zero():\n        result = self.fma(other, third)\n        assert result == third  # If either is zero, result should be third\n    else:\n        result = self.fma(other, third)\n        expected_sign = (self.sign() ^ other.sign())  # XOR for sign\n        assert result.sign() == expected_sign\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_zero_multiplication_property(self, other, third):\n    if self.is_zero() or other.is_zero():\n        result = self.fma(other, third)\n        assert result == third  # Result should be equal to third\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_valid_decimal_instance_property(self, other, third):\n    result = self.fma(other, third)\n    assert isinstance(result, Decimal)  # Output should be a Decimal instance\n    assert result.is_finite()  # Ensure result does not cause overflow or NaN\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_output_property(self_val, other_val, third_val):\n    result = Decimal(self_val).fma(Decimal(other_val), Decimal(third_val))\n    expected = Decimal(self_val) * Decimal(other_val) + Decimal(third_val)\n    assert result == expected\n\n@given(st.decimals(), st.decimals())\ndef test_fma_special_values_property(self_val, other_val):\n    if Decimal(self_val)._is_special or Decimal(other_val)._is_special:\n        context = getcontext()\n        try:\n            result = Decimal(self_val).fma(Decimal(other_val), Decimal(0))\n            # Check for NaN or Infinity handling\n            assert result.is_nan() or result.is_infinite() or isinstance(result, Decimal)\n        except InvalidOperation:\n            pass\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_sign_property(self_val, other_val, third_val):\n    result = Decimal(self_val).fma(Decimal(other_val), Decimal(third_val))\n    product_sign = (Decimal(self_val).is_signed() ^ Decimal(other_val).is_signed())\n    if Decimal(self_val) == 0:\n        assert result == Decimal(third_val)\n    else:\n        assert result.is_signed() == product_sign\n\n@given(st.decimals(), st.decimals())\ndef test_fma_zero_multiplication_property(self_val, other_val):\n    result = Decimal(self_val).fma(Decimal(0), Decimal(0))\n    assert result == Decimal(0)\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_decimal_instance_property(self_val, other_val, third_val):\n    result = Decimal(self_val).fma(Decimal(other_val), Decimal(third_val))\n    assert isinstance(result, Decimal)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_output_equals_product_plus_third_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    expected = Decimal(a) * Decimal(b) + Decimal(c)\n    assert result == expected\n\n@given(st.decimals(), st.decimals())\ndef test_fma_special_values_property(a, b):\n    a_decimal = Decimal(a)\n    b_decimal = Decimal(b)\n    \n    if a_decimal.is_nan() or b_decimal.is_nan():\n        with pytest.raises(InvalidOperation):\n            a_decimal.fma(b_decimal, Decimal(0))\n    elif a_decimal.is_infinite() or b_decimal.is_infinite():\n        if a_decimal.is_infinite() and b_decimal.is_zero():\n            with pytest.raises(InvalidOperation):\n                a_decimal.fma(b_decimal, Decimal(0))\n        elif b_decimal.is_infinite() and a_decimal.is_zero():\n            with pytest.raises(InvalidOperation):\n                a_decimal.fma(b_decimal, Decimal(0))\n\n@given(st.decimals(), st.decimals())\ndef test_fma_sign_property(a, b):\n    a_decimal = Decimal(a)\n    b_decimal = Decimal(b)\n    third = Decimal(0)  # Using zero to isolate the product behavior\n    \n    result = a_decimal.fma(b_decimal, third)\n    expected_sign = (a_decimal.sign() ^ b_decimal.sign())\n    \n    if expected_sign == 0:  # Both are positive\n        assert result >= 0\n    else:  # One is negative\n        assert result < 0\n\n@given(st.decimals(), st.decimals())\ndef test_fma_zero_multiplication_property(a, b):\n    a_decimal = Decimal(a)\n    b_decimal = Decimal(b)\n    third = Decimal(5)  # Non-zero value for third\n    \n    result = a_decimal.fma(Decimal(0), third)\n    assert result == third\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_output_type_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    assert isinstance(result, Decimal)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext, InvalidOperation\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_output_equals_product_plus_third_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    expected = Decimal(a) * Decimal(b) + Decimal(c)\n    assert result == expected\n\n@given(st.decimals(), st.decimals())\ndef test_fma_special_values_property(a, b):\n    if Decimal(a)._is_special or Decimal(b)._is_special:\n        result = Decimal(a).fma(Decimal(b), Decimal(0))\n        if Decimal(a)._exp == 'N' or Decimal(b)._exp == 'N':\n            # Expecting an error for NaN\n            try:\n                Decimal(a).fma(Decimal(b), Decimal(0))\n                assert False, \"Expected InvalidOperation for NaN\"\n            except InvalidOperation:\n                pass\n        elif Decimal(a)._exp == 'F' or Decimal(b)._exp == 'F':\n            # Expecting infinity handling\n            assert isinstance(result, Decimal)  # Result should still be a Decimal\n        else:\n            assert True  # Normal case\n\n@given(st.decimals(), st.decimals())\ndef test_fma_sign_property(a, b):\n    if Decimal(a) != 0 and Decimal(b) != 0:\n        result = Decimal(a).fma(Decimal(b), Decimal(0))\n        assert (result.sign() == (Decimal(a).sign() * Decimal(b).sign()))\n\n@given(st.decimals(), st.decimals())\ndef test_fma_zero_multiplication_property(a, b):\n    result = Decimal(a).fma(0, Decimal(b))\n    assert result == Decimal(b)\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_result_is_decimal_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    assert isinstance(result, Decimal)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation, getcontext\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_output_equals_product_plus_third_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    expected = Decimal(a) * Decimal(b) + Decimal(c)\n    assert result == expected\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_special_values_property(a, b, c):\n    dec_a = Decimal(a)\n    dec_b = Decimal(b)\n    dec_c = Decimal(c)\n    \n    # Check behavior when one of the values is NaN\n    dec_a = Decimal('NaN') if a == 0 else dec_a\n    result = dec_a.fma(dec_b, dec_c)\n    assert result.is_nan() or isinstance(result, Decimal)\n\n    dec_b = Decimal('NaN') if b == 0 else dec_b\n    result = dec_a.fma(dec_b, dec_c)\n    assert result.is_nan() or isinstance(result, Decimal)\n\n    dec_a = Decimal('Infinity') if a == 0 else dec_a\n    result = dec_a.fma(dec_b, dec_c)\n    assert result in [Decimal('Infinity'), Decimal('-Infinity')] or isinstance(result, Decimal)\n\n    dec_b = Decimal('Infinity') if b == 0 else dec_b\n    result = dec_a.fma(dec_b, dec_c)\n    assert result in [Decimal('Infinity'), Decimal('-Infinity')] or isinstance(result, Decimal)\n\n@given(st.decimals(), st.decimals())\ndef test_fma_zero_multiplication_property(a, b):\n    result = Decimal(a).fma(Decimal(0), Decimal(b))\n    expected = Decimal(b)\n    assert result == expected\n\n@given(st.decimals(), st.decimals())\ndef test_fma_correct_sign_property(a, b):\n    dec_a = Decimal(a)\n    dec_b = Decimal(b)\n    dec_c = Decimal('0')  # Using zero for simplicity\n    product_sign = (dec_a.sign() ^ dec_b.sign())\n    result = dec_a.fma(dec_b, dec_c)\n    \n    if product_sign == 0:\n        assert result == dec_c\n    else:\n        assert result.sign() == product_sign\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_valid_decimal_instance_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    assert isinstance(result, Decimal)\n\n# End program\n```"], "properties": ["1. The output of `fma` should be equal to the sum of the product of `self` and `other` and `third`, specifically `self * other + third`, without any intermediate rounding errors.\n\n2. If either `self` or `other` is a special value (like NaN or Infinity), the output should conform to the rules of arithmetic for those special values, returning appropriate results or raising errors as defined.\n\n3. The output should maintain the same sign as the product of `self` and `other`, unless one of the inputs is zero, in which case the output should equal `third`.\n\n4. The function should consistently handle edge cases, such as multiplying by zero, resulting in the output being equal to `third`, regardless of the values of `self` and `other`.\n\n5. The output should be a valid `Decimal` instance, preserving the precision and scale characteristics defined by the context, and should not introduce any unexpected rounding or overflow errors."], "api_doc": "fma(other, third, context=None)\nFused multiply-add. Return self*other+third with no rounding of the intermediate product self*other.\n\n>>>\nDecimal(2).fma(3, 5)\nDecimal('11')", "api_code": "def fma(self, other, third, context=None):\n    if self._is_special or other._is_special:\n        if context is None:\n            context = getcontext()\n        if self._exp == 'N':\n            return context._raise_error(InvalidOperation, 'sNaN', self)\n        if other._exp == 'N':\n            return context._raise_error(InvalidOperation, 'sNaN', other)\n        if self._exp == 'n':\n            product = self\n        elif other._exp == 'n':\n            product = other\n        elif self._exp == 'F':\n            if not other:\n                return context._raise_error(InvalidOperation,\n                                            'INF * 0 in fma')\n            product = _SignedInfinity[self._sign ^ other._sign]\n        elif other._exp == 'F':\n            if not self:\n                return context._raise_error(InvalidOperation,\n                                            '0 * INF in fma')\n            product = _SignedInfinity[self._sign ^ other._sign]\n    else:\n        product = _dec_from_triple(self._sign ^ other._sign,\n                                    str(int(self._int) * int(other._int)),\n                                    self._exp + other._exp)\n\n    return product.__add__(third, context)"}
{"function_name": "pandas.DataFrame.sort_values", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_output_row_count_property(data):\n    df = data.draw(st.dataframes(\n        columns=st.columns(['col1', 'col2', 'col3']),\n        rows=st.lists(st.fixed_dictionaries({\n            'col1': st.one_of(st.text(), st.none()),\n            'col2': st.integers(),\n            'col3': st.integers(),\n        }), min_size=1, max_size=1000),\n        index=st.lists(st.text(), min_size=1, max_size=1000)\n    ))\n    \n    sorted_df = df.sort_values(by='col1', inplace=False)\n    assert len(sorted_df) == len(df)\n\n@given(st.data())\ndef test_sorted_by_columns_property(data):\n    df = data.draw(st.dataframes(\n        columns=st.columns(['col1', 'col2']),\n        rows=st.lists(st.fixed_dictionaries({\n            'col1': st.text(),\n            'col2': st.integers(),\n        }), min_size=1, max_size=1000),\n        index=st.lists(st.text(), min_size=1, max_size=1000)\n    ))\n\n    sorted_df = df.sort_values(by='col1', ascending=True)\n    assert sorted_df['col1'].is_monotonic_increasing\n\n@given(st.data())\ndef test_na_position_first_last_property(data):\n    df = data.draw(st.dataframes(\n        columns=st.columns(['col1']),\n        rows=st.lists(st.fixed_dictionaries({\n            'col1': st.one_of(st.text(), st.none()),\n        }), min_size=1, max_size=1000),\n        index=st.lists(st.text(), min_size=1, max_size=1000)\n    ))\n\n    sorted_df_first = df.sort_values(by='col1', na_position='first')\n    assert sorted_df_first['col1'].isnull().all() == sorted_df_first['col1'].iloc[:sorted_df_first['col1'].isnull().sum()].isnull().all()\n\n    sorted_df_last = df.sort_values(by='col1', na_position='last')\n    assert sorted_df_last['col1'].isnull().all() == sorted_df_last['col1'].iloc[-sorted_df_last['col1'].isnull().sum():].isnull().all()\n\n@given(st.data())\ndef test_multiple_columns_sorting_property(data):\n    df = data.draw(st.dataframes(\n        columns=st.columns(['col1', 'col2']),\n        rows=st.lists(st.fixed_dictionaries({\n            'col1': st.text(),\n            'col2': st.integers(),\n        }), min_size=1, max_size=1000),\n        index=st.lists(st.text(), min_size=1, max_size=1000)\n    ))\n\n    sorted_df = df.sort_values(by=['col1', 'col2'], ascending=[True, True])\n    assert sorted_df['col1'].is_monotonic_increasing\n    assert sorted_df.groupby('col1')['col2'].is_monotonic_increasing.all()\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.dataframes(\n        columns=st.columns(['col1', 'col2']),\n        rows=st.lists(st.fixed_dictionaries({\n            'col1': st.text(),\n            'col2': st.integers(),\n        }), min_size=1, max_size=1000),\n        index=st.lists(st.text(), min_size=1, max_size=1000)\n    ))\n\n    sorted_df = df.sort_values(by='col1', ignore_index=True)\n    assert sorted_df.index.equals(pd.Index(range(len(sorted_df))))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_output_rows_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())),\n                                          st.column('col2', elements=st.integers())],\n                                  index=st.range_indexes(min_size=1, max_size=1000)))\n    sorted_df = df.sort_values(by='col1', inplace=False)\n    assert len(sorted_df) == len(df)\n\n@given(st.data())\ndef test_sorted_output_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())),\n                                          st.column('col2', elements=st.integers())],\n                                  index=st.range_indexes(min_size=1, max_size=1000)))\n    sorted_df = df.sort_values(by='col2', ascending=True, inplace=False)\n    assert sorted_df['col2'].is_monotonic_increasing\n\n@given(st.data())\ndef test_na_position_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())),\n                                          st.column('col2', elements=st.integers())],\n                                  index=st.range_indexes(min_size=1, max_size=1000)))\n    sorted_df_first = df.sort_values(by='col1', na_position='first', inplace=False)\n    sorted_df_last = df.sort_values(by='col1', na_position='last', inplace=False)\n    assert sorted_df_first['col1'].isna().all() or not sorted_df_first['col1'].isna().any()\n    assert sorted_df_last['col1'].isna().any() or not sorted_df_last['col1'].isna().all()\n\n@given(st.data())\ndef test_multi_column_sort_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())),\n                                          st.column('col2', elements=st.integers()),\n                                          st.column('col3', elements=st.integers())],\n                                  index=st.range_indexes(min_size=1, max_size=1000)))\n    sorted_df = df.sort_values(by=['col1', 'col2'], ascending=[True, False], inplace=False)\n    assert sorted_df['col1'].is_monotonic_increasing\n    assert sorted_df['col2'].is_monotonic_decreasing\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())),\n                                          st.column('col2', elements=st.integers())],\n                                  index=st.range_indexes(min_size=1, max_size=1000)))\n    sorted_df = df.sort_values(by='col1', ignore_index=True, inplace=False)\n    assert sorted_df.index.equals(pd.Index(range(len(sorted_df))))\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_output_rows_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', element=st.one_of(st.text(), st.none())),\n        st.column('col2', element=st.integers()),\n        st.column('col3', element=st.integers())\n    ], rows=st.lists(st.one_of(st.tuples(st.text(), st.integers(), st.integers())), min_size=1, max_size=1000)))\n    \n    result = df.sort_values(by='col1', inplace=False)\n    \n    assert result.shape[0] == df.shape[0]\n\n@given(st.data())\ndef test_sorted_columns_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', element=st.one_of(st.text(), st.none())),\n        st.column('col2', element=st.integers()),\n        st.column('col3', element=st.integers())\n    ], rows=st.lists(st.one_of(st.tuples(st.text(), st.integers(), st.integers())), min_size=1, max_size=1000)))\n    \n    result = df.sort_values(by='col1', ascending=True, inplace=False)\n    assert (result['col1'].is_monotonic_increasing or result['col1'].is_monotonic_decreasing)\n\n@given(st.data())\ndef test_na_position_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', element=st.one_of(st.text(), st.none())),\n        st.column('col2', element=st.integers())\n    ], rows=st.lists(st.one_of(st.tuples(st.text(), st.integers())), min_size=1, max_size=1000)))\n    \n    result_first = df.sort_values(by='col1', na_position='first', inplace=False)\n    result_last = df.sort_values(by='col1', na_position='last', inplace=False)\n    \n    assert result_first['col1'].isna().argmax() < result_last['col1'].isna().argmax()\n\n@given(st.data())\ndef test_multiple_columns_sorting_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', element=st.one_of(st.text(), st.none())),\n        st.column('col2', element=st.integers()),\n        st.column('col3', element=st.integers())\n    ], rows=st.lists(st.one_of(st.tuples(st.text(), st.integers(), st.integers())), min_size=1, max_size=1000)))\n    \n    result = df.sort_values(by=['col1', 'col2'], ascending=[True, False], inplace=False)\n    \n    sorted_col1 = result['col1'].is_monotonic_increasing\n    sorted_col2 = result['col2'].is_monotonic_decreasing\n    assert sorted_col1 and sorted_col2\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', element=st.one_of(st.text(), st.none())),\n        st.column('col2', element=st.integers())\n    ], rows=st.lists(st.one_of(st.tuples(st.text(), st.integers())), min_size=1, max_size=1000)))\n    \n    result = df.sort_values(by='col1', ignore_index=True, inplace=False)\n    \n    assert result.index.equals(pd.RangeIndex(start=0, stop=result.shape[0], step=1))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_output_row_count_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), rows=st.integers(min_value=1, max_value=1000)))\n    result_df = df.sort_values(by=df.columns[0], inplace=False)\n    assert len(result_df) == len(df)\n\n@given(st.data())\ndef test_sorting_order_property(data):\n    df = data.draw(st.data_frames(columns=st.lists(st.text()), rows=st.integers(min_value=1, max_value=100)))\n    ascending = data.draw(st.booleans())\n    result_df = df.sort_values(by=df.columns[0], ascending=ascending, inplace=False)\n    assert result_df[df.columns[0]].is_sorted(ascending=ascending)\n\n@given(st.data())\ndef test_na_position_property(data):\n    df = data.draw(st.data_frames(columns=[st.text()], rows=st.integers(min_value=1, max_value=100)))\n    df.loc[2] = [np.nan]  # Introduce a NaN value\n    result_df_first = df.sort_values(by=df.columns[0], na_position='first', inplace=False)\n    result_df_last = df.sort_values(by=df.columns[0], na_position='last', inplace=False)\n    assert result_df_first[df.columns[0]].isna().all()  # Check if NaN is at the start\n    assert result_df_last[df.columns[0]].isna().all()  # Check if NaN is at the end\n\n@given(st.data())\ndef test_multiple_columns_sorting_property(data):\n    df = data.draw(st.data_frames(columns=st.lists(st.text(), min_size=2, max_size=3), rows=st.integers(min_value=1, max_value=100)))\n    ascending = data.draw(st.lists(st.booleans(), min_size=len(df.columns), max_size=len(df.columns)))\n    result_df = df.sort_values(by=df.columns, ascending=ascending, inplace=False)\n    assert all(result_df[df.columns[i]].is_sorted(ascending=ascending[i]) for i in range(len(df.columns)))\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.data_frames(columns=st.lists(st.text(), min_size=1, max_size=3), rows=st.integers(min_value=1, max_value=100)))\n    result_df = df.sort_values(by=df.columns[0], ignore_index=True, inplace=False)\n    assert result_df.index.equals(pd.RangeIndex(start=0, stop=len(result_df), step=1))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_output_row_count_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), \n                                   rows=st.integers(min_value=1, max_value=1000)))\n    output_df = df.sort_values(by=df.columns[0], inplace=False)\n    assert output_df.shape[0] == df.shape[0]\n\n@given(st.data())\ndef test_sorted_output_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), \n                                   rows=st.integers(min_value=1, max_value=100)))\n    sorted_col = df.columns[0]\n    ascending_order = data.draw(st.booleans())\n    output_df = df.sort_values(by=sorted_col, ascending=ascending_order)\n    expected_order = df.sort_values(by=sorted_col, ascending=ascending_order).reset_index(drop=True)\n    pd.testing.assert_frame_equal(output_df.reset_index(drop=True), expected_order)\n\n@given(st.data())\ndef test_na_position_property(data):\n    df = data.draw(st.data_frames(columns=st.one_of(st.text(), st.integers()), \n                                   rows=st.integers(min_value=1, max_value=100)))\n    df = df.append(pd.Series([np.nan]), ignore_index=True)\n    ascending_order = data.draw(st.booleans())\n    output_first = df.sort_values(by=df.columns[0], ascending=ascending_order, na_position='first')\n    output_last = df.sort_values(by=df.columns[0], ascending=ascending_order, na_position='last')\n    \n    if ascending_order:\n        assert output_first.iloc[0][0] is np.nan\n        assert output_last.iloc[-1][0] is np.nan\n    else:\n        assert output_first.iloc[-1][0] is np.nan\n        assert output_last.iloc[0][0] is np.nan\n\n@given(st.data())\ndef test_multiple_column_sorting_property(data):\n    df = data.draw(st.data_frames(columns=st.lists(st.text(), min_size=2, max_size=5), \n                                   rows=st.integers(min_value=1, max_value=100)))\n    output_df = df.sort_values(by=df.columns.tolist(), ascending=[True] * len(df.columns))\n    \n    for i in range(len(df.columns)):\n        assert output_df[self.columns[i]].is_monotonic_increasing\n\n@given(st.data())\ndef test_ignore_index_property(data):\n    df = data.draw(st.data_frames(columns=st.lists(st.text(), min_size=1, max_size=5), \n                                   rows=st.integers(min_value=1, max_value=100)))\n    output_df = df.sort_values(by=df.columns[0], ignore_index=True)\n    assert output_df.index.equals(pd.RangeIndex(start=0, stop=output_df.shape[0], step=1))\n\n# End program\n```"], "properties": ["1. The output DataFrame should have the same number of rows as the input DataFrame when `inplace` is set to `False`.\n\n2. The output DataFrame should be sorted according to the specified `by` columns in the order defined by the `ascending` parameter.\n\n3. If the `na_position` is set to 'first', all NaN values in the sorted column(s) should appear at the beginning of the output DataFrame; if set to 'last', they should appear at the end.\n\n4. When multiple columns are specified in the `by` parameter, the output DataFrame should be sorted first by the first column, then by the second column, and so on, respecting the `ascending` order for each column.\n\n5. If `ignore_index` is set to `True`, the output DataFrame should have a default integer index (0, 1, 2, ...) regardless of the original index of the input DataFrame."], "api_doc": "pandas.DataFrame.sort_values\nDataFrame.sort_values(by, *, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]\nSort by the values along either axis.\n\nParameters\n:\nby\nstr or list of str\nName or list of names to sort by.\n\nif axis is 0 or \u2018index\u2019 then by may contain index levels and/or column labels.\n\nif axis is 1 or \u2018columns\u2019 then by may contain column levels and/or index labels.\n\naxis\n\u201c{0 or \u2018index\u2019, 1 or \u2018columns\u2019}\u201d, default 0\nAxis to be sorted.\n\nascending\nbool or list of bool, default True\nSort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.\n\ninplace\nbool, default False\nIf True, perform operation in-place.\n\nkind\n{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.\n\nna_position\n{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\nPuts NaNs at the beginning if first; last puts NaNs at the end.\n\nignore_index\nbool, default False\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nkey\ncallable, optional\nApply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.\n\nReturns\n:\nDataFrame or None\nDataFrame with sorted values or None if inplace=True.\n\nSee also\n\nDataFrame.sort_index\nSort a DataFrame by the index.\n\nSeries.sort_values\nSimilar method for a Series.\n\nExamples\n\ndf = pd.DataFrame({\n    'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n    'col2': [2, 1, 9, 8, 7, 4],\n    'col3': [0, 1, 9, 4, 2, 3],\n    'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n})\ndf\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nSort by col1\n\ndf.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort by multiple columns\n\ndf.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort Descending\n\ndf.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D\nPutting NAs first\n\ndf.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\nSorting with a key function\n\ndf.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nNatural sort with the key argument, using the natsort <https://github.com/SethMMorton/natsort> package.\n\ndf = pd.DataFrame({\n   \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n   \"value\": [10, 20, 30, 40, 50]\n})\ndf\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\nfrom natsort import index_natsorted\ndf.sort_values(\n    by=\"time\",\n    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n)\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20", "api_code": "def sort_values(\n    self,\n    by: IndexLabel,\n    *,\n    axis: Axis = 0,\n    ascending: bool | list[bool] | tuple[bool, ...] = True,\n    inplace: bool = False,\n    kind: SortKind = \"quicksort\",\n    na_position: str = \"last\",\n    ignore_index: bool = False,\n    key: ValueKeyFunc | None = None,\n) -> DataFrame | None:\n    inplace = validate_bool_kwarg(inplace, \"inplace\")\n    axis = self._get_axis_number(axis)\n    ascending = validate_ascending(ascending)\n    if not isinstance(by, list):\n        by = [by]\n    # error: Argument 1 to \"len\" has incompatible type \"Union[bool, List[bool]]\";\n    # expected \"Sized\"\n    if is_sequence(ascending) and (\n        len(by) != len(ascending)  # type: ignore[arg-type]\n    ):\n        # error: Argument 1 to \"len\" has incompatible type \"Union[bool,\n        # List[bool]]\"; expected \"Sized\"\n        raise ValueError(\n            f\"Length of ascending ({len(ascending)})\"  # type: ignore[arg-type]\n            f\" != length of by ({len(by)})\"\n        )\n    if len(by) > 1:\n        keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n\n        # need to rewrap columns in Series to apply key function\n        if key is not None:\n            # error: List comprehension has incompatible type List[Series];\n            # expected List[ndarray]\n            keys = [\n                Series(k, name=name)  # type: ignore[misc]\n                for (k, name) in zip(keys, by)\n            ]\n\n        indexer = lexsort_indexer(\n            keys, orders=ascending, na_position=na_position, key=key\n        )\n    elif len(by):\n        # len(by) == 1\n\n        k = self._get_label_or_level_values(by[0], axis=axis)\n\n        # need to rewrap column in Series to apply key function\n        if key is not None:\n            # error: Incompatible types in assignment (expression has type\n            # \"Series\", variable has type \"ndarray\")\n            k = Series(k, name=by[0])  # type: ignore[assignment]\n\n        if isinstance(ascending, (tuple, list)):\n            ascending = ascending[0]\n\n        indexer = nargsort(\n            k, kind=kind, ascending=ascending, na_position=na_position, key=key\n        )\n    else:\n        if inplace:\n            return self._update_inplace(self)\n        else:\n            return self.copy(deep=None)\n\n    if is_range_indexer(indexer, len(indexer)):\n        result = self.copy(deep=(not inplace and not using_copy_on_write()))\n        if ignore_index:\n            result.index = default_index(len(result))\n\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result\n\n    new_data = self._mgr.take(\n        indexer, axis=self._get_block_manager_axis(axis), verify=False\n    )\n\n    if ignore_index:\n        new_data.set_axis(\n            self._get_block_manager_axis(axis), default_index(len(indexer))\n        )\n\n    result = self._constructor_from_mgr(new_data, axes=new_data.axes)\n    if inplace:\n        return self._update_inplace(result)\n    else:\n        return result.__finalize__(self, method=\"sort_values\")"}
{"function_name": "networkx.algorithms.assortativity.average_neighbor_degree", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_average_neighbor_degree_contains_all_nodes_property(data):\n    # Generate a random graph\n    G = data.draw(st.from_type(nx.Graph))\n    result = nx.average_neighbor_degree(G)\n    # Check that each node in the graph is a key in the result\n    assert all(node in result for node in G.nodes)\n\n@given(st.data())\ndef test_average_neighbor_degree_non_negative_values_property(data):\n    # Generate a random graph\n    G = data.draw(st.from_type(nx.Graph))\n    result = nx.average_neighbor_degree(G)\n    # Check that the average neighbor degree values are non-negative\n    assert all(value >= 0 for value in result.values())\n\n@given(st.data())\ndef test_average_neighbor_degree_zero_for_isolated_nodes_property(data):\n    # Generate a random graph\n    G = data.draw(st.from_type(nx.Graph))\n    # Add an isolated node\n    G.add_node('isolated_node')\n    result = nx.average_neighbor_degree(G)\n    # Check that the isolated node has an average neighbor degree of zero\n    assert result.get('isolated_node', 0) == 0\n\n@given(st.data())\ndef test_average_neighbor_degree_correctness_property(data):\n    # Generate a random graph\n    G = data.draw(st.from_type(nx.Graph))\n    result = nx.average_neighbor_degree(G)\n    for node in G.nodes:\n        neighbors = list(G.neighbors(node))\n        if neighbors:\n            expected_avg = sum(G.degree(n) for n in neighbors) / len(neighbors)\n            assert result[node] == expected_avg\n\n@given(st.data())\ndef test_average_neighbor_degree_stability_property(data):\n    # Generate a random graph\n    G = data.draw(st.from_type(nx.Graph))\n    result1 = nx.average_neighbor_degree(G)\n    result2 = nx.average_neighbor_degree(G)\n    # Check that the results are consistent across multiple calls\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nimport numpy as np\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    # Generate a random directed graph\n    num_nodes = data.draw(st.integers(min_value=1, max_value=100))\n    G = nx.gnp_random_graph(num_nodes, 0.5, directed=True)\n    avg_neighbor_deg = nx.average_neighbor_degree(G)\n\n    # Check that all nodes are in the output\n    for node in G.nodes:\n        assert node in avg_neighbor_deg\n\n@given(st.data())\ndef test_non_negative_average_degree_property(data):\n    # Generate a random directed graph\n    num_nodes = data.draw(st.integers(min_value=1, max_value=100))\n    G = nx.gnp_random_graph(num_nodes, 0.5, directed=True)\n    avg_neighbor_deg = nx.average_neighbor_degree(G)\n\n    # Check that all average neighbor degrees are non-negative\n    for avg in avg_neighbor_deg.values():\n        assert avg >= 0\n\n@given(st.data())\ndef test_zero_degree_node_average_property(data):\n    # Generate a random directed graph\n    G = nx.DiGraph()\n    G.add_node(0)  # Add a single node with no edges\n    avg_neighbor_deg = nx.average_neighbor_degree(G)\n\n    # Check that the average neighbor degree for the isolated node is zero\n    assert avg_neighbor_deg[0] == 0.0\n\n@given(st.data())\ndef test_correct_average_calculation_property(data):\n    # Generate a random directed graph\n    num_nodes = data.draw(st.integers(min_value=1, max_value=10))\n    G = nx.gnp_random_graph(num_nodes, 0.5, directed=True)\n    avg_neighbor_deg = nx.average_neighbor_degree(G)\n\n    for node in G.nodes:\n        neighbors = list(G.neighbors(node))\n        if neighbors:\n            total_degree = sum(G.degree(neighbor) for neighbor in neighbors)\n            average_degree = total_degree / len(neighbors)\n            assert np.isclose(avg_neighbor_deg[node], average_degree)\n\n@given(st.data())\ndef test_stable_output_property(data):\n    # Generate a random directed graph\n    num_nodes = data.draw(st.integers(min_value=1, max_value=100))\n    G = nx.gnp_random_graph(num_nodes, 0.5, directed=True)\n    avg_neighbor_deg_1 = nx.average_neighbor_degree(G)\n    avg_neighbor_deg_2 = nx.average_neighbor_degree(G)\n\n    # Check that the output is stable across multiple calls\n    assert avg_neighbor_deg_1 == avg_neighbor_deg_2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.path_graph, st.integers(min_value=1, max_value=100)),\n        st.builds(nx.complete_graph, st.integers(min_value=1, max_value=10))\n    ))\n    output = nx.average_neighbor_degree(G)\n    assert all(node in output for node in G.nodes)\n\n@given(st.data())\ndef test_average_neighbor_degree_non_negative_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.path_graph, st.integers(min_value=1, max_value=100)),\n        st.builds(nx.complete_graph, st.integers(min_value=1, max_value=10))\n    ))\n    output = nx.average_neighbor_degree(G)\n    assert all(value >= 0 for value in output.values())\n\n@given(st.data())\ndef test_zero_degree_node_average_property(data):\n    G = nx.Graph()\n    G.add_node(0)  # A node with no neighbors\n    output = nx.average_neighbor_degree(G)\n    assert output.get(0, 0) == 0.0\n\n@given(st.data())\ndef test_average_calculation_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.path_graph, st.integers(min_value=2, max_value=100)),\n        st.builds(nx.complete_graph, st.integers(min_value=2, max_value=10))\n    ))\n    output = nx.average_neighbor_degree(G)\n    for node, avg_degree in output.items():\n        neighbors = list(G.neighbors(node))\n        if neighbors:\n            total_degree = sum(G.degree(n) for n in neighbors)\n            assert avg_degree == total_degree / len(neighbors)\n\n@given(st.data())\ndef test_output_stability_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.path_graph, st.integers(min_value=1, max_value=100)),\n        st.builds(nx.complete_graph, st.integers(min_value=1, max_value=10))\n    ))\n    output1 = nx.average_neighbor_degree(G)\n    output2 = nx.average_neighbor_degree(G)\n    assert output1 == output2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    G = data.draw(st.builds(nx.erdos_renyi_graph, n=st.integers(min_value=1, max_value=100), p=st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degree = nx.average_neighbor_degree(G)\n    for node in G.nodes:\n        assert node in avg_neighbor_degree\n\n@given(st.data())\ndef test_average_neighbor_degree_non_negative_property(data):\n    G = data.draw(st.builds(nx.erdos_renyi_graph, n=st.integers(min_value=1, max_value=100), p=st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degree = nx.average_neighbor_degree(G)\n    for value in avg_neighbor_degree.values():\n        assert value >= 0.0\n\n@given(st.data())\ndef test_zero_degree_node_average_property(data):\n    G = nx.Graph()\n    G.add_node(0)  # Node with degree zero\n    avg_neighbor_degree = nx.average_neighbor_degree(G)\n    assert avg_neighbor_degree[0] == 0.0\n\n@given(st.data())\ndef test_average_neighbor_degree_calculation_property(data):\n    G = data.draw(st.builds(nx.erdos_renyi_graph, n=st.integers(min_value=1, max_value=100), p=st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degree = nx.average_neighbor_degree(G)\n    for node in G.nodes:\n        neighbors = list(G.neighbors(node))\n        if neighbors:\n            total_degree = sum(G.degree(neighbor) for neighbor in neighbors)\n            assert avg_neighbor_degree[node] == total_degree / len(neighbors)\n\n@given(st.data())\ndef test_stable_output_property(data):\n    G = data.draw(st.builds(nx.erdos_renyi_graph, n=st.integers(min_value=1, max_value=100), p=st.floats(min_value=0, max_value=1)))\n    avg_neighbor_degree_1 = nx.average_neighbor_degree(G)\n    avg_neighbor_degree_2 = nx.average_neighbor_degree(G)\n    assert avg_neighbor_degree_1 == avg_neighbor_degree_2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), st.sampled_from(['in', 'out', 'in+out']))\ndef test_output_contains_all_nodes_property(data, source):\n    G = nx.Graph(data)\n    avg_neighbor_deg = nx.average_neighbor_degree(G, source=source)\n    assert all(node in avg_neighbor_deg for node in G.nodes)\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), st.sampled_from(['in', 'out', 'in+out']))\ndef test_average_neighbor_degree_non_negative_property(data, source):\n    G = nx.Graph(data)\n    avg_neighbor_deg = nx.average_neighbor_degree(G, source=source)\n    assert all(value >= 0 for value in avg_neighbor_deg.values())\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), st.sampled_from(['in', 'out', 'in+out']))\ndef test_degree_zero_average_property(data, source):\n    G = nx.Graph(data)\n    avg_neighbor_deg = nx.average_neighbor_degree(G, source=source)\n    for node in G.nodes:\n        if G.degree(node) == 0:\n            assert avg_neighbor_deg[node] == 0.0\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), st.sampled_from(['in', 'out', 'in+out']))\ndef test_average_neighbor_degree_correctness_property(data, source):\n    G = nx.Graph(data)\n    avg_neighbor_deg = nx.average_neighbor_degree(G, source=source)\n\n    for node in G.nodes:\n        neighbors = list(G.neighbors(node))\n        if len(neighbors) > 0:\n            total_degree = sum(G.degree(n) for n in neighbors)\n            assert avg_neighbor_deg[node] == total_degree / len(neighbors)\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), st.sampled_from(['in', 'out', 'in+out']))\ndef test_output_stability_property(data, source):\n    G = nx.Graph(data)\n    avg_neighbor_deg_first = nx.average_neighbor_degree(G, source=source)\n    avg_neighbor_deg_second = nx.average_neighbor_degree(G, source=source)\n    assert avg_neighbor_deg_first == avg_neighbor_deg_second\n# End program\n```"], "properties": ["1. The output dictionary should contain an entry for every node in the input graph, with each node as a key.\n\n2. The average neighbor degree for any node should be a non-negative float value, as it represents an average degree.\n\n3. If a node has no neighbors (degree zero), its average neighbor degree should be zero.\n\n4. The average neighbor degree of a node should be equal to the total degree of its neighbors divided by the number of neighbors, adhering to the mathematical definition provided in the documentation.\n\n5. The average neighbor degree should remain consistent regardless of the order in which nodes are processed, ensuring that the output is stable across multiple invocations with the same graph and parameters."], "api_doc": "average_neighbor_degree\naverage_neighbor_degree(G, source='out', target='out', nodes=None, weight=None)[source]\nReturns the average degree of the neighborhood of each node.\n\nIn an undirected graph, the neighborhood N(i) of node i contains the nodes that are connected to i by an edge.\n\nFor directed graphs, N(i) is defined according to the parameter source:\n\nif source is \u2018in\u2019, then N(i) consists of predecessors of node i.\n\nif source is \u2018out\u2019, then N(i) consists of successors of node i.\n\nif source is \u2018in+out\u2019, then N(i) is both predecessors and successors.\n\nThe average neighborhood degree of a node i is\n\n \n \nwhere N(i) are the neighbors of node i and k_j is the degree of node j which belongs to N(i). For weighted graphs, an analogous measure can be defined [1],\n\n \n \nwhere s_i is the weighted degree of node i, w_{ij} is the weight of the edge that links i and j and N(i) are the neighbors of node i.\n\nParameters\n:\nG\nNetworkX graph\nsource\nstring (\u201cin\u201d|\u201dout\u201d|\u201din+out\u201d), optional (default=\u201dout\u201d)\nDirected graphs only. Use \u201cin\u201d- or \u201cout\u201d-neighbors of source node.\n\ntarget\nstring (\u201cin\u201d|\u201dout\u201d|\u201din+out\u201d), optional (default=\u201dout\u201d)\nDirected graphs only. Use \u201cin\u201d- or \u201cout\u201d-degree for target node.\n\nnodes\nlist or iterable, optional (default=G.nodes)\nCompute neighbor degree only for specified nodes.\n\nweight\nstring or None, optional (default=None)\nThe edge attribute that holds the numerical value used as a weight. If None, then each edge has weight 1.\n\nReturns\n:\nd: dict\nA dictionary keyed by node to the average degree of its neighbors.\n\nRaises\n:\nNetworkXError\nIf either source or target are not one of \u2018in\u2019, \u2018out\u2019, or \u2018in+out\u2019. If either source or target is passed for an undirected graph.\n\nSee also\n\naverage_degree_connectivity\nReferences\n\n[1]\nA. Barrat, M. Barth\u00e9lemy, R. Pastor-Satorras, and A. Vespignani, \u201cThe architecture of complex weighted networks\u201d. PNAS 101 (11): 3747\u20133752 (2004).\n\nExamples\n\nG = nx.path_graph(4)\nG.edges[0, 1][\"weight\"] = 5\nG.edges[2, 3][\"weight\"] = 3\nnx.average_neighbor_degree(G)\n{0: 2.0, 1: 1.5, 2: 1.5, 3: 2.0}\nnx.average_neighbor_degree(G, weight=\"weight\")\n{0: 2.0, 1: 1.1666666666666667, 2: 1.25, 3: 2.0}\nG = nx.DiGraph()\nnx.add_path(G, [0, 1, 2, 3])\nnx.average_neighbor_degree(G, source=\"in\", target=\"in\")\n{0: 0.0, 1: 0.0, 2: 1.0, 3: 1.0}\nnx.average_neighbor_degree(G, source=\"out\", target=\"out\")\n{0: 1.0, 1: 1.0, 2: 0.0, 3: 0.0}", "api_code": "@nx._dispatchable(edge_attrs=\"weight\")\ndef average_neighbor_degree(G, source=\"out\", target=\"out\", nodes=None, weight=None):\n    if G.is_directed():\n        if source == \"in\":\n            source_degree = G.in_degree\n        elif source == \"out\":\n            source_degree = G.out_degree\n        elif source == \"in+out\":\n            source_degree = G.degree\n        else:\n            raise nx.NetworkXError(\n                f\"source argument {source} must be 'in', 'out' or 'in+out'\"\n            )\n\n        if target == \"in\":\n            target_degree = G.in_degree\n        elif target == \"out\":\n            target_degree = G.out_degree\n        elif target == \"in+out\":\n            target_degree = G.degree\n        else:\n            raise nx.NetworkXError(\n                f\"target argument {target} must be 'in', 'out' or 'in+out'\"\n            )\n    else:\n        if source != \"out\" or target != \"out\":\n            raise nx.NetworkXError(\n                f\"source and target arguments are only supported for directed graphs\"\n            )\n        source_degree = target_degree = G.degree\n\n    # precompute target degrees -- should *not* be weighted degree\n    t_deg = dict(target_degree())\n\n    # Set up both predecessor and successor neighbor dicts leaving empty if not needed\n    G_P = G_S = {n: {} for n in G}\n    if G.is_directed():\n        # \"in\" or \"in+out\" cases: G_P contains predecessors\n        if \"in\" in source:\n            G_P = G.pred\n        # \"out\" or \"in+out\" cases: G_S contains successors\n        if \"out\" in source:\n            G_S = G.succ\n    else:\n        # undirected leave G_P empty but G_S is the adjacency\n        G_S = G.adj\n\n    # Main loop: Compute average degree of neighbors\n    avg = {}\n    for n, deg in source_degree(nodes, weight=weight):\n        # handle degree zero average\n        if deg == 0:\n            avg[n] = 0.0\n            continue\n\n        # we sum over both G_P and G_S, but one of the two is usually empty.\n        if weight is None:\n            avg[n] = (\n                sum(t_deg[nbr] for nbr in G_S[n]) + sum(t_deg[nbr] for nbr in G_P[n])\n            ) / deg\n        else:\n            avg[n] = (\n                sum(dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_S[n].items())\n                + sum(dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_P[n].items())\n            ) / deg\n    return avg"}
{"function_name": "networkx.algorithms.dag.is_aperiodic", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, unique=True))\ndef test_is_aperiodic_acylc_graph_property(edges):\n    DG = nx.DiGraph(edges)\n    if not nx.is_directed(DG):\n        return  # Skip if the graph is not directed\n    if len(DG) == 0 or nx.is_empty(DG):\n        return  # Skip if the graph is empty\n    assert nx.is_aperiodic(DG) is False  # Acyclic graphs should not be aperiodic\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, unique=True))\ndef test_is_aperiodic_single_cycle_property(edges):\n    if not edges:\n        return  # Skip if there are no edges\n    DG = nx.DiGraph(edges)\n    if len(DG) == 2 and edges[0][0] == edges[1][1]:  # A simple cycle of length 2\n        assert nx.is_aperiodic(DG) is False  # Should not be aperiodic\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, unique=True))\ndef test_is_aperiodic_coprime_cycles_property(edges):\n    DG = nx.DiGraph(edges)\n    if len(DG) == 0:\n        return  # Skip if the graph is empty\n    cycle_lengths = [2, 3]  # Example with coprime lengths\n    if set(cycle_lengths).issubset(set(DG.nodes())):\n        assert nx.is_aperiodic(DG) is True  # Should be aperiodic\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, unique=True))\ndef test_is_aperiodic_empty_graph_property(edges):\n    DG = nx.DiGraph(edges)\n    if len(DG) == 0:\n        with pytest.raises(nx.NetworkXPointlessConcept):\n            nx.is_aperiodic(DG)  # Must raise an error for empty graph\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, unique=True))\ndef test_is_aperiodic_dag_property(edges):\n    DG = nx.DiGraph(edges)\n    if nx.is_directed(DG) and not nx.has_path(DG, edges[0][0], edges[0][1]):\n        assert nx.is_aperiodic(DG) is False  # Directed acyclic graphs should not be aperiodic\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom hypothesis.strategies import integers, lists\n\n@given(st.data())\ndef test_is_aperiodic_property_acyclic_graph():\n    # Generate a directed acyclic graph\n    nodes = st.lists(st.integers(min_value=1, max_value=100), min_size=1, max_size=10).example()\n    edges = st.lists(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes))).filter(lambda x: not any(v == u for u, v in x)).example()\n    G = nx.DiGraph(edges)\n    assert nx.is_aperiodic(G) is False\n\n@given(st.data())\ndef test_is_aperiodic_property_single_cycle():\n    # Generate a directed graph with a single cycle of length k > 1\n    k = st.integers(min_value=2, max_value=10).example()\n    nodes = list(range(1, k + 1))\n    edges = [(i, i + 1) for i in nodes[:-1]] + [(k, 1)]\n    G = nx.DiGraph(edges)\n    assert nx.is_aperiodic(G) is False\n\n@given(st.data())\ndef test_is_aperiodic_property_coprime_cycles():\n    # Generate a directed graph with coprime cycle lengths\n    cycle1_length = st.integers(min_value=2, max_value=10).example()\n    cycle2_length = st.integers(min_value=11, max_value=20).filter(lambda x: x % cycle1_length != 0).example()\n    \n    nodes = list(range(1, cycle1_length + cycle2_length + 1))\n    edges = [(i, i + 1) for i in range(1, cycle1_length)] + [(cycle1_length, 1)]  # First cycle\n    edges += [(i + cycle1_length, i + cycle1_length + 1) for i in range(1, cycle2_length)] + [(cycle1_length + cycle2_length, cycle1_length + 1)]\n    \n    G = nx.DiGraph(edges)\n    assert nx.is_aperiodic(G) is True\n\n@given(st.data())\ndef test_is_aperiodic_property_empty_graph():\n    # Test with an empty graph\n    G = nx.DiGraph()\n    try:\n        nx.is_aperiodic(G)\n        assert False, \"Expected a NetworkXPointlessConcept error for empty graph\"\n    except nx.NetworkXPointlessConcept:\n        pass\n\n@given(st.data())\ndef test_is_aperiodic_property_directed_acyclic_graph():\n    # Generate a directed acyclic graph\n    nodes = st.lists(st.integers(min_value=1, max_value=100), min_size=1, max_size=10).example()\n    edges = st.lists(st.tuples(st.sampled_from(nodes), st.sampled_from(nodes))).filter(lambda x: not any(v == u for u, v in x)).example()\n    G = nx.DiGraph(edges)\n    assert nx.is_aperiodic(G) is False\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXPointlessConcept\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_acyclic_graph_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(G):\n        assert nx.is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_single_cycle_property(edges):\n    G = nx.DiGraph(edges)\n    if len(edges) == 1:  # Single edge creates a cycle\n        assert nx.is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=2, max_size=100))\ndef test_multiple_coprime_cycles_property(edges):\n    G = nx.DiGraph(edges)\n    # Assuming there's a strategy to ensure the cycles are coprime. \n    # This is a placeholder for illustrative purposes.\n    # In practice, you would need to generate edges that ensure this condition.\n    cycle_lengths = [3, 5]  # Example of coprime lengths\n    assert nx.is_aperiodic(G) is True\n\n@given(st.data())\ndef test_empty_graph_property(data):\n    G = nx.DiGraph()\n    try:\n        nx.is_aperiodic(G)\n    except NetworkXPointlessConcept:\n        assert True  # Asserting that the exception is raised\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_directed_acyclic_graph_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(G):\n        assert nx.is_aperiodic(G) is False\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom hypothesis import HealthCheck, settings\n\n# Set Hypothesis settings to avoid excessively large tests\nsettings.register_profile(\"ci\", max_examples=1000, suppress_health_check=[HealthCheck.too_slow])\nsettings.set_profile(\"ci\")\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_input_graph_is_acyclic_property(edges):\n    G = nx.DiGraph(edges)\n    if not nx.is_aperiodic(G):\n        assert nx.is_aperiodic(G) == False  # Acyclic graphs should return False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_single_cycle_of_length_k_property(edges):\n    G = nx.DiGraph(edges)\n    cycles = list(nx.simple_cycles(G))\n    for cycle in cycles:\n        if len(cycle) > 1:\n            assert nx.is_aperiodic(G) == False  # Single cycle of length k > 1 should return False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=2))\ndef test_multiple_cycles_coprime_lengths_property(edges):\n    G = nx.DiGraph(edges)\n    cycles = list(nx.simple_cycles(G))\n    if all(len(cycle) > 1 for cycle in cycles):\n        lengths = [len(cycle) for cycle in cycles]\n        if len(set(lengths)) == len(lengths) and all(\n            gcd(lengths[i], lengths[j]) == 1 for i in range(len(lengths)) for j in range(i + 1, len(lengths))\n        ):\n            assert nx.is_aperiodic(G) == True  # Coprime cycle lengths should return True\n\n@given(st.lists(st.tuples(), min_size=0))\ndef test_empty_graph_property(edges):\n    G = nx.DiGraph(edges)\n    if len(G) == 0:\n        try:\n            nx.is_aperiodic(G)\n        except nx.NetworkXPointlessConcept:\n            assert True  # Should raise an error for empty graph\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_directed_acyclic_graph_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(G):\n        assert nx.is_aperiodic(G) == False  # Directed acyclic graphs should return False\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXPointlessConcept\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), unique_by=lambda x: x[0])\ndef test_is_aperiodic_acylclic_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(G):\n        assert nx.is_aperiodic(G) is False\n\n@given(st.integers(min_value=2, max_value=100), st.integers(min_value=1, max_value=50))\ndef test_is_aperiodic_single_cycle_property(cycle_length, node):\n    G = nx.DiGraph([(node, (node + 1) % cycle_length) for node in range(cycle_length)])\n    assert nx.is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=2, max_size=100), unique_by=lambda x: x[0])\ndef test_is_aperiodic_coprime_cycles_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.number_of_cycles(G) > 1 and nx.is_aperiodic(G):\n        assert nx.is_aperiodic(G) is True\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=0, max_size=0))\ndef test_is_aperiodic_empty_graph_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        nx.is_aperiodic(G)\n    except NetworkXPointlessConcept:\n        assert True\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=2, max_size=100))\ndef test_is_aperiodic_directed_acyclic_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(G):\n        assert nx.is_aperiodic(G) is False\n# End program\n```"], "properties": ["1. If the input graph is acyclic, the output should always be False, indicating that acyclic graphs are not aperiodic.\n\n2. If the input graph contains a single cycle of length k, where k > 1, the output should be False, as k divides the length of the cycle.\n\n3. If the input graph has multiple cycles with lengths that are coprime (i.e., their greatest common divisor is 1), the output should be True, indicating that the graph is aperiodic.\n\n4. If the input graph is empty (i.e., contains no nodes), the output should raise a NetworkXPointlessConcept error, indicating that the concept of aperiodicity does not apply.\n\n5. If the input graph is directed but has no cycles, the output should be False, confirming that directed acyclic graphs are not aperiodic."], "api_doc": "is_aperiodic\nis_aperiodic(G)[source]\nReturns True if G is aperiodic.\n\nA directed graph is aperiodic if there is no integer k > 1 that divides the length of every cycle in the graph.\n\nParameters\n:\nG\nNetworkX DiGraph\nA directed graph\n\nReturns\n:\nbool\nTrue if the graph is aperiodic False otherwise\n\nRaises\n:\nNetworkXError\nIf G is not directed\n\nNotes\n\nThis uses the method outlined in [1], which runs in \n time given \n edges in G. Note that a graph is not aperiodic if it is acyclic as every integer trivial divides length 0 cycles.\n\nReferences\n\n[1]\nJarvis, J. P.; Shier, D. R. (1996), \u201cGraph-theoretic analysis of finite Markov chains,\u201d in Shier, D. R.; Wallenius, K. T., Applied Mathematical Modeling: A Multidisciplinary Approach, CRC Press.\n\nExamples\n\nA graph consisting of one cycle, the length of which is 2. Therefore k = 2 divides the length of every cycle in the graph and thus the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 1)])\nnx.is_aperiodic(DG)\nFalse\nA graph consisting of two cycles: one of length 2 and the other of length 3. The cycle lengths are coprime, so there is no single value of k where k > 1 that divides each cycle length and therefore the graph is aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 3), (3, 1), (1, 4), (4, 1)])\nnx.is_aperiodic(DG)\nTrue\nA graph consisting of two cycles: one of length 2 and the other of length 4. The lengths of the cycles share a common factor k = 2, and therefore the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 1), (3, 4), (4, 5), (5, 6), (6, 3)])\nnx.is_aperiodic(DG)\nFalse\nAn acyclic graph, therefore the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 3)])\nnx.is_aperiodic(DG)\nFalse", "api_code": "@nx._dispatchable\ndef is_aperiodic(G):\n    if not G.is_directed():\n        raise nx.NetworkXError(\"is_aperiodic not defined for undirected graphs\")\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept(\"Graph has no nodes.\")\n    s = arbitrary_element(G)\n    levels = {s: 0}\n    this_level = [s]\n    g = 0\n    lev = 1\n    while this_level:\n        next_level = []\n        for u in this_level:\n            for v in G[u]:\n                if v in levels:  # Non-Tree Edge\n                    g = gcd(g, levels[u] - levels[v] + 1)\n                else:  # Tree Edge\n                    next_level.append(v)\n                    levels[v] = lev\n        this_level = next_level\n        lev += 1\n    if len(levels) == len(G):  # All nodes in tree\n        return g == 1\n    else:\n        return g == 1 and nx.is_aperiodic(G.subgraph(set(G) - set(levels)))"}
{"function_name": "statistics.geometric_mean", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport math\nfrom statistics import geometric_mean, StatisticsError\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False), min_size=1))\ndef test_output_is_float_property(data):\n    result = geometric_mean(data)\n    assert isinstance(result, float)\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False), min_size=0))\ndef test_empty_input_raises_property(data):\n    if len(data) == 0:\n        with st.raises(StatisticsError):\n            geometric_mean(data)\n\n@given(st.lists(st.floats(min_value=-math.inf, max_value=math.inf), min_size=1))\ndef test_zero_in_input_returns_zero_property(data):\n    if 0.0 in data:\n        result = geometric_mean(data)\n        assert result == 0.0 or math.isnan(result)\n\n@given(st.lists(st.floats(min_value=-math.inf, max_value=math.inf), min_size=1))\ndef test_negative_input_raises_property(data):\n    if any(x < 0 for x in data):\n        with st.raises(StatisticsError):\n            geometric_mean(data)\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False), min_size=1))\ndef test_geometric_mean_less_than_arithmetic_mean_property(data):\n    if all(x > 0 for x in data):\n        geom_mean = geometric_mean(data)\n        arith_mean = sum(data) / len(data)\n        assert geom_mean <= arith_mean\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport math\nfrom statistics import geometric_mean, StatisticsError\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False), min_size=1))\ndef test_geometric_mean_non_empty_property(data):\n    result = geometric_mean(data)\n    assert isinstance(result, float)\n\n@given(st.lists(st.floats(), min_size=0))\ndef test_geometric_mean_empty_input_property(data):\n    if not data:\n        try:\n            geometric_mean(data)\n            assert False, \"Expected StatisticsError for empty input\"\n        except StatisticsError:\n            pass\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False), min_size=1))\ndef test_geometric_mean_zero_input_property(data):\n    data_with_zero = data + [0.0]\n    result = geometric_mean(data_with_zero)\n    assert result == 0.0 or math.isnan(result)\n\n@given(st.lists(st.floats(min_value=-math.inf, max_value=-1e-10), min_size=1))\ndef test_geometric_mean_negative_input_property(data):\n    try:\n        geometric_mean(data)\n        assert False, \"Expected StatisticsError for negative input\"\n    except StatisticsError:\n        pass\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10), min_size=2))\ndef test_geometric_mean_relational_property(data):\n    arithmetic_mean = sum(data) / len(data)\n    result = geometric_mean(data)\n    assert result <= arithmetic_mean\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport math\nfrom statistics import geometric_mean, StatisticsError\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False), min_size=1))\ndef test_non_empty_positive_inputs_property(data):\n    result = geometric_mean(data)\n    assert isinstance(result, float)\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False), min_size=0))\ndef test_empty_input_property(data):\n    with st.raises(StatisticsError):\n        geometric_mean(data)\n\n@given(st.lists(st.floats(min_value=0, allow_nan=False), min_size=1))\ndef test_zero_input_property(data):\n    data_with_zero = data + [0.0]\n    result = geometric_mean(data_with_zero)\n    assert result == 0.0 or math.isnan(result)\n\n@given(st.lists(st.floats(min_value=-math.inf, max_value=0, allow_nan=False), min_size=1))\ndef test_negative_input_property(data):\n    with st.raises(StatisticsError):\n        geometric_mean(data)\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10, allow_nan=False), min_size=1))\ndef test_geometric_mean_less_than_arithmetic_mean_property(data):\n    geom_mean = geometric_mean(data)\n    arith_mean = sum(data) / len(data)\n    assert geom_mean <= arith_mean\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import geometric_mean, StatisticsError\nimport math\n\n@given(st.lists(st.floats(min_value=0, allow_nan=True), min_size=1))\ndef test_geometric_mean_non_empty_property(data):\n    result = geometric_mean(data)\n    assert isinstance(result, float)  # Output should be a float\n\n@given(st.lists(st.floats(min_value=0, allow_nan=True), min_size=0))\ndef test_geometric_mean_empty_property(data):\n    try:\n        geometric_mean(data)\n        assert False, \"Expected StatisticsError for empty dataset\"\n    except StatisticsError:\n        pass  # Expected behavior\n\n@given(st.lists(st.floats(min_value=0, allow_nan=True), min_size=1))\ndef test_geometric_mean_zero_property(data):\n    data_with_zero = data + [0.0]\n    result = geometric_mean(data_with_zero)\n    assert result in (0.0, math.nan)  # Should return 0.0 or math.nan if zero is present\n\n@given(st.lists(st.floats(min_value=-math.inf, max_value=-1e-10), min_size=1))\ndef test_geometric_mean_negative_value_property(data):\n    try:\n        geometric_mean(data)\n        assert False, \"Expected StatisticsError for negative inputs\"\n    except StatisticsError:\n        pass  # Expected behavior\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10), min_size=1))\ndef test_geometric_mean_arithmetic_mean_property(data):\n    positive_data = [x for x in data if x > 0]\n    if positive_data:\n        geom_mean = geometric_mean(positive_data)\n        arith_mean = sum(positive_data) / len(positive_data)\n        assert geom_mean <= arith_mean  # Geometric mean should be <= arithmetic mean\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import geometric_mean, StatisticsError\nimport math\n\n@given(st.lists(st.floats(min_value=0.0001, max_value=1e10), min_size=1))\ndef test_geometric_mean_positive_values_property(data):\n    result = geometric_mean(data)\n    assert isinstance(result, float)  # Output should be a float\n    assert all(x > 0 for x in data)  # Input values are positive\n\n@given(st.lists(st.floats(), min_size=0))\ndef test_geometric_mean_empty_input_property(data):\n    if len(data) == 0:\n        with st.raises(StatisticsError):\n            geometric_mean(data)\n\n@given(st.lists(st.floats(min_value=0, max_value=1e10), min_size=1))\ndef test_geometric_mean_zero_input_property(data):\n    data.append(0.0)  # Ensure there's a zero in the data\n    result = geometric_mean(data)\n    assert result in (0.0, math.nan)  # Output should be 0.0 or NaN\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=0), min_size=1))\ndef test_geometric_mean_negative_input_property(data):\n    with st.raises(StatisticsError):\n        geometric_mean(data)\n\n@given(st.lists(st.floats(min_value=0.0001, max_value=1e10), min_size=1))\ndef test_geometric_mean_arithmetic_vs_geometric_property(data):\n    arithmetic_mean = sum(data) / len(data)\n    result = geometric_mean(data)\n    assert result <= arithmetic_mean  # Geometric mean should be <= arithmetic mean\n# End program\n```"], "properties": ["1. The output should be a float representing the geometric mean of the input data, provided the data is valid (non-empty, contains no zeros, and no negative values).\n\n2. If the input data is empty, the function should raise a `StatisticsError`.\n\n3. If the input data contains a zero, the function should return `0.0` or `math.nan` based on the context of the total computed.\n\n4. If the input data contains any negative values, the function should raise a `StatisticsError`.\n\n5. The output should be consistent with the mathematical definition of the geometric mean, meaning that if all input values are positive, the geometric mean should always be less than or equal to the arithmetic mean of the same values."], "api_doc": "statistics.geometric_mean(data)\nConvert data to floats and compute the geometric mean.\n\nThe geometric mean indicates the central tendency or typical value of the data using the product of the values (as opposed to the arithmetic mean which uses their sum).\n\nRaises a StatisticsError if the input dataset is empty, if it contains a zero, or if it contains a negative value. The data may be a sequence or iterable.\n\nNo special efforts are made to achieve exact results. (However, this may change in the future.)\n\n>>>\nround(geometric_mean([54, 24, 36]), 1)\n36.0\nAdded in version 3.8.", "api_code": "def geometric_mean(data):\n    n = 0\n    found_zero = False\n    def count_positive(iterable):\n        nonlocal n, found_zero\n        for n, x in enumerate(iterable, start=1):\n            if x > 0.0 or math.isnan(x):\n                yield x\n            elif x == 0.0:\n                found_zero = True\n            else:\n                raise StatisticsError('No negative inputs allowed', x)\n    total = fsum(map(log, count_positive(data)))\n    if not n:\n        raise StatisticsError('Must have a non-empty dataset')\n    if math.isnan(total):\n        return math.nan\n    if found_zero:\n        return math.nan if total == math.inf else 0.0\n    return exp(total / n)"}
{"function_name": "datetime.datetime.fromisoformat", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport re\n\n# Strategy to generate valid ISO 8601 date strings\n@st.composite\ndef iso_format_dates(draw):\n    year = draw(st.integers(min_value=1, max_value=9999))\n    month = draw(st.integers(min_value=1, max_value=12))\n    day = draw(st.integers(min_value=1, max_value=31))\n    hour = draw(st.integers(min_value=0, max_value=23))\n    minute = draw(st.integers(min_value=0, max_value=59))\n    second = draw(st.integers(min_value=0, max_value=59))\n    microsecond = draw(st.integers(min_value=0, max_value=999999))\n    tz_offset = draw(st.one_of(st.just('Z'), st.from_regex(r'[\\+\\-]\\d{2}:\\d{2}')))\n    \n    # Generate a valid ISO 8601 string\n    date_string = f'{year:04}-{month:02}-{day:02}T{hour:02}:{minute:02}:{second:02}.{microsecond:06}{tz_offset}'\n    return date_string\n\n@given(iso_format_dates())\ndef test_datetime_datetime_fromisoformat_validity_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    year, month, day, hour, minute, second, microsecond = dt.timetuple()[:7]\n    assert 1 <= month <= 12\n    assert 1 <= day <= 31\n    assert 0 <= hour < 24\n    assert 0 <= minute < 60\n    assert 0 <= second < 60\n    assert 0 <= microsecond < 1000000\n\n@given(iso_format_dates())\ndef test_datetime_datetime_fromisoformat_time_component_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    time_component_present = re.search(r'T(\\d{2}):(\\d{2}):(\\d{2})', date_string)\n    if time_component_present:\n        hour, minute, second = map(int, time_component_present.groups())\n        assert dt.hour == hour\n        assert dt.minute == minute\n        assert dt.second == second\n\n@given(iso_format_dates())\ndef test_datetime_datetime_fromisoformat_timezone_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    tz_offset = re.search(r'([+\\-]\\d{2}:\\d{2}|Z)', date_string)\n    if tz_offset:\n        assert dt.tzinfo is not None\n\n@given(st.text())\ndef test_datetime_datetime_fromisoformat_invalid_strings_property(invalid_string):\n    try:\n        datetime.fromisoformat(invalid_string)\n    except ValueError:\n        pass  # Expected behavior\n    except TypeError:\n        pass  # Expected behavior for non-string inputs\n\n@given(iso_format_dates())\ndef test_datetime_datetime_fromisoformat_default_time_midnight_property(date_string):\n    if 'T' not in date_string and ' ' not in date_string:\n        dt = datetime.fromisoformat(date_string)\n        assert dt.hour == 0\n        assert dt.minute == 0\n        assert dt.second == 0\n        assert dt.microsecond == 0\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport re\n\n# Helper function to generate valid ISO 8601 date strings\ndef valid_isoformat_date_strings():\n    return st.one_of(\n        st.fixed_dictionaries({\n            'year': st.integers(min_value=1900, max_value=2100),\n            'month': st.integers(min_value=1, max_value=12),\n            'day': st.integers(min_value=1, max_value=31),\n            'hour': st.integers(min_value=0, max_value=23),\n            'minute': st.integers(min_value=0, max_value=59),\n            'second': st.integers(min_value=0, max_value=59),\n            'microsecond': st.integers(min_value=0, max_value=999999),\n            'tz_offset': st.tuples(st.integers(min_value=-14, max_value=14), st.integers(min_value=0, max_value=59)),\n        }).map(lambda d: f\"{d['year']}-{d['month']:02d}-{d['day']:02d}T{d['hour']:02d}:{d['minute']:02d}:{d['second']:02d}.{d['microsecond']:06d}{f'+{d['tz_offset'][0]:02d}:{d['tz_offset'][1]:02d}' if d['tz_offset'][0] >= 0 else f'-{abs(d['tz_offset'][0]):02d}:{d['tz_offset'][1]:02d}'}\")\n    )\n\n@given(date_string=valid_isoformat_date_strings())\ndef test_output_validity_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    assert dt.year >= 1900 and dt.year <= 2100\n    assert dt.month >= 1 and dt.month <= 12\n    assert 1 <= dt.day <= 31  # Note: this does not check for month-specific validity.\n\n@given(date_string=valid_isoformat_date_strings())\ndef test_output_time_component_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    if 'T' in date_string:\n        time_part = date_string.split('T')[1].split('+')[0].split('-')[0]\n        time_components = re.findall(r'(\\d+)', time_part)\n        assert len(time_components) >= 3  # At least hour, minute, second.\n        assert dt.hour == int(time_components[0])\n        assert dt.minute == int(time_components[1])\n        assert dt.second == int(time_components[2])\n\n@given(date_string=valid_isoformat_date_strings())\ndef test_output_timezone_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    if '+' in date_string or '-' in date_string:\n        assert dt.tzinfo is not None\n    else:\n        assert dt.tzinfo is None\n\n@given(date_string=st.text(min_size=1, max_size=50))\ndef test_invalid_format_raises_value_error_property(date_string):\n    try:\n        datetime.fromisoformat(date_string)\n        assert False  # We expect a ValueError\n    except ValueError:\n        pass  # Expected behavior\n\n@given(date_string=valid_isoformat_date_strings())\ndef test_default_time_midnight_property(date_string):\n    if 'T' not in date_string:\n        date_part = date_string.split('+')[0].split('-')[0]\n        dt = datetime.fromisoformat(date_string)\n        assert dt.hour == 0\n        assert dt.minute == 0\n        assert dt.second == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport re\n\n# Define a strategy for generating valid ISO 8601 date strings\ndef valid_iso_date_strings():\n    return st.one_of(\n        st.builds(lambda year, month, day: f\"{year:04}-{month:02}-{day:02}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31)),\n        st.builds(lambda year, month, day, hour, minute, second: \n                   f\"{year:04}-{month:02}-{day:02}T{hour:02}:{minute:02}:{second:02}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31),\n                   st.integers(min_value=0, max_value=23),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=59)),\n        st.builds(lambda year, month, day, hour, minute, second, tz: \n                   f\"{year:04}-{month:02}-{day:02}T{hour:02}:{minute:02}:{second:02}{tz}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31),\n                   st.integers(min_value=0, max_value=23),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=59),\n                   st.one_of(st.just('+00:00'), st.just('-05:00'))),\n    )\n\n@given(valid_iso_date_strings())\ndef test_output_validity_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    year, month, day = dt.year, dt.month, dt.day\n    assert 1 <= month <= 12\n    assert 1 <= day <= 31  # Simplified check; a more complex check could be done\n\n@given(valid_iso_date_strings())\ndef test_output_time_components_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    if 'T' in date_string:\n        time_part = date_string.split('T')[1].split('+')[0].split('-')[0]\n        time_components = re.findall(r'\\d+', time_part)\n        assert len(time_components) <= 3  # Only hour, minute, second\n        assert all(0 <= int(tc) < 60 for tc in time_components[1:])  # Minute and second checks\n        if time_components:\n            assert 0 <= int(time_components[0]) < 24  # Hour check\n\n@given(valid_iso_date_strings())\ndef test_output_timezone_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    if 'Z' in date_string or re.search(r'[+-]\\d{2}:\\d{2}', date_string):\n        assert dt.tzinfo is not None\n    else:\n        assert dt.tzinfo is None\n\n@given(st.text())\ndef test_invalid_format_property(invalid_date_string):\n    try:\n        datetime.fromisoformat(invalid_date_string)\n        assert False, f\"Expected ValueError for invalid date string: {invalid_date_string}\"\n    except ValueError:\n        pass  # Expected exception\n\n@given(valid_iso_date_strings())\ndef test_default_midnight_property(date_string):\n    if 'T' not in date_string and ' ' not in date_string:\n        dt = datetime.fromisoformat(date_string)\n        assert dt.hour == 0 and dt.minute == 0 and dt.second == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport re\n\n# Define a strategy for generating valid ISO 8601 date strings\ndef valid_iso_date_strings():\n    # Examples of valid formats\n    formats = [\n        'YYYY-MM-DD',\n        'YYYYMMDD',\n        'YYYY-MM-DDTHH:MM:SS',\n        'YYYYMMDDTHHMMSS',\n        'YYYY-Www-DdTHH:MM:SS',\n        'YYYY-MM-DD HH:MM:SS',\n        'YYYY-MM-DDTHH:MM:SSZ',\n        'YYYY-MM-DDTHH:MM:SS\u00b1HH:MM'\n    ]\n    \n    # Generate strings for valid year, month, day, etc.\n    for year in range(2000, 2100):\n        for month in range(1, 13):\n            for day in range(1, 32):  # Days will be validated later\n                if day <= (days_in_month := (31 if month in [1, 3, 5, 7, 8, 10, 12] else 30 if month != 2 else 29 if year % 4 == 0 else 28)):\n                    yield f\"{year:04}-{month:02}-{day:02}\"\n                    yield f\"{year}{month:02}{day:02}\"\n                    yield f\"{year:04}-{month:02}-{day:02}T00:00:00\"\n                    yield f\"{year}{month:02}{day:02}T000000\"\n                    yield f\"{year:04}-W01-1T00:00:00\"  # Example of week date\n                    yield f\"{year:04}-{month:02}-{day:02} 00:00:00\"\n                    yield f\"{year:04}-{month:02}-{day:02}T00:00:00Z\"\n                    yield f\"{year:04}-{month:02}-{day:02}T00:00:00+00:00\"\n\n@given(st.data())\ndef test_output_validity_property(data):\n    date_string = data.draw(st.sampled_from(list(valid_iso_date_strings())))\n    try:\n        result = datetime.fromisoformat(date_string)\n        # Check if the output is a valid datetime object\n        assert isinstance(result, datetime)\n        # Validate year, month, and day\n        assert result.year == int(date_string[:4])\n        assert result.month == int(date_string[5:7])\n        assert result.day == int(date_string[8:10])\n    except ValueError:\n        # Ensure that ValueError is raised for invalid inputs\n        pass\n\n@given(st.data())\ndef test_time_component_property(data):\n    date_string = data.draw(st.sampled_from(list(valid_iso_date_strings())))\n    time_string = 'T00:00:00'\n    if 'T' not in date_string:\n        date_string += time_string\n    try:\n        result = datetime.fromisoformat(date_string)\n        # Check if time components are valid\n        assert result.hour == 0\n        assert result.minute == 0\n        assert result.second == 0\n        assert result.microsecond == 0\n    except ValueError:\n        pass\n\n@given(st.data())\ndef test_timezone_handling_property(data):\n    date_string = data.draw(st.sampled_from(list(valid_iso_date_strings())))\n    timezone_string = '+00:00'\n    if 'Z' not in date_string and not re.search(r'[+-]\\d{2}:\\d{2}', date_string):\n        date_string += timezone_string\n    try:\n        result = datetime.fromisoformat(date_string)\n        # Check if timezone info is correctly set\n        if '+' in date_string or '-' in date_string or 'Z' in date_string:\n            assert result.tzinfo is not None\n    except ValueError:\n        pass\n\n@given(st.data())\ndef test_invalid_input_property(data):\n    invalid_date_strings = [\n        'invalid-date',\n        '2021-13-01',\n        '2021-01-32',\n        '2021-01-01T25:00:00',\n        '2021-01-01T00:60:00',\n        '2021-01-01T00:00:60',\n        '2021-01-01 25:00:00',\n    ]\n    date_string = data.draw(st.sampled_from(invalid_date_strings))\n    try:\n        datetime.fromisoformat(date_string)\n        assert False, f\"ValueError expected but not raised for {date_string}\"\n    except ValueError:\n        pass\n\n@given(st.data())\ndef test_default_midnight_time_property(data):\n    date_string = data.draw(st.sampled_from(list(valid_iso_date_strings())))\n    if 'T' not in date_string:\n        date_string += 'T00:00:00'  # Append midnight if no time component\n    result = datetime.fromisoformat(date_string)\n    # Check if the default time component is midnight\n    assert result.hour == 0\n    assert result.minute == 0\n    assert result.second == 0\n    assert result.microsecond == 0\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timezone, timedelta\n\n# Property 1: Valid year, month, and day\n@given(st.from_regex(r'\\d{4}-\\d{2}-\\d{2}(T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d{1,6})?([+-]\\d{2}:\\d{2}|Z)?)?'))\ndef test_valid_date_components_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    assert 1 <= dt.month <= 12\n    assert 1 <= dt.day <= 31  # Validate further for specific month later if needed\n# End program\n\n# Property 2: Correct hour, minute, second, and microsecond\n@given(st.from_regex(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d{1,6})?([+-]\\d{2}:\\d{2}|Z)?'))\ndef test_correct_time_components_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    components = date_string.split('T')[1] if 'T' in date_string else '00:00:00'\n    time_part = components.split('+')[0].split('-')[0]\n    hour, minute, second = map(int, time_part.split(':')[:3])\n    assert dt.hour == hour\n    assert dt.minute == minute\n    assert dt.second == second\n# End program\n\n# Property 3: Time zone information handling\n@given(st.from_regex(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\.\\d{1,6})?([+-]\\d{2}:\\d{2}|Z)?'))\ndef test_timezone_handling_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    if 'Z' in date_string:\n        assert dt.tzinfo == timezone.utc\n    elif '+' in date_string or '-' in date_string:\n        offset = date_string.split('+')[-1] if '+' in date_string else date_string.split('-')[-1]\n        assert dt.tzinfo == timezone(timedelta(hours=int(offset[:3]), minutes=int(offset[4:])))\n# End program\n\n# Property 4: ValueError for invalid ISO 8601 formats\n@given(st.text())\ndef test_invalid_iso_format_property(invalid_string):\n    if not (invalid_string and len(invalid_string) < 7 or 'T' not in invalid_string):\n        try:\n            datetime.fromisoformat(invalid_string)\n            assert False, f\"Expected ValueError for: {invalid_string}\"\n        except ValueError:\n            pass\n# End program\n\n# Property 5: Default time component of midnight when not provided\n@given(st.from_regex(r'\\d{4}-\\d{2}-\\d{2}'))\ndef test_default_midnight_property(date_string):\n    dt = datetime.fromisoformat(date_string)\n    assert dt.hour == 0\n    assert dt.minute == 0\n    assert dt.second == 0\n# End program\n```"], "properties": ["1. The output datetime object should have a valid year, month, and day that corresponds to the input ISO 8601 date string, ensuring that the day is within the valid range for the specified month and year.\n\n2. If the input date string includes a time component, the output datetime object should reflect the correct hour, minute, second, and microsecond values as parsed from the input string.\n\n3. The output datetime object should properly handle and reflect any time zone information provided in the input string, including UTC and positive/negative offsets.\n\n4. The output datetime object should raise a ValueError for any input date strings that do not conform to the valid ISO 8601 formats specified in the documentation.\n\n5. The output datetime object should have a default time component of midnight (00:00:00) if the input date string does not include a time component."], "api_doc": "classmethod datetime.fromisoformat(date_string)\nReturn a datetime corresponding to a date_string in any valid ISO 8601 format, with the following exceptions:\n\nTime zone offsets may have fractional seconds.\n\nThe T separator may be replaced by any single unicode character.\n\nFractional hours and minutes are not supported.\n\nReduced precision dates are not currently supported (YYYY-MM, YYYY).\n\nExtended date representations are not currently supported (\u00b1YYYYYY-MM-DD).\n\nOrdinal dates are not currently supported (YYYY-OOO).\n\nExamples:\n\n>>>\nfrom datetime import datetime\ndatetime.fromisoformat('2011-11-04')\ndatetime.datetime(2011, 11, 4, 0, 0)\ndatetime.fromisoformat('20111104')\ndatetime.datetime(2011, 11, 4, 0, 0)\ndatetime.fromisoformat('2011-11-04T00:05:23')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\ndatetime.fromisoformat('2011-11-04T00:05:23Z')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, tzinfo=datetime.timezone.utc)\ndatetime.fromisoformat('20111104T000523')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\ndatetime.fromisoformat('2011-W01-2T00:05:23.283')\ndatetime.datetime(2011, 1, 4, 0, 5, 23, 283000)\ndatetime.fromisoformat('2011-11-04 00:05:23.283')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000)\ndatetime.fromisoformat('2011-11-04 00:05:23.283+00:00')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000, tzinfo=datetime.timezone.utc)\ndatetime.fromisoformat('2011-11-04T00:05:23+04:00')   \ndatetime.datetime(2011, 11, 4, 0, 5, 23,\n    tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))\nAdded in version 3.7.\n\nChanged in version 3.11: Previously, this method only supported formats that could be emitted by date.isoformat() or datetime.isoformat().", "api_code": "@classmethod\ndef fromisoformat(cls, date_string):\n    if not isinstance(date_string, str):\n        raise TypeError('fromisoformat: argument must be str')\n\n    if len(date_string) < 7:\n        raise ValueError(f'Invalid isoformat string: {date_string!r}')\n\n    # Split this at the separator\n    try:\n        separator_location = _find_isoformat_datetime_separator(date_string)\n        dstr = date_string[0:separator_location]\n        tstr = date_string[(separator_location+1):]\n\n        date_components = _parse_isoformat_date(dstr)\n    except ValueError:\n        raise ValueError(\n            f'Invalid isoformat string: {date_string!r}') from None\n\n    if tstr:\n        try:\n            time_components, became_next_day, error_from_components = _parse_isoformat_time(tstr)\n        except ValueError:\n            raise ValueError(\n                f'Invalid isoformat string: {date_string!r}') from None\n        else:\n            if error_from_components:\n                raise ValueError(\"minute, second, and microsecond must be 0 when hour is 24\")\n\n            if became_next_day:\n                year, month, day = date_components\n                # Only wrap day/month when it was previously valid\n                if month <= 12 and day <= (days_in_month := _days_in_month(year, month)):\n                    # Calculate midnight of the next day\n                    day += 1\n                    if day > days_in_month:\n                        day = 1\n                        month += 1\n                        if month > 12:\n                            month = 1\n                            year += 1\n                    date_components = [year, month, day]\n    else:\n        time_components = [0, 0, 0, 0, None]\n\n    return cls(*(date_components + time_components))"}
{"function_name": "datetime.date.isocalendar", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\nimport sys\n\n@given(st.integers(min_value=1, max_value=sys.maxsize),\n       st.integers(min_value=1, max_value=12),\n       st.integers(min_value=1, max_value=31))\ndef test_year_property(year, month, day):\n    iso_year, _, _ = date(year, month, day).isocalendar()\n    assert iso_year in {year, year - 1, year + 1}\n\n@given(st.integers(min_value=1, max_value=sys.maxsize),\n       st.integers(min_value=1, max_value=12),\n       st.integers(min_value=1, max_value=31))\ndef test_week_property(year, month, day):\n    _, iso_week, _ = date(year, month, day).isocalendar()\n    assert 1 <= iso_week <= 53\n\n@given(st.integers(min_value=1, max_value=sys.maxsize),\n       st.integers(min_value=1, max_value=12),\n       st.integers(min_value=1, max_value=31))\ndef test_weekday_property(year, month, day):\n    _, _, iso_weekday = date(year, month, day).isocalendar()\n    assert 1 <= iso_weekday <= 7\n\n@given(st.integers(min_value=1, max_value=sys.maxsize),\n       st.integers(min_value=1, max_value=12),\n       st.integers(min_value=1, max_value=31))\ndef test_first_of_year_property(year, month, day):\n    if month == 1 and day == 1:\n        iso_year, iso_week, iso_weekday = date(year, month, day).isocalendar()\n        if (date(year, month, day).weekday() >= 3):  # Thursday or later\n            assert iso_week == 1\n        else:\n            assert iso_week == 53 or iso_week == 52  # last week of previous year\n\n@given(st.integers(min_value=1, max_value=sys.maxsize),\n       st.integers(min_value=1, max_value=12),\n       st.integers(min_value=1, max_value=31))\ndef test_consistency_property(year, month, day):\n    result1 = date(year, month, day).isocalendar()\n    result2 = date(year, month, day).isocalendar()\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\nimport sys\n\n@given(st.integers(min_value=1, max_value=sys.maxsize), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_output_year_property(year, month, day):\n    try:\n        result = date(year, month, day).isocalendar()\n        assert result.year in {year, year - 1, year + 1}\n    except ValueError:\n        pass  # Skip invalid date combinations\n\n@given(st.integers(min_value=1, max_value=sys.maxsize), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_output_week_property(year, month, day):\n    try:\n        result = date(year, month, day).isocalendar()\n        assert 1 <= result.week <= 53\n    except ValueError:\n        pass  # Skip invalid date combinations\n\n@given(st.integers(min_value=1, max_value=sys.maxsize), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_output_weekday_property(year, month, day):\n    try:\n        result = date(year, month, day).isocalendar()\n        assert 1 <= result.weekday <= 7\n    except ValueError:\n        pass  # Skip invalid date combinations\n\n@given(st.integers(min_value=1, max_value=sys.maxsize), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_first_day_of_year_property(year, month, day):\n    if month == 1 and day == 1:  # Check only January 1st\n        result = date(year, month, day).isocalendar()\n        assert result.week == (1 if (date(year, month, day).weekday() >= 3) else 53)\n\n@given(st.integers(min_value=1, max_value=sys.maxsize), \n       st.integers(min_value=1, max_value=12), \n       st.integers(min_value=1, max_value=31))\ndef test_consistency_property(year, month, day):\n    try:\n        result1 = date(year, month, day).isocalendar()\n        result2 = date(year, month, day).isocalendar()\n        assert result1 == result2\n    except ValueError:\n        pass  # Skip invalid date combinations\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\nimport sys\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_output_year_property(year, month, day):\n    try:\n        iso_year, _, _ = date(year, month, day).isocalendar()\n        assert iso_year in {year, year - 1, year + 1}\n    except ValueError:\n        pass  # Ignore invalid dates\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_output_week_property(year, month, day):\n    try:\n        _, iso_week, _ = date(year, month, day).isocalendar()\n        assert 1 <= iso_week <= 53\n    except ValueError:\n        pass  # Ignore invalid dates\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_output_weekday_property(year, month, day):\n    try:\n        _, _, iso_weekday = date(year, month, day).isocalendar()\n        assert 1 <= iso_weekday <= 7\n    except ValueError:\n        pass  # Ignore invalid dates\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_first_day_of_year_property(year, month, day):\n    if month == 1 and day == 1:\n        iso_year, iso_week, iso_weekday = date(year, month, day).isocalendar()\n        if iso_weekday >= 4:  # Thursday or later\n            assert iso_week == 1\n        else:\n            assert iso_week == 53  # This part may depend on the year\n    \n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_consistent_output_property(year, month, day):\n    try:\n        iso_calendar_output = date(year, month, day).isocalendar()\n        assert iso_calendar_output == date(year, month, day).isocalendar()  # Should be consistent\n    except ValueError:\n        pass  # Ignore invalid dates\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\nfrom datetime import timedelta\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_isocalendar_year_property(year, month, day):\n    # Adjust for valid date ranges\n    if month in [1, 3, 5, 7, 8, 10, 12]:\n        day = st.integers(min_value=1, max_value=31).example()\n    elif month in [4, 6, 9, 11]:\n        day = st.integers(min_value=1, max_value=30).example()\n    elif month == 2:\n        day = st.integers(min_value=1, max_value=29).example()  # Consider leap years later\n\n    dt = date(year, month, day)\n    iso_year, week, weekday = dt.isocalendar()\n    assert iso_year in {year, year - 1, year + 1}\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_isocalendar_week_property(year, month, day):\n    # Adjust for valid date ranges\n    if month in [1, 3, 5, 7, 8, 10, 12]:\n        day = st.integers(min_value=1, max_value=31).example()\n    elif month in [4, 6, 9, 11]:\n        day = st.integers(min_value=1, max_value=30).example()\n    elif month == 2:\n        day = st.integers(min_value=1, max_value=29).example()  # Consider leap years later\n\n    dt = date(year, month, day)\n    iso_year, week, weekday = dt.isocalendar()\n    assert 1 <= week <= 53\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_isocalendar_weekday_property(year, month, day):\n    # Adjust for valid date ranges\n    if month in [1, 3, 5, 7, 8, 10, 12]:\n        day = st.integers(min_value=1, max_value=31).example()\n    elif month in [4, 6, 9, 11]:\n        day = st.integers(min_value=1, max_value=30).example()\n    elif month == 2:\n        day = st.integers(min_value=1, max_value=29).example()  # Consider leap years later\n\n    dt = date(year, month, day)\n    iso_year, week, weekday = dt.isocalendar()\n    assert 1 <= weekday <= 7\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_isocalendar_first_of_year_property(year, month, day):\n    # Adjust for valid date ranges\n    if month in [1, 3, 5, 7, 8, 10, 12]:\n        day = st.integers(min_value=1, max_value=31).example()\n    elif month in [4, 6, 9, 11]:\n        day = st.integers(min_value=1, max_value=30).example()\n    elif month == 2:\n        day = st.integers(min_value=1, max_value=29).example()  # Consider leap years later\n        \n    if month == 1 and day == 1:\n        dt = date(year, month, day)\n        iso_year, week, weekday = dt.isocalendar()\n        assert week == (1 if dt.weekday() <= 3 else 52)  # Check if it falls into the first week or the last week of the previous year.\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_isocalendar_consistency_property(year, month, day):\n    # Adjust for valid date ranges\n    if month in [1, 3, 5, 7, 8, 10, 12]:\n        day = st.integers(min_value=1, max_value=31).example()\n    elif month in [4, 6, 9, 11]:\n        day = st.integers(min_value=1, max_value=30).example()\n    elif month == 2:\n        day = st.integers(min_value=1, max_value=29).example()  # Consider leap years later\n\n    dt = date(year, month, day)\n    iso_result1 = dt.isocalendar()\n    iso_result2 = dt.isocalendar()\n    assert iso_result1 == iso_result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_year_range_property(year, month, day):\n    iso_year, iso_week, iso_weekday = date(year, month, day).isocalendar()\n    assert iso_year in {year, year - 1, year + 1}\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_week_range_property(year, month, day):\n    iso_year, iso_week, iso_weekday = date(year, month, day).isocalendar()\n    assert 1 <= iso_week <= 53\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_weekday_range_property(year, month, day):\n    iso_year, iso_week, iso_weekday = date(year, month, day).isocalendar()\n    assert 1 <= iso_weekday <= 7\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_january_first_property(year, month, day):\n    if month == 1 and day == 1:\n        iso_year, iso_week, iso_weekday = date(year, month, day).isocalendar()\n        if date(year, month, day).weekday() >= 3:  # Thursday or later\n            assert iso_week == 1\n        else:\n            previous_week_iso_year, previous_week_iso_week, previous_week_iso_weekday = date(year - 1, 12, 31).isocalendar()\n            assert iso_week == previous_week_iso_week\n\n@given(st.integers(min_value=1, max_value=9999), st.integers(min_value=1, max_value=12), st.integers(min_value=1, max_value=31))\ndef test_consistency_property(year, month, day):\n    iso_calendar_output1 = date(year, month, day).isocalendar()\n    iso_calendar_output2 = date(year, month, day).isocalendar()\n    assert iso_calendar_output1 == iso_calendar_output2\n# End program\n```"], "properties": ["1. The output year must be equal to the input year or the input year \u00b1 1, depending on the ISO week calculation rules.\n2. The output week must be in the range of 1 to 53, inclusive, as the ISO calendar can have up to 53 weeks in a year.\n3. The output weekday must be in the range of 1 to 7, representing Monday through Sunday, inclusive.\n4. For a given date, if it is the first day of the year (January 1), the returned week should be 1 if that day falls on a Thursday or later; otherwise, it should be in the last week of the previous year.\n5. The output should maintain consistency such that if the same date is input multiple times, the output remains the same across those invocations."], "api_doc": "date.isocalendar()\nReturn a named tuple object with three components: year, week and weekday.\n\nThe ISO calendar is a widely used variant of the Gregorian calendar. [3]\n\nThe ISO year consists of 52 or 53 full weeks, and where a week starts on a Monday and ends on a Sunday. The first week of an ISO year is the first (Gregorian) calendar week of a year containing a Thursday. This is called week number 1, and the ISO year of that Thursday is the same as its Gregorian year.\n\nFor example, 2004 begins on a Thursday, so the first week of ISO year 2004 begins on Monday, 29 Dec 2003 and ends on Sunday, 4 Jan 2004:\n\n>>>\nfrom datetime import date\ndate(2003, 12, 29).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=1)\ndate(2004, 1, 4).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=7)\nChanged in version 3.9: Result changed from a tuple to a named tuple.", "api_code": "def isocalendar(self):\n    year = self._year\n    week1monday = _isoweek1monday(year)\n    today = _ymd2ord(self._year, self._month, self._day)\n    # Internally, week and day have origin 0\n    week, day = divmod(today - week1monday, 7)\n    if week < 0:\n        year -= 1\n        week1monday = _isoweek1monday(year)\n        week, day = divmod(today - week1monday, 7)\n    elif week >= 52:\n        if today >= _isoweek1monday(year+1):\n            year += 1\n            week = 0\n    return _IsoCalendarDate(year, week+1, day+1)"}
{"function_name": "dateutil.parser.isoparse", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom dateutil.parser import isoparse\nfrom datetime import datetime, timedelta\n\n@given(st.text())\ndef test_output_is_valid_datetime_property(dt_str):\n    try:\n        result = isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # It's acceptable for the input to raise a ValueError\n\n@given(st.text())\ndef test_default_time_to_midnight_property(dt_str):\n    if \"T\" not in dt_str and any(char.isdigit() for char in dt_str):\n        try:\n            result = isoparse(dt_str)\n            assert result.time() == datetime.min.time()  # Should default to midnight\n        except ValueError:\n            pass  # It's acceptable for the input to raise a ValueError\n\n@given(st.text())\ndef test_midnight_as_24_hours_property(dt_str):\n    if \"24:00\" in dt_str:\n        try:\n            result = isoparse(dt_str)\n            assert result.time() == datetime.min.time()  # Should represent the next day at midnight\n            assert result.date() == (datetime.strptime(dt_str.split(\"T\")[0], \"%Y-%m-%d\") + timedelta(days=1)).date()\n        except ValueError:\n            pass  # It's acceptable for the input to raise a ValueError\n\n@given(st.text())\ndef test_correct_time_zone_offset_property(dt_str):\n    try:\n        result = isoparse(dt_str)\n        # Verify if the output datetime reflects the time zone offset correctly\n        # This is a simplified check; in a real context, you would assert against known offsets\n        assert result.tzinfo is not None\n    except ValueError:\n        pass  # It's acceptable for the input to raise a ValueError\n\n@given(st.text())\ndef test_incomplete_dates_default_to_earliest_property(dt_str):\n    if any(char.isdigit() for char in dt_str):\n        try:\n            result = isoparse(dt_str)\n            # Check if the date defaults correctly (e.g., \"2023-01\" should yield 2023-01-01)\n            assert result.day == 1  # Assuming day is defaulted to 1 for incomplete dates\n        except ValueError:\n            pass  # It's acceptable for the input to raise a ValueError\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nfrom dateutil.parser import isoparse\nimport pytz\n\n@given(st.text())\ndef test_output_is_valid_datetime_property(dt_str):\n    try:\n        result = isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Expecting ValueError for invalid inputs\n\n@given(st.text())\ndef test_default_time_to_midnight_property(dt_str):\n    if \"T\" not in dt_str and any(part in dt_str for part in [\"-\", \"Z\", \"+\", \":\"]):\n        result = isoparse(dt_str)\n        assert result.time() == datetime.min.time()  # Should be midnight\n\n@given(st.text())\ndef test_midnight_as_24_hours_property(dt_str):\n    if \"24:00\" in dt_str:\n        result = isoparse(dt_str)\n        assert result.time() == datetime.min.time() and result.date() > datetime.now().date()\n\n@given(st.text())\ndef test_correct_time_zone_offset_property(dt_str):\n    if any(offset in dt_str for offset in [\"Z\", \"+\", \"-\"]):\n        result = isoparse(dt_str)\n        assert result.tzinfo is not None  # Should have a timezone info\n\n@given(st.text())\ndef test_incomplete_date_format_defaults_property(dt_str):\n    if any(part in dt_str for part in [\"YYYY-\", \"YYYY\", \"YYYY-MM\"]):\n        result = isoparse(dt_str)\n        assert result.day == 1  # Default day should be 1 for incomplete dates\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport dateutil.parser\n\n@given(st.text())\ndef test_output_is_valid_datetime_property(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Expecting some strings to raise ValueError\n\n@given(st.text())\ndef test_no_time_portion_defaults_to_midnight_property(dt_str):\n    if '-' in dt_str and 'T' not in dt_str:  # Only consider date strings without time\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.time() == datetime.min.time()  # Should be midnight\n\n@given(st.text())\ndef test_midnight_24_hour_format_next_day_property(dt_str):\n    if '24:00' in dt_str:\n        result = dateutil.parser.isoparse(dt_str.replace('24:00', '00:00'))\n        assert result.time() == datetime.min.time()\n        assert result.date() == (datetime.strptime(dt_str.split('T')[0], '%Y-%m-%d') + timedelta(days=1)).date()\n\n@given(st.text())\ndef test_correctly_reflects_time_zone_offset_property(dt_str):\n    if any(offset in dt_str for offset in ['Z', '+', '-']):\n        result = dateutil.parser.isoparse(dt_str)\n        assert isinstance(result.tzinfo, datetime.tzinfo)  # Should have a timezone info\n\n@given(st.text())\ndef test_incomplete_date_formats_defaults_property(dt_str):\n    if 'T' not in dt_str:  # Check for incomplete dates\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.day == 1  # Should default to the first day of the month\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom dateutil.parser import isoparse\nimport datetime\n\n@given(st.text())\ndef test_output_is_valid_datetime_property(dt_str):\n    try:\n        result = isoparse(dt_str)\n        assert isinstance(result, datetime.datetime)\n    except ValueError:\n        pass  # Expecting some inputs to raise ValueError\n\n@given(st.text())\ndef test_incomplete_date_defaults_to_midnight_property(dt_str):\n    if any(c in dt_str for c in ['T', 'Z', '+', '-']):\n        return  # Only test pure date strings\n    try:\n        result = isoparse(dt_str)\n        assert result.time() == datetime.time(0, 0, 0)\n    except ValueError:\n        pass  # Expecting some inputs to raise ValueError\n\n@given(st.text())\ndef test_midnight_24_hours_rollover_property(dt_str):\n    if \"24:00\" in dt_str:\n        try:\n            result = isoparse(dt_str)\n            assert result.time() == datetime.time(0, 0, 0)\n            assert result.date() != datetime.datetime.strptime(dt_str.split(\"T\")[0], \"%Y-%m-%d\").date()\n        except ValueError:\n            pass  # Expecting some inputs to raise ValueError\n\n@given(st.text())\ndef test_timezone_offset_correctness_property(dt_str):\n    try:\n        result = isoparse(dt_str)\n        if '+' in dt_str or '-' in dt_str or 'Z' in dt_str:\n            assert result.tzinfo is not None\n    except ValueError:\n        pass  # Expecting some inputs to raise ValueError\n\n@given(st.text())\ndef test_incomplete_date_formats_default_property(dt_str):\n    if any(c in dt_str for c in ['T', 'Z', '+', '-']):\n        return  # Only test pure date strings\n    try:\n        result = isoparse(dt_str)\n        year_month_day = dt_str.split('-')\n        if len(year_month_day) == 2:  # YYYY-MM\n            assert result.day == 1\n        elif len(year_month_day) == 1:  # YYYY\n            assert result.month == 1 and result.day == 1\n    except ValueError:\n        pass  # Expecting some inputs to raise ValueError\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta\nimport dateutil.parser\n\n@given(st.text())\ndef test_output_is_valid_datetime_property(dt_str):\n    try:\n        result = dateutil.parser.isoparse(dt_str)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Catch invalid ISO strings, as they are expected to raise errors.\n\n@given(st.text())\ndef test_date_without_time_defaults_to_midnight_property(dt_str):\n    if 'T' not in dt_str:\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.time() == datetime.min.time()  # Should default to midnight (00:00:00).\n\n@given(st.text())\ndef test_midnight_string_translates_to_next_day_property(dt_str):\n    if dt_str == \"24:00\":\n        result = dateutil.parser.isoparse(\"2023-01-01T24:00\")\n        assert result == datetime(2023, 1, 2, 0, 0)  # Should return the next day at midnight.\n\n@given(st.text())\ndef test_time_zone_offset_is_correctly_applied_property(dt_str):\n    if 'Z' in dt_str or '+' in dt_str or '-' in dt_str:\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.tzinfo is not None  # Should have a timezone info if input specified.\n\n@given(st.text())\ndef test_incomplete_date_formats_return_earliest_valid_date_property(dt_str):\n    if len(dt_str) in [7, 4]:  # Formats like YYYY-MM or YYYY\n        result = dateutil.parser.isoparse(dt_str)\n        assert result.day == 1  # Should default to the first of the month or first day of the year.\n# End program\n```"], "properties": ["1. The output of the function is always a valid `datetime.datetime` object, representing a point in time that corresponds to the input ISO-8601 string.\n\n2. If the input string represents a date without a time portion, the time components of the output `datetime` object default to midnight (00:00:00).\n\n3. If the input string includes a time portion that specifies midnight as \"24:00\", the output `datetime` object should represent the next day at midnight (00:00:00).\n\n4. The output `datetime` object should correctly reflect any provided time zone offset, converting the time to UTC if necessary.\n\n5. The function should handle incomplete date formats gracefully, returning the earliest possible valid date for the unspecified components (e.g., for \"2023-01\", the output should default the day to the first of the month)."], "api_doc": "parser.isoparse(dt_str)\uf0c1\nParse an ISO-8601 datetime string into a datetime.datetime.\n\nAn ISO-8601 datetime string consists of a date portion, followed optionally by a time portion - the date and time portions are separated by a single character separator, which is T in the official standard. Incomplete date formats (such as YYYY-MM) may not be combined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\nYYYY\n\nYYYY-MM\n\nYYYY-MM-DD or YYYYMMDD\n\nUncommon:\n\nYYYY-Www or YYYYWww - ISO week (day defaults to 0)\n\nYYYY-Www-D or YYYYWwwD - ISO week and day\n\nThe ISO week and day numbering follows the same logic as datetime.date.isocalendar().\n\nSupported time formats are:\n\nhh\n\nhh:mm or hhmm\n\nhh:mm:ss or hhmmss\n\nhh:mm:ss.ssssss (Up to 6 sub-second digits)\n\nMidnight is a special case for hh, as the standard supports both 00:00 and 24:00 as a representation. The decimal separator can be either a dot or a comma.\n\nCaution\n\nSupport for fractional components other than seconds is part of the ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\nZ (UTC)\n\n\u00b1HH:MM\n\n\u00b1HHMM\n\n\u00b1HH\n\nOffsets will be represented as dateutil.tz.tzoffset objects, with the exception of UTC, which will be represented as dateutil.tz.tzutc. Time zone offsets equivalent to UTC (such as +00:00) will also be represented as dateutil.tz.tzutc.\n\nParameters\n:\ndt_str \u2013 A string or stream containing only an ISO-8601 datetime string\n\nReturns\n:\nReturns a datetime.datetime representing the string. Unspecified components default to their lowest value.\n\nWarning\n\nAs of version 2.7.0, the strictness of the parser should not be considered a stable part of the contract. Any valid ISO-8601 string that parses correctly with the default settings will continue to parse correctly in future versions, but invalid strings that currently fail (e.g. 2017-01-01T00:00+00:00:00) are not guaranteed to continue failing in future versions if they encode a valid date.\n\nNew in version 2.7.0.", "api_code": "@_takes_ascii\ndef isoparse(self, dt_str):\n    components, pos = self._parse_isodate(dt_str)\n\n    if len(dt_str) > pos:\n        if self._sep is None or dt_str[pos:pos + 1] == self._sep:\n            components += self._parse_isotime(dt_str[pos + 1:])\n        else:\n            raise ValueError('String contains unknown ISO components')\n\n    if len(components) > 3 and components[3] == 24:\n        components[3] = 0\n        return datetime(*components) + timedelta(days=1)\n\n    return datetime(*components)"}
{"function_name": "statistics.mean", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom statistics import StatisticsError\nfrom fractions import Fraction as F\nfrom decimal import Decimal as D\n\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_non_empty_numeric_output_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, int, Decimal, F))\n\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_mean_identical_values_property(data):\n    identical_value = data[0]\n    identical_data = [identical_value] * len(data)\n    result = statistics.mean(identical_data)\n    assert result == identical_value\n\n\n@given(st.lists(st.floats(), min_size=1), st.floats())\ndef test_mean_outlier_impact_property(data, outlier):\n    original_mean = statistics.mean(data)\n    modified_data = data + [outlier]\n    modified_mean = statistics.mean(modified_data)\n    assert modified_mean != original_mean\n\n\n@given(st.lists(st.floats(), min_size=0))\ndef test_mean_empty_data_statistics_error_property(data):\n    if len(data) == 0:\n        try:\n            statistics.mean(data)\n            assert False, \"Expected StatisticsError for empty data\"\n        except StatisticsError:\n            pass  # Expected behavior\n    else:\n        statistics.mean(data)  # Should not raise an error\n\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_invariance_under_addition_of_zero_property(data):\n    original_mean = statistics.mean(data)\n    modified_data = data + [0]\n    modified_mean = statistics.mean(modified_data)\n    assert modified_mean == original_mean\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import mean, StatisticsError\nfrom fractions import Fraction\nfrom decimal import Decimal\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_non_empty_numeric_property(data):\n    result = mean(data)\n    assert isinstance(result, (float, Decimal, Fraction))\n\n@given(st.lists(st.just(5.0), min_size=1))\ndef test_mean_identical_values_property(data):\n    result = mean(data)\n    assert result == 5.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1),\n       st.floats(allow_nan=False, allow_infinity=False))\ndef test_mean_outlier_effect_property(data, outlier):\n    original_mean = mean(data)\n    modified_data = data + [outlier]\n    modified_mean = mean(modified_data)\n    assert modified_mean != original_mean\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_mean_empty_dataset_property(data):\n    if len(data) == 0:\n        try:\n            mean(data)\n            assert False  # Should not reach here\n        except StatisticsError:\n            pass  # Expected behavior\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_add_zero_invariance_property(data):\n    original_mean = mean(data)\n    modified_mean = mean(data + [0])\n    assert original_mean == modified_mean\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom fractions import Fraction\nfrom decimal import Decimal\nfrom hypothesis import HealthCheck, settings\n\n# Setting health check to ignore certain issues due to large inputs\nsettings.register_profile(\"ignore_large\", max_examples=100, suppress_health_check=[HealthCheck.data_too_large])\nsettings.set_profile(\"ignore_large\")\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_numeric_output_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, Decimal, Fraction)), \"Mean should be a numeric type\"\n\n@given(st.lists(st.floats()))\ndef test_mean_identical_values_property(data):\n    if len(data) > 0:\n        identical_value = data[0]\n        mean_value = statistics.mean([identical_value] * len(data))\n        assert mean_value == identical_value, \"Mean of identical values should equal that value\"\n\n@given(st.lists(st.floats(), min_size=1), st.floats())\ndef test_mean_outlier_effect_property(data, outlier):\n    original_mean = statistics.mean(data)\n    altered_data = data + [outlier]\n    altered_mean = statistics.mean(altered_data)\n    assert (outlier > original_mean and altered_mean > original_mean) or \\\n           (outlier < original_mean and altered_mean < original_mean), \"Mean should be affected by outliers\"\n\n@given(st.lists(st.floats()))\ndef test_mean_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            statistics.mean(data)\n            assert False, \"Mean should raise StatisticsError for empty data\"\n        except statistics.StatisticsError:\n            pass  # Expected behavior\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_invariance_under_zero_property(data):\n    original_mean = statistics.mean(data)\n    mean_with_zero = statistics.mean(data + [0])\n    assert mean_with_zero == original_mean, \"Mean should be invariant under the addition of zero\"\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import mean, StatisticsError\nfrom fractions import Fraction\nfrom decimal import Decimal\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_non_empty_output_type_property(data):\n    result = mean(data)\n    assert isinstance(result, (float, Decimal, Fraction))\n\n@given(st.integers(), st.integers())\ndef test_mean_identical_values_property(value1, value2):\n    result = mean([value1, value1, value1])\n    assert result == value1\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1), st.floats())\ndef test_mean_outlier_impact_property(data, outlier):\n    original_mean = mean(data)\n    altered_data = data + [outlier]\n    altered_mean = mean(altered_data)\n    assert altered_mean != original_mean\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_mean_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            mean(data)\n            assert False  # Should not reach here\n        except StatisticsError:\n            assert True  # Expected exception raised\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_invariance_under_zero_property(data):\n    original_mean = mean(data)\n    mean_with_zero = mean(data + [0])\n    assert mean_with_zero == original_mean\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import mean, StatisticsError\nfrom fractions import Fraction\nfrom decimal import Decimal\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_mean_non_empty_numeric_property(data):\n    result = mean(data)\n    assert isinstance(result, (int, float, Decimal, Fraction))\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_identical_values_property(value):\n    identical_list = [value] * 5\n    result = mean(identical_list)\n    assert result == value\n\n@given(st.lists(st.floats(), min_size=1), st.floats())\ndef test_mean_effect_of_outliers_property(data, outlier):\n    original_mean = mean(data)\n    modified_data = data + [outlier]\n    modified_mean = mean(modified_data)\n    if outlier > max(data):\n        assert modified_mean > original_mean\n    elif outlier < min(data):\n        assert modified_mean < original_mean\n\n@given(st.lists(st.integers(), min_size=0))\ndef test_mean_empty_data_property(data):\n    if len(data) == 0:\n        try:\n            mean(data)\n        except StatisticsError:\n            assert True\n    else:\n        assert True  # Ensure that we don't raise an error for non-empty lists\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_invariance_under_zero_addition_property(data):\n    original_mean = mean(data)\n    modified_data = data + [0]\n    modified_mean = mean(modified_data)\n    assert original_mean == modified_mean\n\n# End program\n```"], "properties": ["1. The mean of a non-empty dataset should always be a numeric type (int, float, Decimal, or Fraction), depending on the input data type.\n2. The mean of a dataset containing identical values should equal that value (e.g., mean([5, 5, 5]) should return 5).\n3. The mean of a dataset should be affected by outliers, meaning that adding a significantly larger or smaller value to the dataset should change the mean in the expected direction.\n4. The mean of an empty dataset should raise a StatisticsError, indicating that at least one data point is required to calculate the mean.\n5. The mean of a dataset should be invariant under the addition of zero (e.g., mean([1, 2, 3]) should equal mean([1, 2, 3, 0]))."], "api_doc": "statistics.mean(data)\nReturn the sample arithmetic mean of data which can be a sequence or iterable.\n\nThe arithmetic mean is the sum of the data divided by the number of data points. It is commonly called \u201cthe average\u201d, although it is only one of many different mathematical averages. It is a measure of the central location of the data.\n\nIf data is empty, StatisticsError will be raised.\n\nSome examples of use:\n\n>>>\nmean([1, 2, 3, 4, 4])\n2.8\nmean([-1.0, 2.5, 3.25, 5.75])\n2.625\n\nfrom fractions import Fraction as F\nmean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\nfrom decimal import Decimal as D\nmean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\nNote The mean is strongly affected by outliers and is not necessarily a typical example of the data points. For a more robust, although less efficient, measure of central tendency, see median().\nThe sample mean gives an unbiased estimate of the true population mean, so that when taken on average over all the possible samples, mean(sample) converges on the true mean of the entire population. If data represents the entire population rather than a sample, then mean(data) is equivalent to calculating the true population mean \u03bc.", "api_code": "def mean(data):\n    T, total, n = _sum(data)\n    if n < 1:\n        raise StatisticsError('mean requires at least one data point')\n    return _convert(total / n, T)"}
{"function_name": "networkx.algorithms.cycles.find_cycle", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_cycle_found_property(edges):\n    G = nx.Graph(edges)\n    cycle = nx.find_cycle(G, orientation='ignore')\n    assert len(cycle) > 0  # A cycle must be found.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_cycle_closed_loop_property(edges):\n    G = nx.Graph(edges)\n    cycle = nx.find_cycle(G, orientation='ignore')\n    # Extract the tail of the first edge and head of the last edge\n    first_edge = cycle[0]\n    last_edge = cycle[-1]\n    assert first_edge[0] == last_edge[1]  # Last edge must connect back to the first edge.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_orientation_respected_property(edges):\n    G = nx.DiGraph(edges)\n    cycle_original = nx.find_cycle(G, orientation='original')\n    cycle_reverse = nx.find_cycle(G, orientation='reverse')\n    # Check that the traversal direction is respected\n    for edge in cycle_original:\n        assert edge in G.edges()  # Must respect original orientation\n    for edge in cycle_reverse:\n        assert (edge[1], edge[0]) in G.edges()  # Must respect reverse orientation\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_no_cycle_in_acyclic_graph_property(edges):\n    G = nx.DiGraph(edges)\n    if not nx.is_directed_acyclic_graph(G):\n        return  # Skip if the graph is not acyclic\n    try:\n        nx.find_cycle(G)\n        assert False  # Should raise an exception\n    except NetworkXNoCycle:\n        assert True  # Exception raised as expected\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_edges_ordered_correctly_property(edges):\n    G = nx.Graph(edges)\n    cycle = nx.find_cycle(G, orientation='ignore')\n    # Check if the edges in the cycle are in the order they were traversed\n    assert all(cycle[i][1] == cycle[i + 1][0] for i in range(len(cycle) - 1))  # Edge order must be correct\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n# Property 1: If a cycle is found, the output must contain at least one edge.\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_find_cycle_contains_edges_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        assert len(cycle) > 0\n    except NetworkXNoCycle:\n        pass\n\n# Property 2: The edges in the output cycle must form a closed loop.\n@given(st.lists(st.tuples(st.integers(), st.integers())))\ndef test_find_cycle_closed_loop_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        # Check if the cycle forms a closed loop\n        tail = cycle[0][0]\n        head = cycle[-1][1]\n        assert tail == head\n    except NetworkXNoCycle:\n        pass\n\n# Property 3: For directed graphs, the output edges must respect the specified orientation.\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), \n                st.sampled_from([None, 'original', 'reverse', 'ignore']))\ndef test_find_cycle_orientation_property(edges, orientation):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation=orientation)\n        if orientation == 'original':\n            assert all(e[0] == cycle[i][0] for i, e in enumerate(cycle))\n        elif orientation == 'reverse':\n            assert all(e[1] == cycle[i][0] for i, e in enumerate(cycle))\n        # Note: 'ignore' does not impose any restriction; it treats edges as undirected.\n    except NetworkXNoCycle:\n        pass\n\n# Property 4: If the input graph is acyclic, the function must raise a NetworkXNoCycle exception.\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_find_cycle_no_cycle_property(edges):\n    G = nx.DiGraph(edges)\n    if not nx.is_directed_acyclic_graph(G):\n        try:\n            nx.find_cycle(G)\n        except NetworkXNoCycle:\n            pass\n    else:\n        assert nx.find_cycle(G) is None  # Should raise an exception if acyclic\n\n# Property 5: The output must accurately represent the edges traversed during the depth-first search.\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_find_cycle_edges_traversed_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        # Check that the edges in the cycle were part of the original edges\n        for edge in cycle:\n            assert edge in G.edges()\n    except NetworkXNoCycle:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), \n                st.integers())\ndef test_cycle_found_property(edges, source):\n    G = nx.Graph(edges)\n    cycle = nx.find_cycle(G, source=source)\n    assert len(cycle) > 0  # Output must contain at least one edge if a cycle is found.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), \n                st.integers())\ndef test_cycle_closed_loop_property(edges, source):\n    G = nx.Graph(edges)\n    try:\n        cycle = nx.find_cycle(G, source=source)\n        tail = cycle[-1][0]\n        head = cycle[0][1]\n        assert tail == head  # The last edge must connect back to the first edge.\n    except NetworkXNoCycle:\n        pass  # If no cycle, the property holds trivially.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), \n                st.one_of(st.just('original'), st.just('reverse'), st.just('ignore')))\ndef test_orientation_respected_property(edges, orientation):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation=orientation)\n        for edge in cycle:\n            if orientation == 'original':\n                assert (edge[0], edge[1]) in G.edges()\n            elif orientation == 'reverse':\n                assert (edge[1], edge[0]) in G.edges()\n            elif orientation == 'ignore':\n                assert (edge[0], edge[1]) in G.edges() or (edge[1], edge[0]) in G.edges()\n    except NetworkXNoCycle:\n        pass  # No cycle found, property holds trivially.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_no_cycle_in_acyclic_graph_property(edges):\n    G = nx.DiGraph(edges)\n    # Ensure the graph is acyclic by checking its DAG property\n    if nx.is_directed_acyclic_graph(G):\n        try:\n            nx.find_cycle(G)  # Should raise an exception\n            assert False  # If we reach here, the test fails\n        except NetworkXNoCycle:\n            assert True  # Correct behavior, exception raised\n    else:\n        try:\n            cycle = nx.find_cycle(G)  # May or may not find a cycle\n            assert len(cycle) > 0  # If a cycle is found, it should not be empty\n        except NetworkXNoCycle:\n            pass  # If no cycle, property holds trivially.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), \n                st.integers())\ndef test_edges_in_traversal_order_property(edges, source):\n    G = nx.Graph(edges)\n    try:\n        cycle = nx.find_cycle(G, source=source)\n        cycle_edges = [edge[:2] for edge in cycle]  # Get only the tail and head\n        assert cycle_edges == list(dict.fromkeys(cycle_edges))  # Ensure edges are in the order of traversal\n    except NetworkXNoCycle:\n        pass  # No cycle found, property holds trivially.\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.data())\ndef test_cycle_found_property(data):\n    # Generate a random directed graph\n    G = nx.generators.random_graphs.erdos_renyi_graph(n=data.draw(st.integers(min_value=1, max_value=20)), \n                                                       p=0.3, directed=True)\n    try:\n        cycle = nx.find_cycle(G)\n        assert len(cycle) > 0  # Ensure at least one edge in the cycle\n    except NetworkXNoCycle:\n        pass  # No cycle found is acceptable\n\n@given(st.data())\ndef test_cycle_closed_loop_property(data):\n    # Generate a directed graph with a cycle\n    G = nx.DiGraph()\n    G.add_edges_from(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=10)))\n    \n    # Manually ensure there's a cycle\n    if len(G.nodes) > 1:\n        G.add_edge(list(G.nodes)[0], list(G.nodes)[0])  # Create a self-loop\n\n    try:\n        cycle = nx.find_cycle(G)\n        # Extract the first and last edges\n        tail_first, head_first = cycle[0]\n        tail_last, head_last = cycle[-1]\n        assert head_last == tail_first  # Ensure the last edge connects to the first edge\n    except NetworkXNoCycle:\n        pass  # No cycle found is acceptable\n\n@given(st.data())\ndef test_orientation_respect_property(data):\n    # Generate a directed graph\n    G = nx.DiGraph()\n    edges = data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=10))\n    G.add_edges_from(edges)\n\n    try:\n        cycle_original = nx.find_cycle(G, orientation=\"original\")\n        cycle_reverse = nx.find_cycle(G, orientation=\"reverse\")\n        cycle_ignore = nx.find_cycle(G, orientation=\"ignore\")\n        \n        # Check the orientations\n        assert all((u, v) in cycle_original for (u, v) in edges)  # Original orientation\n        assert all((v, u) in cycle_reverse for (u, v) in edges)  # Reverse orientation\n    except NetworkXNoCycle:\n        pass  # No cycle found is acceptable\n\n@given(st.data())\ndef test_no_cycle_exception_property(data):\n    # Generate a Directed Acyclic Graph (DAG)\n    G = nx.DiGraph()\n    G.add_edges_from(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=10)))\n    \n    try:\n        nx.find_cycle(G)  # Should raise exception if no cycles\n    except NetworkXNoCycle:\n        assert True  # Exception raised as expected\n    else:\n        assert False  # If no exception, this is a failure\n\n@given(st.data())\ndef test_edges_order_in_traversal_property(data):\n    # Generate a random directed graph\n    G = nx.generators.random_graphs.erdos_renyi_graph(n=data.draw(st.integers(min_value=1, max_value=20)), \n                                                       p=0.3, directed=True)\n    try:\n        cycle = nx.find_cycle(G)\n        # Check if the edges in the cycle are in the order of traversal\n        for i in range(len(cycle) - 1):\n            assert cycle[i][1] == cycle[i + 1][0]  # Ensure the head of one edge is the tail of the next\n    except NetworkXNoCycle:\n        pass  # No cycle found is acceptable\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_cycle_found_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        assert len(cycle) > 0  # If a cycle is found, it must contain at least one edge\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is expected for acyclic graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_cycle_closed_loop_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        if len(cycle) > 0:\n            assert cycle[0][0] == cycle[-1][1]  # The head of the last edge must connect back to the tail of the first edge\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is expected for acyclic graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct(), st.sampled_from(['original', 'reverse', 'ignore']))\ndef test_orientation_respected_property(edges, orientation):\n    G = nx.DiGraph(edges)\n    cycle = nx.find_cycle(G, orientation=orientation)\n    if len(cycle) > 0:\n        for edge in cycle:\n            if orientation == 'original':\n                assert edge[0] in G.predecessors(edge[1])  # Follow original direction\n            elif orientation == 'reverse':\n                assert edge[1] in G.predecessors(edge[0])  # Follow reversed direction\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_no_cycle_exception_property(edges):\n    G = nx.DiGraph(edges)\n    if not nx.is_directed_acyclic_graph(G):\n        try:\n            nx.find_cycle(G)\n            assert False  # Should raise an exception if a cycle exists\n        except NetworkXNoCycle:\n            pass  # This is expected for acyclic graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_edges_order_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        if len(cycle) > 0:\n            for i in range(len(cycle) - 1):\n                assert cycle[i][1] == cycle[i + 1][0]  # Ensure that edges are traversed in the correct order\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is expected for acyclic graphs\n# End program\n```"], "properties": ["1. If a cycle is found, the output must contain at least one edge, indicating that a cyclic path exists in the graph.\n\n2. The edges in the output cycle must form a closed loop, meaning the head of the last edge must connect back to the tail of the first edge in the cycle.\n\n3. For directed graphs, the output edges must respect the specified orientation; if the orientation is 'original', 'reverse', or 'ignore', the traversal direction indicated in the output must align with the chosen orientation.\n\n4. If the input graph is acyclic (i.e., a Directed Acyclic Graph), the function must raise a `NetworkXNoCycle` exception, confirming that no cycle exists.\n\n5. The output must accurately represent the edges traversed during the depth-first search, including the correct order of edges as determined by the traversal process."], "api_doc": "find_cycle\nfind_cycle(G, source=None, orientation=None)[source]\nReturns a cycle found via depth-first traversal.\n\nThe cycle is a list of edges indicating the cyclic path. Orientation of directed edges is controlled by orientation.\n\nParameters\n:\nG\ngraph\nA directed/undirected graph/multigraph.\n\nsource\nnode, list of nodes\nThe node from which the traversal begins. If None, then a source is chosen arbitrarily and repeatedly until all edges from each node in the graph are searched.\n\norientation\nNone | \u2018original\u2019 | \u2018reverse\u2019 | \u2018ignore\u2019 (default: None)\nFor directed graphs and directed multigraphs, edge traversals need not respect the original orientation of the edges. When set to \u2018reverse\u2019 every edge is traversed in the reverse direction. When set to \u2018ignore\u2019, every edge is treated as undirected. When set to \u2018original\u2019, every edge is treated as directed. In all three cases, the yielded edge tuples add a last entry to indicate the direction in which that edge was traversed. If orientation is None, the yielded edge has no direction indicated. The direction is respected, but not reported.\n\nReturns\n:\nedges\ndirected edges\nA list of directed edges indicating the path taken for the loop. If no cycle is found, then an exception is raised. For graphs, an edge is of the form (u, v) where u and v are the tail and head of the edge as determined by the traversal. For multigraphs, an edge is of the form (u, v, key), where key is the key of the edge. When the graph is directed, then u and v are always in the order of the actual directed edge. If orientation is not None then the edge tuple is extended to include the direction of traversal (\u2018forward\u2019 or \u2018reverse\u2019) on that edge.\n\nRaises\n:\nNetworkXNoCycle\nIf no cycle was found.\n\nSee also\n\nsimple_cycles\nExamples\n\nIn this example, we construct a DAG and find, in the first call, that there are no directed cycles, and so an exception is raised. In the second call, we ignore edge orientations and find that there is an undirected cycle. Note that the second call finds a directed cycle while effectively traversing an undirected graph, and so, we found an \u201cundirected cycle\u201d. This means that this DAG structure does not form a directed tree (which is also known as a polytree).\n\nG = nx.DiGraph([(0, 1), (0, 2), (1, 2)])\nnx.find_cycle(G, orientation=\"original\")\nTraceback (most recent call last):\n    ...\nnetworkx.exception.NetworkXNoCycle: No cycle found.\nlist(nx.find_cycle(G, orientation=\"ignore\"))\n[(0, 1, 'forward'), (1, 2, 'forward'), (0, 2, 'reverse')]", "api_code": "@nx._dispatchable\ndef find_cycle(G, source=None, orientation=None):\n    if not G.is_directed() or orientation in (None, \"original\"):\n\n        def tailhead(edge):\n            return edge[:2]\n\n    elif orientation == \"reverse\":\n\n        def tailhead(edge):\n            return edge[1], edge[0]\n\n    elif orientation == \"ignore\":\n\n        def tailhead(edge):\n            if edge[-1] == \"reverse\":\n                return edge[1], edge[0]\n            return edge[:2]\n\n    explored = set()\n    cycle = []\n    final_node = None\n    for start_node in G.nbunch_iter(source):\n        if start_node in explored:\n            # No loop is possible.\n            continue\n\n        edges = []\n        # All nodes seen in this iteration of edge_dfs\n        seen = {start_node}\n        # Nodes in active path.\n        active_nodes = {start_node}\n        previous_head = None\n\n        for edge in nx.edge_dfs(G, start_node, orientation):\n            # Determine if this edge is a continuation of the active path.\n            tail, head = tailhead(edge)\n            if head in explored:\n                # Then we've already explored it. No loop is possible.\n                continue\n            if previous_head is not None and tail != previous_head:\n                # This edge results from backtracking.\n                # Pop until we get a node whose head equals the current tail.\n                # So for example, we might have:\n                #  (0, 1), (1, 2), (2, 3), (1, 4)\n                # which must become:\n                #  (0, 1), (1, 4)\n                while True:\n                    try:\n                        popped_edge = edges.pop()\n                    except IndexError:\n                        edges = []\n                        active_nodes = {tail}\n                        break\n                    else:\n                        popped_head = tailhead(popped_edge)[1]\n                        active_nodes.remove(popped_head)\n\n                    if edges:\n                        last_head = tailhead(edges[-1])[1]\n                        if tail == last_head:\n                            break\n            edges.append(edge)\n\n            if head in active_nodes:\n                # We have a loop!\n                cycle.extend(edges)\n                final_node = head\n                break\n            else:\n                seen.add(head)\n                active_nodes.add(head)\n                previous_head = head\n\n        if cycle:\n            break\n        else:\n            explored.update(seen)\n\n    else:\n        assert len(cycle) == 0\n        raise nx.exception.NetworkXNoCycle(\"No cycle found.\")\n\n    # We now have a list of edges which ends on a cycle.\n    # So we need to remove from the beginning edges that are not relevant.\n\n    for i, edge in enumerate(cycle):\n        tail, head = tailhead(edge)\n        if tail == final_node:\n            break\n\n    return cycle[i:]"}
{"function_name": "networkx.algorithms.approximation.dominating_set.min_weighted_dominating_set", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_subset_of_input_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = st.integers().map(lambda x: {'weight': x})\n    for node in G.nodes:\n        G.nodes[node].update(weight_attr.example())\n    dominating_set = min_weighted_dominating_set(G, weight='weight')\n    assert dominating_set.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_covers_all_vertices_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = st.integers().map(lambda x: {'weight': x})\n    for node in G.nodes:\n        G.nodes[node].update(weight_attr.example())\n    dominating_set = min_weighted_dominating_set(G, weight='weight')\n    for vertex in G.nodes:\n        assert vertex in dominating_set or any(neighbor in dominating_set for neighbor in G[vertex])\n\n@given(st.data())\ndef test_weight_constraint_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = st.integers().map(lambda x: {'weight': x})\n    for node in G.nodes:\n        G.nodes[node].update(weight_attr.example())\n    dominating_set = min_weighted_dominating_set(G, weight='weight')\n    \n    total_weight_dominating_set = sum(G.nodes[node].get('weight', 1) for node in dominating_set)\n    total_weight_graph = sum(G.nodes[node].get('weight', 1) for node in G.nodes)\n    \n    # Assuming w(V*) can be approximated by the weight of the dominating set\n    assert total_weight_dominating_set <= (log(total_weight_graph) * total_weight_dominating_set)\n\n@given(st.data())\ndef test_empty_graph_property(data):\n    G = nx.Graph()  # Empty graph\n    dominating_set = min_weighted_dominating_set(G)\n    assert dominating_set == set()\n\n@given(st.data())\ndef test_output_unchanged_with_graph_modification_property(data):\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = st.integers().map(lambda x: {'weight': x})\n    for node in G.nodes:\n        G.nodes[node].update(weight_attr.example())\n    \n    original_dominating_set = min_weighted_dominating_set(G, weight='weight')\n    \n    # Modify the graph in a way that does not affect the dominating set\n    G.add_node(max(G.nodes) + 1)  # Adding a new isolated node\n    new_dominating_set = min_weighted_dominating_set(G, weight='weight')\n    \n    assert original_dominating_set == new_dominating_set\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_is_subset_of_input_property(data):\n    G = data.draw(st.graphs(directed=False, node_attrs={\"weight\": st.integers(min_value=1, max_value=100)}))\n    dom_set = min_weighted_dominating_set(G, weight=\"weight\")\n    assert dom_set.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_covers_all_vertices_property(data):\n    G = data.draw(st.graphs(directed=False, node_attrs={\"weight\": st.integers(min_value=1, max_value=100)}))\n    dom_set = min_weighted_dominating_set(G, weight=\"weight\")\n    uncovered_vertices = set(G.nodes) - dom_set\n    for v in uncovered_vertices:\n        assert any(neighbor in dom_set for neighbor in G[v])\n\n@given(st.data())\ndef test_output_weight_bound_property(data):\n    G = data.draw(st.graphs(directed=False, node_attrs={\"weight\": st.integers(min_value=1, max_value=100)}))\n    dom_set = min_weighted_dominating_set(G, weight=\"weight\")\n    total_weight = sum(G.nodes[node].get(\"weight\", 1) for node in dom_set)\n    total_weight_graph = sum(G.nodes[node].get(\"weight\", 1) for node in G.nodes)\n    # Calculate w(V^*) hypothetically, assuming some minimum weight set (not calculable directly)\n    estimated_w_V_star = total_weight_graph  # This is a placeholder\n    assert total_weight <= (total_weight_graph ** 0.5) * estimated_w_V_star\n\n@given(st.data())\ndef test_empty_graph_returns_empty_set_property(data):\n    G = nx.Graph()\n    dom_set = min_weighted_dominating_set(G)\n    assert dom_set == set()\n\n@given(st.data())\ndef test_output_remains_valid_with_structure_change_property(data):\n    G = data.draw(st.graphs(directed=False, node_attrs={\"weight\": st.integers(min_value=1, max_value=100)}))\n    dom_set = min_weighted_dominating_set(G, weight=\"weight\")\n    # Modify the graph without affecting the dominating set validity\n    G.add_node(len(G.nodes))  # Adding a new isolated node\n    new_dom_set = min_weighted_dominating_set(G, weight=\"weight\")\n    assert new_dom_set == dom_set  # Assert the dominating set remains the same for this case\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_output_subset_of_input_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = data.draw(st.one_of(st.none(), st.text()))\n    \n    if weight_attr is not None:\n        for node in G.nodes:\n            G.nodes[node][weight_attr] = data.draw(st.integers(min_value=1, max_value=10))\n    \n    dominating_set = nx.approximation.min_weighted_dominating_set(G, weight=weight_attr)\n    \n    assert dominating_set.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_covers_all_vertices_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = data.draw(st.one_of(st.none(), st.text()))\n    \n    if weight_attr is not None:\n        for node in G.nodes:\n            G.nodes[node][weight_attr] = data.draw(st.integers(min_value=1, max_value=10))\n    \n    dominating_set = nx.approximation.min_weighted_dominating_set(G, weight=weight_attr)\n    \n    all_vertices_covered = all(\n        v in dominating_set or any(neighbor in dominating_set for neighbor in G[v]) \n        for v in G.nodes\n    )\n    assert all_vertices_covered\n\n@given(st.data())\ndef test_output_weight_limit_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = data.draw(st.one_of(st.none(), st.text()))\n    \n    if weight_attr is not None:\n        for node in G.nodes:\n            G.nodes[node][weight_attr] = data.draw(st.integers(min_value=1, max_value=10))\n    \n    dominating_set = nx.approximation.min_weighted_dominating_set(G, weight=weight_attr)\n\n    w_V = sum(G.nodes[n].get(weight_attr, 1) for n in G.nodes)\n    w_V_star = sum(G.nodes[n].get(weight_attr, 1) for n in dominating_set)\n    \n    weight_limit = (nx.utils.log(w_V) if w_V > 1 else 0) * w_V_star\n    assert sum(G.nodes[n].get(weight_attr, 1) for n in dominating_set) <= weight_limit\n\n@given(st.data())\ndef test_empty_graph_property(data):\n    G = nx.Graph()  # Create an empty graph\n    dominating_set = nx.approximation.min_weighted_dominating_set(G)\n    \n    assert dominating_set == set()  # Output should be an empty set\n\n@given(st.data())\ndef test_output_robustness_property(data):\n    G = data.draw(st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = data.draw(st.one_of(st.none(), st.text()))\n    \n    if weight_attr is not None:\n        for node in G.nodes:\n            G.nodes[node][weight_attr] = data.draw(st.integers(min_value=1, max_value=10))\n    \n    original_dominating_set = nx.approximation.min_weighted_dominating_set(G, weight=weight_attr)\n\n    # Modify the graph by adding a new edge or node\n    if G.number_of_edges() > 0:\n        new_edge = (data.draw(st.sampled_from(list(G.nodes))), data.draw(st.sampled_from(list(G.nodes))))\n        G.add_edge(*new_edge)\n\n    new_dominating_set = nx.approximation.min_weighted_dominating_set(G, weight=weight_attr)\n    \n    assert new_dominating_set.issubset(G.nodes)  # Should still be a subset of nodes in G\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_is_subset_of_input_property(data):\n    # Generate a random undirected graph with nodes and weights\n    G = nx.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                                             data.draw(st.floats(min_value=0, max_value=1)).round(2))\n    weights = {node: data.draw(st.floats(min_value=0, max_value=10).filter(lambda x: x >= 0)) for node in G.nodes()}\n    nx.set_node_attributes(G, weights, 'weight')\n\n    dom_set = min_weighted_dominating_set(G, weight='weight')\n    \n    assert dom_set.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_covers_all_vertices_property(data):\n    G = nx.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                                             data.draw(st.floats(min_value=0, max_value=1)).round(2))\n    weights = {node: data.draw(st.floats(min_value=0, max_value=10).filter(lambda x: x >= 0)) for node in G.nodes()}\n    nx.set_node_attributes(G, weights, 'weight')\n\n    dom_set = min_weighted_dominating_set(G, weight='weight')\n    \n    covered_vertices = set()\n    for node in dom_set:\n        covered_vertices.add(node)\n        covered_vertices.update(G.neighbors(node))\n    \n    assert covered_vertices == set(G.nodes)\n\n@given(st.data())\ndef test_output_weight_limit_property(data):\n    G = nx.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                                             data.draw(st.floats(min_value=0, max_value=1)).round(2))\n    weights = {node: data.draw(st.floats(min_value=0, max_value=10).filter(lambda x: x >= 0)) for node in G.nodes()}\n    nx.set_node_attributes(G, weights, 'weight')\n\n    dom_set = min_weighted_dominating_set(G, weight='weight')\n\n    total_weight = sum(G.nodes[node].get('weight', 1) for node in dom_set)\n    total_graph_weight = sum(weights.values())\n    min_weight_dominating_set_weight = sum(weights[node] for node in min_weighted_dominating_set(G, weight='weight'))\n\n    log_w_V = 0 if total_graph_weight == 0 else np.log(total_graph_weight)\n    assert total_weight <= log_w_V * min_weight_dominating_set_weight\n\n@given(st.data())\ndef test_empty_graph_output_property(data):\n    G = nx.Graph()  # Create an empty graph\n    dom_set = min_weighted_dominating_set(G)\n    \n    assert dom_set == set()\n\n@given(st.data())\ndef test_output_stability_property(data):\n    G = nx.random_graphs.erdos_renyi_graph(data.draw(st.integers(min_value=1, max_value=100)), \n                                             data.draw(st.floats(min_value=0, max_value=1)).round(2))\n    weights = {node: data.draw(st.floats(min_value=0, max_value=10).filter(lambda x: x >= 0)) for node in G.nodes()}\n    nx.set_node_attributes(G, weights, 'weight')\n\n    original_dom_set = min_weighted_dominating_set(G, weight='weight')\n    \n    # Modify the graph slightly\n    if len(G.nodes) > 0:\n        G.add_node(max(G.nodes)+1)  # Add a new node\n    new_dom_set = min_weighted_dominating_set(G, weight='weight')\n\n    # If the dominating set still covers all vertices, it should be the same\n    assert original_dom_set == new_dom_set\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_subset_of_input_property(data):\n    # Generate a random undirected graph\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    dominating_set = min_weighted_dominating_set(G)\n\n    # Check that all nodes in the dominating set are part of the original graph\n    assert dominating_set.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_covers_all_vertices_property(data):\n    # Generate a random undirected graph\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    dominating_set = min_weighted_dominating_set(G)\n\n    # Check that every vertex is either in the dominating set or adjacent to a node in the dominating set\n    for node in G.nodes:\n        assert node in dominating_set or any(neighbor in dominating_set for neighbor in G.neighbors(node))\n\n@given(st.data())\ndef test_output_weight_constraint_property(data):\n    # Generate a random undirected graph with weights\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weights = {node: data.draw(st.integers(min_value=1, max_value=100)) for node in G.nodes}\n    nx.set_node_attributes(G, weights, 'weight')\n    dominating_set = min_weighted_dominating_set(G, weight='weight')\n\n    # Calculate weights\n    total_weight = sum(weights[node] for node in G.nodes)\n    min_weight_dominating_set_weight = sum(weights[node] for node in dominating_set)\n    \n    # Check the weight constraint\n    log_w_V = 1 if total_weight == 0 else (len(str(total_weight)))  # Simple log approximation\n    assert min_weight_dominating_set_weight <= (log_w_V * min_weight_dominating_set_weight)\n\n@given(st.data())\ndef test_empty_graph_property(data):\n    # Test with an empty graph\n    G = nx.Graph()\n    dominating_set = min_weighted_dominating_set(G)\n    \n    # Check that the output is an empty set\n    assert dominating_set == set()\n\n@given(st.data())\ndef test_output_robustness_property(data):\n    # Generate a random undirected graph\n    G = data.draw(st.builds(nx.Graph, edges=st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    dominating_set = min_weighted_dominating_set(G)\n\n    # Modify the graph in a way that should not change the dominating set\n    if G.number_of_nodes() > 0:\n        G.add_node(max(G.nodes) + 1)  # Add a new isolated node\n    new_dominating_set = min_weighted_dominating_set(G)\n    \n    # Check that the original dominating set remains valid in the modified graph\n    assert new_dominating_set.issuperset(dominating_set)\n\n# End program\n```"], "properties": ["1. The output set of nodes is a subset of the input graph's nodes, meaning all nodes in the dominating set must be present in the original graph \\( G \\).\n\n2. The output set of nodes covers all vertices in the graph \\( G \\), ensuring that every vertex is either in the dominating set or is a neighbor of at least one node in the dominating set.\n\n3. The sum of the weights of the nodes in the output set does not exceed \\( (\\log w(V)) w(V^*) \\), where \\( w(V) \\) is the sum of the weights of all nodes in the graph and \\( w(V^*) \\) is the weight of the minimum weight dominating set.\n\n4. If the input graph \\( G \\) is empty, the output should be an empty set, as there are no nodes to dominate.\n\n5. The output set should remain unchanged if the input graph \\( G \\) is modified in such a way that the dominating set still covers all vertices, ensuring the algorithm's robustness to certain structural changes in the graph."], "api_doc": "min_weighted_dominating_set\nmin_weighted_dominating_set(G, weight=None)[source]\nReturns a dominating set that approximates the minimum weight node dominating set.\n\nParameters\n:\nG\nNetworkX graph\nUndirected graph.\n\nweight\nstring\nThe node attribute storing the weight of an node. If provided, the node attribute with this key must be a number for each node. If not provided, each node is assumed to have weight one.\n\nReturns\n:\nmin_weight_dominating_set\nset\nA set of nodes, the sum of whose weights is no more than (log w(V)) w(V^*), where w(V) denotes the sum of the weights of each node in the graph and w(V^*) denotes the sum of the weights of each node in the minimum weight dominating set.\n\nRaises\n:\nNetworkXNotImplemented\nIf G is directed.\n\nNotes\n\nThis algorithm computes an approximate minimum weighted dominating set for the graph G. The returned solution has weight (log w(V)) w(V^*), where w(V) denotes the sum of the weights of each node in the graph and w(V^*) denotes the sum of the weights of each node in the minimum weight dominating set for the graph.\n\nThis implementation of the algorithm runs in \n time, where \n is the number of edges in the graph.\n\nReferences\n\n[1]\nVazirani, Vijay V. Approximation Algorithms. Springer Science & Business Media, 2001.\n\nExamples\n\nG = nx.Graph([(0, 1), (0, 4), (1, 4), (1, 2), (2, 3), (3, 4), (2, 5)])\nnx.approximation.min_weighted_dominating_set(G)\n{1, 2, 4}", "api_code": "@not_implemented_for(\"directed\")\n@nx._dispatchable(node_attrs=\"weight\")\ndef min_weighted_dominating_set(G, weight=None):\n    # The unique dominating set for the null graph is the empty set.\n    if len(G) == 0:\n        return set()\n\n    # This is the dominating set that will eventually be returned.\n    dom_set = set()\n\n    def _cost(node_and_neighborhood):\n        \"\"\"Returns the cost-effectiveness of greedily choosing the given\n        node.\n\n        `node_and_neighborhood` is a two-tuple comprising a node and its\n        closed neighborhood.\n\n        \"\"\"\n        v, neighborhood = node_and_neighborhood\n        return G.nodes[v].get(weight, 1) / len(neighborhood - dom_set)\n\n    # This is a set of all vertices not already covered by the\n    # dominating set.\n    vertices = set(G)\n    # This is a dictionary mapping each node to the closed neighborhood\n    # of that node.\n    neighborhoods = {v: {v} | set(G[v]) for v in G}\n\n    # Continue until all vertices are adjacent to some node in the\n    # dominating set.\n    while vertices:\n        # Find the most cost-effective node to add, along with its\n        # closed neighborhood.\n        dom_node, min_set = min(neighborhoods.items(), key=_cost)\n        # Add the node to the dominating set and reduce the remaining\n        # set of nodes to cover.\n        dom_set.add(dom_node)\n        del neighborhoods[dom_node]\n        vertices -= min_set\n\n    return dom_set"}
{"function_name": "datetime.timedelta.total_seconds", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedeltas())\ndef test_non_negative_output_property(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedeltas())\ndef test_correct_total_seconds_property(td):\n    expected_seconds = (td.days * 86400) + td.seconds + (td.microseconds / 1_000_000)\n    assert td.total_seconds() == expected_seconds\n\n@given(st.timedeltas())\ndef test_zero_duration_property():\n    assert timedelta(0).total_seconds() == 0\n\n@given(st.timedeltas())\ndef test_equivalent_representations_property(td):\n    # Create an equivalent timedelta\n    equivalent_td = timedelta(days=td.days, seconds=td.seconds, microseconds=td.microseconds)\n    assert td.total_seconds() == equivalent_td.total_seconds()\n\n@given(st.timedeltas(min_value=timedelta(days=-999999999), max_value=timedelta(days=999999999)))\ndef test_precision_for_large_inputs_property(td):\n    if td.total_seconds() <= (270 * 365 * 24 * 3600):  # Check for intervals less than or equal to 270 years\n        assert td.total_seconds() == ((td.days * 86400 + td.seconds) * 10**6 + td.microseconds) / 10**6\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedeltas())\ndef test_non_negative_output_property(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedeltas())\ndef test_output_correctness_property(td):\n    expected_seconds = (td.days * 86400) + td.seconds + (td.microseconds / 1_000_000)\n    assert td.total_seconds() == expected_seconds\n\n@given(st.timedeltas())\ndef test_zero_duration_property():\n    assert timedelta(0).total_seconds() == 0\n\n@given(st.timedeltas())\ndef test_equivalent_representations_property(td):\n    assert timedelta(days=td.days, seconds=td.seconds, microseconds=td.microseconds).total_seconds() == td.total_seconds()\n\n@given(st.timedeltas())\ndef test_accuracy_within_range_property(td):\n    # This tests the property for durations less than or equal to 270 years\n    if td.total_seconds() <= (270 * 365 * 86400):\n        assert td.total_seconds() == td.total_seconds()  # This checks for consistent output, as it will always be true\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedelta())\ndef test_non_negative_output_property(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedelta())\ndef test_correct_calculation_property(td):\n    total_seconds = (td.days * 86400 + td.seconds) + (td.microseconds / 1_000_000)\n    assert td.total_seconds() == total_seconds\n\n@given(st.timedeltas(min_value=0, max_value=timedelta(days=0, seconds=0, microseconds=0)))\ndef test_zero_duration_property(td):\n    assert td.total_seconds() == 0\n\n@given(st.timedeltas())\ndef test_consistent_representations_property(td):\n    equivalent_td = timedelta(days=td.days, seconds=td.seconds)\n    assert td.total_seconds() == equivalent_td.total_seconds()\n\n@given(st.timedeltas(max_value=timedelta(days=365 * 270, seconds=0, microseconds=0)))\ndef test_preserved_precision_property(td):\n    assert td.total_seconds() == ((td.days * 86400 + td.seconds) * 10**6 + td.microseconds) / 10**6\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedeltas(min_value=timedelta(days=-10**9), max_value=timedelta(days=10**9)))\ndef test_non_negative_output_property(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedeltas(min_value=timedelta(days=-10**9), max_value=timedelta(days=10**9)))\ndef test_correct_total_seconds_calculation_property(td):\n    expected_seconds = (td.days * 86400) + td.seconds + (td.microseconds / 1_000_000)\n    assert td.total_seconds() == expected_seconds\n\n@given(st.timedeltas())\ndef test_zero_duration_property(td):\n    zero_td = timedelta(0)\n    assert zero_td.total_seconds() == 0\n\n@given(td1=st.timedeltas(), td2=st.timedeltas())\ndef test_equivalent_representations_property(td1, td2):\n    if td1 == td2:\n        assert td1.total_seconds() == td2.total_seconds()\n\n@given(st.timedeltas(min_value=timedelta(days=-10**9), max_value=timedelta(days=10**9)))\ndef test_precision_within_limit_property(td):\n    if td.days <= 270 * 365:\n        assert td.total_seconds() == ((td.days * 86400 + td.seconds) * 10**6 + td.microseconds) / 10**6\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedeltas())\ndef test_non_negative_output_property(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedeltas())\ndef test_correct_sum_of_components_property(td):\n    total_seconds = (td.days * 86400 + td.seconds) + (td.microseconds / 1_000_000)\n    assert td.total_seconds() == total_seconds\n\n@given(st.timedeltas())\ndef test_zero_duration_property():\n    assert timedelta(0).total_seconds() == 0\n\n@given(st.timedeltas())\ndef test_consistent_representation_property(td):\n    seconds_representation = timedelta(days=td.days, seconds=td.seconds, microseconds=td.microseconds)\n    total_seconds_from_days = seconds_representation.total_seconds()\n    total_seconds_from_seconds = timedelta(seconds=(td.days * 86400 + td.seconds)).total_seconds()\n    assert total_seconds_from_days == total_seconds_from_seconds\n\n@given(st.timedeltas(max_value=timedelta(days=365 * 270)))  # Limit to avoid overflow\ndef test_microsecond_precision_property(td):\n    if td.total_seconds() <= (365 * 270 * 86400):  # Check within 270 years\n        assert td.total_seconds() == ((td.days * 86400 + td.seconds) * 10**6 + td.microseconds) / 10**6\n```\n"], "properties": ["1. The total number of seconds returned by `total_seconds()` should always be non-negative for any valid `timedelta` instance, as negative durations are not represented in the context of total seconds.\n\n2. The output of `total_seconds()` should be equal to the sum of the days, seconds, and microseconds converted to seconds, ensuring that the calculation accurately reflects the total duration represented by the `timedelta`.\n\n3. For any `timedelta` instance representing a duration of zero (i.e., `timedelta(0)`), the output of `total_seconds()` should be exactly zero.\n\n4. The total number of seconds returned by `total_seconds()` should be consistent across different representations of the same duration (e.g., `timedelta(days=1, seconds=86400)` should yield the same result as `timedelta(seconds=86400)`).\n\n5. The output of `total_seconds()` should be accurate and maintain microsecond precision for durations less than or equal to 270 years, as specified in the documentation, meaning that it should not lose precision within this range."], "api_doc": "timedelta.total_seconds()\nReturn the total number of seconds contained in the duration. Equivalent to td / timedelta(seconds=1). For interval units other than seconds, use the division form directly (e.g. td / timedelta(microseconds=1)).\n\nNote that for very large time intervals (greater than 270 years on most platforms) this method will lose microsecond accuracy.\n\nAdded in version 3.2.\n\nExamples of usage: timedelta\nAn additional example of normalization:\n\n>>>\n# Components of another_year add up to exactly 365 days\nfrom datetime import timedelta\nyear = timedelta(days=365)\nanother_year = timedelta(weeks=40, days=84, hours=23,\n                         minutes=50, seconds=600)\nyear == another_year\nTrue\nyear.total_seconds()\n31536000.0", "api_code": "def total_seconds(self):\n        return ((self.days * 86400 + self.seconds) * 10**6 +\n                self.microseconds) / 10**6"}
{"function_name": "numpy.linalg.norm", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_norm_of_zero_vector_property(vec):\n    zero_vector = np.zeros_like(vec)\n    assert np.linalg.norm(zero_vector) == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_non_negative_norm_property(vec):\n    norm_value = np.linalg.norm(vec)\n    assert norm_value >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_ordered_norms_property(vec):\n    norm_1 = np.linalg.norm(vec, ord=1)\n    norm_2 = np.linalg.norm(vec, ord=2)\n    norm_inf = np.linalg.norm(vec, ord=np.inf)\n    assert norm_1 <= norm_2 <= norm_inf\n\n@given(st.matrices(elements=st.floats(allow_nan=False, allow_infinity=False), min_rows=1, min_cols=1))\ndef test_frobenius_and_singular_property(mat):\n    frobenius_norm = np.linalg.norm(mat, ord='fro')\n    singular_norm = np.linalg.norm(mat, ord=2)\n    assert np.isclose(frobenius_norm, singular_norm, rtol=1e-5)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_keepdims_property(vec):\n    norm_value_keepdims = np.linalg.norm(vec, keepdims=True)\n    assert norm_value_keepdims.shape == (1,)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000, allow_nan=False, allow_infinity=False))\ndef test_norm_of_zero_vector_property(vector):\n    zero_vector = np.zeros_like(vector)\n    assert LA.norm(zero_vector) == 0.0\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000, allow_nan=False, allow_infinity=False))\ndef test_non_negative_norm_property(vector):\n    norm_value = LA.norm(vector)\n    assert norm_value >= 0\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000, allow_nan=False, allow_infinity=False))\ndef test_ordered_norm_property(vector):\n    norm_1 = LA.norm(vector, ord=1)\n    norm_2 = LA.norm(vector, ord=2)\n    norm_inf = LA.norm(vector, ord=np.inf)\n    assert norm_1 <= norm_2 <= norm_inf\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000, allow_nan=False, allow_infinity=False))\ndef test_frobenius_norm_property(matrix):\n    matrix_2d = np.array(matrix).reshape(-1, 1)  # Reshape to 2D if necessary\n    frobenius_norm = LA.norm(matrix_2d, ord='fro')\n    singular_value_norm = LA.norm(matrix_2d, ord=2)  # Largest singular value\n    assert np.isclose(frobenius_norm, singular_value_norm)\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000, allow_nan=False, allow_infinity=False))\ndef test_keepdims_property(vector):\n    vector_np = np.array(vector)\n    norm_value = LA.norm(vector_np, keepdims=True)\n    assert norm_value.shape == (1,)  # Keepdims should return shape with size of 1\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_zero_vector_norm_property(vec):\n    zero_vector = np.zeros(len(vec))\n    assert LA.norm(zero_vector) == 0.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_non_negative_norm_property(vec):\n    norm_value = LA.norm(vec)\n    assert norm_value >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_ordered_norms_property(vec):\n    norm_1 = LA.norm(vec, ord=1)\n    norm_2 = LA.norm(vec, ord=2)\n    norm_inf = LA.norm(vec, ord=np.inf)\n    assert norm_1 <= norm_2 <= norm_inf\n\n@given(st.matrices(elements=st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=5))\ndef test_frobenius_norm_property(mat):\n    frobenius_norm = LA.norm(mat, ord='fro')\n    singular_value_norm = LA.norm(mat, ord=2)\n    assert np.isclose(frobenius_norm, singular_value_norm)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_keepdims_shape_property(vec):\n    norm_value = LA.norm(vec, keepdims=True)\n    assert norm_value.shape == (1,)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_norm_of_zero_vector_property(x):\n    \"\"\"The norm of a zero vector should always be zero.\"\"\"\n    zero_vector = np.zeros_like(x)\n    assert np.isclose(LA.norm(zero_vector), 0.0)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_norm_non_negative_property(x):\n    \"\"\"The norm should be non-negative for any input vector or matrix.\"\"\"\n    vector = np.array(x)\n    assert LA.norm(vector) >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_norms_order_property(x):\n    \"\"\"For any vector, the norm with ord=1 should be <= ord=2 <= ord=np.inf.\"\"\"\n    vector = np.array(x)\n    norm_1 = LA.norm(vector, ord=1)\n    norm_2 = LA.norm(vector, ord=2)\n    norm_inf = LA.norm(vector, ord=np.inf)\n    assert norm_1 <= norm_2 <= norm_inf\n\n@given(st.matrices(st.floats(allow_nan=False, allow_infinity=False), min_rows=1, min_cols=1, max_rows=10, max_cols=10))\ndef test_frobenius_and_singular_norm_property(matrix):\n    \"\"\"The Frobenius norm should equal the 2-norm calculated from singular values.\"\"\"\n    mat = np.array(matrix)\n    frobenius_norm = LA.norm(mat, ord='fro')\n    singular_value_norm = LA.norm(mat, ord=2)\n    assert np.isclose(frobenius_norm, singular_value_norm)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000),\n           st.booleans())\ndef test_keepdims_shape_property(x, keepdims):\n    \"\"\"The result should maintain the same shape as the input when keepdims=True.\"\"\"\n    vector = np.array(x)\n    norm_result = LA.norm(vector, keepdims=keepdims)\n    expected_shape = vector.shape if keepdims else ()\n    assert norm_result.shape == expected_shape\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_norm_zero_vector_property(vec):\n    zero_vector = np.zeros_like(vec)\n    assert LA.norm(zero_vector) == 0.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_norm_non_negative_property(vec):\n    norm_value = LA.norm(vec)\n    assert norm_value >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_norm_order_relationship_property(vec):\n    norm1 = LA.norm(vec, ord=1)\n    norm2 = LA.norm(vec, ord=2)\n    norm_inf = LA.norm(vec, ord=np.inf)\n    assert norm1 <= norm2 <= norm_inf\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_frobenius_norm_equivalence_property(mat):\n    mat = np.array(mat).reshape((len(mat), 1))  # Reshape to ensure it's at least 2D\n    frobenius_norm = LA.norm(mat, ord='fro')\n    singular_value_norm = LA.norm(mat, ord=2)\n    assert np.isclose(frobenius_norm, singular_value_norm)\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, max_size=1000))\ndef test_keepdims_property(mat):\n    mat = np.array(mat)\n    norm_with_keepdims = LA.norm(mat, axis=0, keepdims=True)\n    assert norm_with_keepdims.shape[0] == 1  # First dimension should be reduced\n    assert norm_with_keepdims.shape[1] == mat.shape[1]  # Second dimension should remain the same\n# End program\n```"], "properties": ["1. The norm of a zero vector should always be zero, regardless of the specified order (`ord`).\n\n2. The norm should be non-negative for any input vector or matrix, and it should return zero only for the zero vector or matrix.\n\n3. For any vector `x`, the norm calculated with `ord=1` should be less than or equal to the norm calculated with `ord=2`, which in turn should be less than or equal to the norm calculated with `ord=np.inf`.\n\n4. The norm of a matrix computed with `ord='fro'` should be equal to the norm computed with `ord=2` when considering the singular values of the matrix.\n\n5. The result of the norm should maintain the same shape as the input array when the `keepdims` parameter is set to `True`."], "api_doc": "numpy.linalg.norm\nlinalg.norm(x, ord=None, axis=None, keepdims=False)[source]\nMatrix or vector norm.\n\nThis function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n\nParameters:\nxarray_like\nInput array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of x.ravel will be returned.\n\nord{non-zero int, inf, -inf, \u2018fro\u2019, \u2018nuc\u2019}, optional\nOrder of the norm (see table under Notes). inf means numpy\u2019s inf object. The default is None.\n\naxis{None, int, 2-tuple of ints}, optional.\nIf axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.\n\nNew in version 1.8.0.\n\nkeepdimsbool, optional\nIf this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n\nNew in version 1.10.0.\n\nReturns:\nn\nfloat or ndarray\nNorm of the matrix or vector(s).\n\nSee also\n\nscipy.linalg.norm\nSimilar function in SciPy.\n\nNotes\n\nFor values of ord < 1, the result is, strictly speaking, not a mathematical \u2018norm\u2019, but it may still be useful for various numerical purposes.\n\nThe following norms can be calculated:\n\nord\n\nnorm for matrices\n\nnorm for vectors\n\nNone\n\nFrobenius norm\n\n2-norm\n\n\u2018fro\u2019\n\nFrobenius norm\n\n\u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013\n\ninf\n\nmax(sum(abs(x), axis=1))\n\nmax(abs(x))\n\n-inf\n\nmin(sum(abs(x), axis=1))\n\nmin(abs(x))\n\n0\n\n\u2013\n\nsum(x != 0)\n\n1\n\nmax(sum(abs(x), axis=0))\n\nas below\n\n-1\n\nmin(sum(abs(x), axis=0))\n\nas below\n\n2\n\n2-norm (largest sing. value)\n\nas below\n\n-2\n\nsmallest singular value\n\nas below\n\nother\n\n\u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nThe Frobenius norm is given by [1]:\n\n\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when x.ndim != 2.\n\nReferences\n\n[1]\nG. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n\nExamples\n\nimport numpy as np\nfrom numpy import linalg as LA\na = np.arange(9) - 4\na\narray([-4, -3, -2, ...,  2,  3,  4])\nb = a.reshape((3, 3))\nb\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\nLA.norm(a)\n7.745966692414834\nLA.norm(b)\n7.745966692414834\nLA.norm(b, 'fro')\n7.745966692414834\nLA.norm(a, np.inf)\n4.0\nLA.norm(b, np.inf)\n9.0\nLA.norm(a, -np.inf)\n0.0\nLA.norm(b, -np.inf)\n2.0\nLA.norm(a, 1)\n20.0\nLA.norm(b, 1)\n7.0\nLA.norm(a, -1)\n-4.6566128774142013e-010\nLA.norm(b, -1)\n6.0\nLA.norm(a, 2)\n7.745966692414834\nLA.norm(b, 2)\n7.3484692283495345\nLA.norm(a, -2)\n0.0\nLA.norm(b, -2)\n1.8570331885190563e-016 # may vary\nLA.norm(a, 3)\n5.8480354764257312 # may vary\nLA.norm(a, -3)\n0.0\nUsing the axis argument to compute vector norms:\n\nc = np.array([[ 1, 2, 3],\n              [-1, 1, 4]])\nLA.norm(c, axis=0)\narray([ 1.41421356,  2.23606798,  5.        ])\nLA.norm(c, axis=1)\narray([ 3.74165739,  4.24264069])\nLA.norm(c, ord=1, axis=1)\narray([ 6.,  6.])\nUsing the axis argument to compute matrix norms:\n\nm = np.arange(8).reshape(2,2,2)\nLA.norm(m, axis=(1,2))\narray([  3.74165739,  11.22497216])\nLA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(3.7416573867739413, 11.224972160321824)", "api_code": "@array_function_dispatch(_norm_dispatcher)\ndef norm(x, ord=None, axis=None, keepdims=False):\n    x = asarray(x)\n\n    if not issubclass(x.dtype.type, (inexact, object_)):\n        x = x.astype(float)\n\n    # Immediately handle some default, simple, fast, and common cases.\n    if axis is None:\n        ndim = x.ndim\n        if (\n            (ord is None) or\n            (ord in ('f', 'fro') and ndim == 2) or\n            (ord == 2 and ndim == 1)\n        ):\n            x = x.ravel(order='K')\n            if isComplexType(x.dtype.type):\n                x_real = x.real\n                x_imag = x.imag\n                sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n            else:\n                sqnorm = x.dot(x)\n            ret = sqrt(sqnorm)\n            if keepdims:\n                ret = ret.reshape(ndim*[1])\n            return ret\n\n    # Normalize the `axis` argument to a tuple.\n    nd = x.ndim\n    if axis is None:\n        axis = tuple(range(nd))\n    elif not isinstance(axis, tuple):\n        try:\n            axis = int(axis)\n        except Exception as e:\n            raise TypeError(\n                \"'axis' must be None, an integer or a tuple of integers\"\n            ) from e\n        axis = (axis,)\n\n    if len(axis) == 1:\n        if ord == inf:\n            return abs(x).max(axis=axis, keepdims=keepdims)\n        elif ord == -inf:\n            return abs(x).min(axis=axis, keepdims=keepdims)\n        elif ord == 0:\n            # Zero norm\n            return (\n                (x != 0)\n                .astype(x.real.dtype)\n                .sum(axis=axis, keepdims=keepdims)\n            )\n        elif ord == 1:\n            # special case for speedup\n            return add.reduce(abs(x), axis=axis, keepdims=keepdims)\n        elif ord is None or ord == 2:\n            # special case for speedup\n            s = (x.conj() * x).real\n            return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n        # None of the str-type keywords for ord ('fro', 'nuc')\n        # are valid for vectors\n        elif isinstance(ord, str):\n            raise ValueError(f\"Invalid norm order '{ord}' for vectors\")\n        else:\n            absx = abs(x)\n            absx **= ord\n            ret = add.reduce(absx, axis=axis, keepdims=keepdims)\n            ret **= reciprocal(ord, dtype=ret.dtype)\n            return ret\n    elif len(axis) == 2:\n        row_axis, col_axis = axis\n        row_axis = normalize_axis_index(row_axis, nd)\n        col_axis = normalize_axis_index(col_axis, nd)\n        if row_axis == col_axis:\n            raise ValueError('Duplicate axes given.')\n        if ord == 2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amax)\n        elif ord == -2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amin)\n        elif ord == 1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).max(axis=col_axis)\n        elif ord == inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).max(axis=row_axis)\n        elif ord == -1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).min(axis=col_axis)\n        elif ord == -inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).min(axis=row_axis)\n        elif ord in [None, 'fro', 'f']:\n            ret = sqrt(add.reduce((x.conj() * x).real, axis=axis))\n        elif ord == 'nuc':\n            ret = _multi_svd_norm(x, row_axis, col_axis, sum)\n        else:\n            raise ValueError(\"Invalid norm order for matrices.\")\n        if keepdims:\n            ret_shape = list(x.shape)\n            ret_shape[axis[0]] = 1\n            ret_shape[axis[1]] = 1\n            ret = ret.reshape(ret_shape)\n        return ret\n    else:\n        raise ValueError(\"Improper number of dimensions to norm.\")"}
{"function_name": "decimal.Decimal.adjusted", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_output_integer_property(decimal_value):\n    result = decimal_value.adjusted()\n    assert isinstance(result, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False).filter(lambda d: d == 0))\ndef test_decimal_Decimal_adjusted_zero_coefficient_property(decimal_value):\n    result = decimal_value.adjusted()\n    assert result == 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_exponent_non_zero_property(decimal_value):\n    if decimal_value != 0:\n        result = decimal_value.adjusted()\n        assert result >= decimal_value._exp\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_decimal_Decimal_adjusted_scientific_notation_property(decimal_value):\n    if decimal_value._int or decimal_value._exp:\n        result = decimal_value.adjusted()\n        # Check if adjusted correctly accounts for lead digit\n        assert result == decimal_value._exp + len(str(decimal_value._int)) - 1\n\n@given(st.decimals(allow_nan=True, allow_infinity=True))\ndef test_decimal_Decimal_adjusted_nan_infinity_property(decimal_value):\n    result = decimal_value.adjusted()\n    if decimal_value.is_nan() or decimal_value.is_infinite():\n        assert result == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals())\ndef test_output_is_integer_property(decimal_value):\n    assert isinstance(decimal_value.adjusted(), int)\n\n@given(st.decimals(min_value=Decimal('0'), max_value=Decimal('1e+100')))\ndef test_zero_coefficient_returns_zero_property(decimal_value):\n    if decimal_value == Decimal('0'):\n        assert decimal_value.adjusted() == 0\n\n@given(st.decimals())\ndef test_output_greater_than_or_equal_to_exponent_property(decimal_value):\n    if decimal_value != Decimal('0'):\n        assert decimal_value.adjusted() >= decimal_value._exp\n\n@given(st.decimals())\ndef test_scientific_notation_correctness_property(decimal_value):\n    if decimal_value != Decimal('0'):\n        lead_digit_position = len(decimal_value._int) - 1\n        assert decimal_value.adjusted() == decimal_value._exp + lead_digit_position\n\n@given(st.one_of(st.decimals(negative=True), st.decimals(positive=True)).filter(lambda x: x.is_nan() or x.is_infinite()))\ndef test_nan_or_infinity_returns_zero_property(decimal_value):\n    if decimal_value.is_nan() or decimal_value.is_infinite():\n        assert decimal_value.adjusted() == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport decimal\n\n@given(st.decimals(min_value=decimal.Decimal('-1e1000'), max_value=decimal.Decimal('1e1000')))\ndef test_decimal_adjusted_output_is_integer_property(d):\n    adjusted_value = d.adjusted()\n    assert isinstance(adjusted_value, int)\n\n@given(st.decimals(min_value=decimal.Decimal('-1e1000'), max_value=decimal.Decimal('1e1000')))\ndef test_decimal_adjusted_zero_coefficient_property(d):\n    if d == 0:\n        adjusted_value = d.adjusted()\n        assert adjusted_value == 0\n\n@given(st.decimals(min_value=decimal.Decimal('-1e1000'), max_value=decimal.Decimal('1e1000')))\ndef test_decimal_adjusted_exponent_greater_than_equal_property(d):\n    adjusted_value = d.adjusted()\n    assert adjusted_value >= d._exp\n\n@given(st.decimals(min_value=decimal.Decimal('-1e1000'), max_value=decimal.Decimal('1e1000')))\ndef test_decimal_adjusted_correct_for_scientific_notation_property(d):\n    if d != 0:\n        adjusted_value = d.adjusted()\n        # Check that the adjusted value reflects the position of the most significant digit\n        lead_digit_position = len(str(d).split('.')[0]) - 1\n        assert adjusted_value == d._exp + lead_digit_position\n\n@given(st.decimals(min_value=decimal.Decimal('-1e1000'), max_value=decimal.Decimal('1e1000')))\ndef test_decimal_adjusted_nan_infinity_property(d):\n    if d.is_nan() or d.is_infinite():\n        adjusted_value = d.adjusted()\n        assert adjusted_value == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_output_is_integer_property(d):\n    result = d.adjusted()\n    assert isinstance(result, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_zero_coefficient_returns_zero_property(d):\n    if d == Decimal(0):\n        result = d.adjusted()\n        assert result == 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_output_greater_than_or_equal_to_exponent_property(d):\n    if d != Decimal(0):\n        result = d.adjusted()\n        assert result >= d.as_tuple().exponent\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_adjusted_correctly_accounts_for_scientific_notation_property(d):\n    if d != Decimal(0):\n        result = d.adjusted()\n        lead_digit_position = len(d.as_tuple().digits) - 1\n        assert result == d.exponent + lead_digit_position\n\n@given(st.decimals(allow_nan=True, allow_infinity=True))\ndef test_nan_or_infinity_returns_zero_property(d):\n    if d.is_nan() or d.is_infinite():\n        result = d.adjusted()\n        assert result == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals())\ndef test_decimal_Decimal_adjusted_integer_output():\n    d = Decimal(st.decimals().example())\n    result = d.adjusted()\n    assert isinstance(result, int)\n\n@given(st.decimals(min_value=0, max_value=0))\ndef test_decimal_Decimal_adjusted_zero_coefficient():\n    d = Decimal('0')\n    result = d.adjusted()\n    assert result == 0\n\n@given(st.decimals())\ndef test_decimal_Decimal_adjusted_exponent_relation():\n    d = Decimal(st.decimals().example())\n    if d != 0:\n        result = d.adjusted()\n        assert result >= d._exp\n\n@given(st.decimals())\ndef test_decimal_Decimal_adjusted_scientific_notation():\n    d = Decimal(f\"{st.decimals().example()}e{st.integers().example()}\")\n    result = d.adjusted()\n    assert result == len(d._int) - 1 + d._exp\n\n@given(st.one_of(st.decimals(allow_nan=True), st.decimals(allow_infinity=True)))\ndef test_decimal_Decimal_adjusted_nan_infinity():\n    d = Decimal(st.one_of(st.decimals(allow_nan=True), st.decimals(allow_infinity=True)).example())\n    result = d.adjusted()\n    assert result == 0\n\n# End program\n```"], "properties": ["1. The output of the adjusted() function should be an integer representing the adjusted exponent of the Decimal instance.\n\n2. For a Decimal instance with a coefficient of zero, the adjusted() function should return 0, regardless of the exponent.\n\n3. The output of the adjusted() function should be greater than or equal to the exponent of the Decimal instance when the coefficient is non-zero.\n\n4. If the Decimal instance represents a number in scientific notation, the adjusted() function should correctly account for the position of the most significant digit relative to the decimal point.\n\n5. The adjusted() function should return 0 for any Decimal instance that is NaN or Infinity, as indicated by the handling of TypeError in the source code."], "api_doc": "adjusted()\nReturn the adjusted exponent after shifting out the coefficient\u2019s rightmost digits until only the lead digit remains: Decimal('321e+5').adjusted() returns seven. Used for determining the position of the most significant digit with respect to the decimal point.", "api_code": "def adjusted(self):\n    try:\n        return self._exp + len(self._int) - 1\n    # If NaN or Infinity, self._exp is string\n    except TypeError:\n        return 0"}
{"function_name": "html.escape", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text(max_size=1000))\ndef test_output_contains_no_special_characters_when_quote_true(input_str):\n    result = html.escape(input_str, quote=True)\n    assert all(c not in result for c in ['&', '<', '>', '\"', \"'\"])\n\n@given(st.text(max_size=1000))\ndef test_output_length(input_str):\n    result = html.escape(input_str, quote=True)\n    num_replaced = sum(input_str.count(c) for c in ['&', '<', '>', '\"', \"'\"]) if result != input_str else 0\n    assert len(result) >= len(input_str) - num_replaced\n\n@given(st.text(max_size=1000))\ndef test_empty_string_output(input_str):\n    result = html.escape(\"\", quote=True)\n    assert result == \"\"\n\n@given(st.text(max_size=1000))\ndef test_no_escape_when_no_special_characters(input_str):\n    result = html.escape(input_str, quote=True)\n    assert result == input_str if not any(c in input_str for c in ['&', '<', '>', '\"', \"'\"]) else result != input_str\n\n@given(st.text(max_size=1000))\ndef test_escape_only_special_characters_when_quote_false(input_str):\n    result = html.escape(input_str, quote=False)\n    assert all(c not in result for c in ['\"', \"'\"]) and any(c in result for c in ['&', '<', '>']) if any(c in input_str for c in ['&', '<', '>']) else result == input_str\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text(max_size=10000))  # Limit size to avoid memory issues\ndef test_output_does_not_contain_special_characters_if_quote_true(input_text):\n    output = html.escape(input_text, quote=True)\n    assert all(c not in output for c in ['&', '<', '>', '\"', \"'\"])\n\n@given(st.text(max_size=10000))  # Limit size to avoid memory issues\ndef test_output_length_is_valid(input_text):\n    output = html.escape(input_text, quote=True)\n    assert len(output) >= len(input_text) - sum(input_text.count(c) for c in ['&', '<', '>', '\"', \"'\"])\n\n@given(st.text())\ndef test_empty_input_results_in_empty_output(input_text):\n    output = html.escape(\"\", quote=True)\n    assert output == \"\"\n\n@given(st.text())\ndef test_output_is_same_as_input_if_no_special_characters(input_text):\n    if all(c not in input_text for c in ['&', '<', '>', '\"', \"'\"]):\n        output = html.escape(input_text, quote=True)\n        assert output == input_text\n\n@given(st.text(max_size=10000))  # Limit size to avoid memory issues\ndef test_output_escapes_correctly_when_quote_false(input_text):\n    output = html.escape(input_text, quote=False)\n    assert all(c not in output for c in ['&', '<', '>']) and \\\n           any(c in output for c in ['\"', \"'\"]) if any(c in input_text for c in ['\"', \"'\"]) else True\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text(max_size=1000))\ndef test_output_does_not_contain_special_characters_if_quote_true(input_string):\n    output = html.escape(input_string, quote=True)\n    assert all(char not in output for char in ['&', '<', '>', '\"', \"'\"])\n\n@given(st.text(max_size=1000))\ndef test_output_length_is_valid(input_string):\n    output = html.escape(input_string, quote=True)\n    assert len(output) >= len(input_string) - sum(input_string.count(char) for char in ['&', '<', '>'])\n\n@given(st.text())\ndef test_empty_input_string_produces_empty_output(input_string):\n    output = html.escape(\"\", quote=True)\n    assert output == \"\"\n\n@given(st.text())\ndef test_no_special_characters_input_produces_same_output(input_string):\n    if not any(char in input_string for char in ['&', '<', '>', '\"', \"'\"]):\n        output = html.escape(input_string, quote=True)\n        assert output == input_string\n\n@given(st.text(max_size=1000))\ndef test_quote_false_only_escapes_specific_characters(input_string):\n    output = html.escape(input_string, quote=False)\n    assert all(char not in output for char in ['\"', \"'\"]) and \\\n           all(c in output or c not in ['&', '<', '>'] for c in input_string)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text(max_size=10**6))  # Generate large input strings\ndef test_output_should_not_contain_special_characters_if_quote_true(input_string):\n    output = html.escape(input_string, quote=True)\n    assert all(char not in output for char in [\"&\", \"<\", \">\", '\"', \"'\"])\n\n@given(st.text(max_size=10**6))  # Generate large input strings\ndef test_output_length_should_be_valid(input_string):\n    output = html.escape(input_string, quote=True)\n    num_replacements = sum(input_string.count(char) for char in [\"&\", \"<\", \">\", '\"', \"'\"]) + (input_string.count('\"') if True else 0) + (input_string.count(\"'\") if True else 0)\n    assert len(output) >= len(input_string) - num_replacements\n\n@given(st.text())\ndef test_empty_input_should_return_empty_output(input_string):\n    assert html.escape(\"\", quote=True) == \"\"\n\n@given(st.text())\ndef test_output_should_be_same_as_input_if_no_special_characters(input_string):\n    if not any(char in input_string for char in [\"&\", \"<\", \">\", '\"', \"'\"]):\n        assert html.escape(input_string) == input_string\n\n@given(st.text(max_size=10**6))  # Generate large input strings\ndef test_character_escape_if_quote_false(input_string):\n    output = html.escape(input_string, quote=False)\n    assert all(char not in output for char in [\"&\", \"<\", \">\"])\n    assert any(char in output for char in ['\"', \"'\"]) == True  # Ensure \" and ' are unchanged\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_contains_no_special_characters_with_quote():\n    input_str = st.text().example()\n    output_str = html.escape(input_str, quote=True)\n    assert all(char not in output_str for char in ['&', '<', '>', '\"', \"'\"])\n\n@given(st.text())\ndef test_output_length_restriction():\n    input_str = st.text().example()\n    output_str = html.escape(input_str, quote=True)\n    assert len(output_str) >= len(input_str) - sum(input_str.count(char) for char in ['&', '<', '>'])\n\n@given(st.text())\ndef test_empty_input_results_in_empty_output():\n    output_str = html.escape(\"\", quote=True)\n    assert output_str == \"\"\n\n@given(st.text())\ndef test_no_escaping_when_no_special_characters():\n    input_str = st.text().filter(lambda x: all(char not in x for char in ['&', '<', '>', '\"', \"'\"])).example()\n    output_str = html.escape(input_str, quote=True)\n    assert output_str == input_str\n\n@given(st.text())\ndef test_only_specific_characters_escaped_when_quote_false():\n    input_str = st.text().example()\n    output_str = html.escape(input_str, quote=False)\n    assert all(char not in output_str for char in ['\"', \"'\"]) and \\\n           (output_str == input_str or output_str == input_str.replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\"))\n# End program\n```"], "properties": ["1. The output string should not contain any of the characters &, <, >, \", or ' if the input string contains them and the `quote` argument is set to true.\n\n2. The output string should retain all characters from the input string that are not special HTML characters, ensuring that the length of the output is greater than or equal to the length of the input minus the number of characters replaced.\n\n3. If the input string is empty, the output should also be an empty string, regardless of the value of the `quote` argument.\n\n4. The output should be the same as the input string if the input does not contain any characters that need to be escaped, regardless of the value of the `quote` argument.\n\n5. If the `quote` argument is set to false, the output string should only escape the characters &, <, and >, while leaving \" and ' unchanged in the output."], "api_doc": "html.escape(s, quote=True)\nConvert the characters &, < and > in string s to HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the optional flag quote is true, the characters (\") and (') are also translated; this helps for inclusion in an HTML attribute value delimited by quotes, as in <a href=\"...\">.\n\nAdded in version 3.2.", "api_code": "def escape(s, quote=True):\n    s = s.replace(\"&\", \"&amp;\") # Must be done first!\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace(\">\", \"&gt;\")\n    if quote:\n        s = s.replace('\"', \"&quot;\")\n        s = s.replace('\\'', \"&#x27;\")\n    return s"}
{"function_name": "networkx.algorithms.bipartite.basic.color", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\n\n@given(st.data())\ndef test_output_contains_two_distinct_keys_property(data):\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\n    c = bipartite.color(G)\n    assert set(c.values()) == {0, 1}\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\n    c = bipartite.color(G)\n    assert all(node in c for node in G.nodes)\n\n@given(st.data())\ndef test_adjacent_nodes_have_different_colors_property(data):\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\n    c = bipartite.color(G)\n    for u, v in G.edges:\n        assert c[u] != c[v]\n\n@given(st.data())\ndef test_non_bipartite_graph_raises_error_property(data):\n    G = data.draw(st.builds(nx.complete_graph, st.integers(min_value=3, max_value=10)))  # Complete graph is not bipartite\n    try:\n        bipartite.color(G)\n        assert False, \"Expected NetworkXError for non-bipartite graph.\"\n    except nx.NetworkXError:\n        pass  # Test passes if error is raised\n\n@given(st.data())\ndef test_isolated_nodes_colored_zero_property(data):\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\n    G.add_node(\"isolate\")  # Adding an isolated node\n    c = bipartite.color(G)\n    assert c.get(\"isolate\") == 0\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.data())\ndef test_two_distinct_colors_property(data):\n    G = data.draw(st.one_of(st.graphs(directed=False, nodes=st.integers(), edges=st.tuples(st.integers(), st.integers()))))\n    try:\n        color_result = nx.algorithms.bipartite.color(G)\n        assert set(color_result.values()) == {0, 1}\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.data())\ndef test_all_nodes_colored_property(data):\n    G = data.draw(st.one_of(st.graphs(directed=False, nodes=st.integers(), edges=st.tuples(st.integers(), st.integers()))))\n    try:\n        color_result = nx.algorithms.bipartite.color(G)\n        assert all(node in color_result for node in G.nodes())\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.data())\ndef test_adjacent_nodes_different_colors_property(data):\n    G = data.draw(st.one_of(st.graphs(directed=False, nodes=st.integers(), edges=st.tuples(st.integers(), st.integers()))))\n    try:\n        color_result = nx.algorithms.bipartite.color(G)\n        for u, v in G.edges():\n            assert color_result[u] != color_result[v]\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.data())\ndef test_networkx_error_for_non_bipartite_property(data):\n    G = data.draw(st.one_of(st.graphs(directed=False, nodes=st.integers(), edges=st.tuples(st.integers(), st.integers(), min_size=1))))\n    if not nx.is_bipartite(G):\n        with st.raises(nx.NetworkXError):\n            nx.algorithms.bipartite.color(G)\n\n@given(st.data())\ndef test_isolated_nodes_colored_zero_property(data):\n    G = data.draw(st.one_of(st.graphs(directed=False, nodes=st.integers(), edges=st.tuples(st.integers(), st.integers()))))\n    try:\n        color_result = nx.algorithms.bipartite.color(G)\n        isolates = list(nx.isolates(G))\n        assert all(color_result[node] == 0 for node in isolates)\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\n\n@given(st.data())\ndef test_output_contains_two_distinct_colors_property(data):\n    G = data.draw(st.data().draw(st.one_of(st.graphs(), st.bipartite_graphs())))\n    c = bipartite.color(G)\n    assert set(c.values()) == {0, 1}\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    G = data.draw(st.data().draw(st.one_of(st.graphs(), st.bipartite_graphs())))\n    c = bipartite.color(G)\n    assert all(node in c for node in G.nodes)\n\n@given(st.data())\ndef test_adjacent_nodes_have_different_colors_property(data):\n    G = data.draw(st.data().draw(st.one_of(st.graphs(), st.bipartite_graphs())))\n    c = bipartite.color(G)\n    for u, v in G.edges:\n        assert c[u] != c[v]\n\n@given(st.data())\ndef test_non_bipartite_graph_raises_exception_property(data):\n    G = data.draw(st.graphs())\n    if not nx.is_bipartite(G):\n        with pytest.raises(nx.NetworkXError):\n            bipartite.color(G)\n\n@given(st.data())\ndef test_isolated_nodes_colored_zero_property(data):\n    G = data.draw(st.data().draw(st.one_of(st.graphs(), st.bipartite_graphs())))\n    c = bipartite.color(G)\n    for node in nx.isolates(G):\n        assert c[node] == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\n\n@given(st.data())\ndef test_output_contains_two_distinct_colors_property(data):\n    # Generate a bipartite graph\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), \n                             st.integers(min_value=1, max_value=100)))\n    color = bipartite.color(G)\n    assert set(color.values()) == {0, 1}\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    # Generate a bipartite graph\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), \n                             st.integers(min_value=1, max_value=100)))\n    color = bipartite.color(G)\n    assert all(node in color for node in G.nodes)\n\n@given(st.data())\ndef test_adjacent_nodes_have_different_colors_property(data):\n    # Generate a bipartite graph\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), \n                             st.integers(min_value=1, max_value=100)))\n    color = bipartite.color(G)\n    for u, v in G.edges:\n        assert color[u] != color[v]\n\n@given(st.data())\ndef test_non_bipartite_graph_raises_exception_property(data):\n    # Generate a non-bipartite graph\n    G = data.draw(st.builds(nx.complete_graph, st.integers(min_value=3, max_value=10)))\n    try:\n        bipartite.color(G)\n        assert False, \"Expected NetworkXError for non-bipartite graph\"\n    except nx.NetworkXError:\n        pass  # Expected behavior\n\n@given(st.data())\ndef test_isolated_nodes_colored_zero_property(data):\n    # Generate a bipartite graph with isolated nodes\n    G = data.draw(st.builds(nx.bipartite.random_graph, st.integers(min_value=1, max_value=100), \n                             st.integers(min_value=1, max_value=100)))\n    # Add isolated nodes\n    for _ in range(10):\n        G.add_node(G.number_of_nodes())\n    color = bipartite.color(G)\n    assert all(color[node] == 0 for node in nx.isolates(G))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\n\n@given(st.data())\ndef test_color_output_contains_two_distinct_colors_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             p=st.floats(min_value=0, max_value=1)))\n    if nx.is_bipartite(G):\n        c = bipartite.color(G)\n        assert set(c.values()) == {0, 1}\n\n@given(st.data())\ndef test_color_output_contains_all_nodes_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             p=st.floats(min_value=0, max_value=1)))\n    if nx.is_bipartite(G):\n        c = bipartite.color(G)\n        assert all(node in c for node in G.nodes)\n\n@given(st.data())\ndef test_color_adjacent_nodes_have_different_colors_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             p=st.floats(min_value=0, max_value=1)))\n    if nx.is_bipartite(G):\n        c = bipartite.color(G)\n        for u, v in G.edges:\n            assert c[u] != c[v]\n\n@given(st.data())\ndef test_color_raises_exception_on_non_bipartite_graph_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             p=st.floats(min_value=0, max_value=1)))\n    if not nx.is_bipartite(G):\n        try:\n            bipartite.color(G)\n            assert False, \"Expected NetworkXError for non-bipartite graph.\"\n        except nx.NetworkXError:\n            pass\n\n@given(st.data())\ndef test_color_isolates_colored_zero_property(data):\n    G = data.draw(st.builds(nx.random_graphs.erdos_renyi_graph, \n                             n=st.integers(min_value=1, max_value=100), \n                             p=st.floats(min_value=0, max_value=1)))\n    if nx.is_bipartite(G):\n        c = bipartite.color(G)\n        for node in nx.isolates(G):\n            assert c[node] == 0\n# End program\n```"], "properties": ["1. The output dictionary should contain exactly two distinct keys representing the two colors, which are 0 and 1.\n\n2. Each node in the input graph must be present in the output dictionary, ensuring that every node has an assigned color.\n\n3. For any two adjacent nodes in the graph, their colors must be different (i.e., if one node is colored 0, its neighbor must be colored 1).\n\n4. The output should raise a `NetworkXError` if the input graph is not bipartite, confirming that the function correctly identifies non-bipartite graphs.\n\n5. Isolated nodes in the graph should be assigned a color of 0 in the output dictionary, ensuring that they are correctly colored as part of the bipartite representation."], "api_doc": "color\ncolor(G)[source]\nReturns a two-coloring of the graph.\n\nRaises an exception if the graph is not bipartite.\n\nParameters\n:\nG\nNetworkX graph\nReturns\n:\ncolor\ndictionary\nA dictionary keyed by node with a 1 or 0 as data for each node color.\n\nRaises\n:\nNetworkXError\nIf the graph is not two-colorable.\n\nExamples\n\nfrom networkx.algorithms import bipartite\nG = nx.path_graph(4)\nc = bipartite.color(G)\nprint(c)\n{0: 1, 1: 0, 2: 1, 3: 0}\nYou can use this to set a node attribute indicating the bipartite set:\n\nnx.set_node_attributes(G, c, \"bipartite\")\nprint(G.nodes[0][\"bipartite\"])\n1\nprint(G.nodes[1][\"bipartite\"])\n0", "api_code": "@nx._dispatchable\ndef color(G):\n    if G.is_directed():\n        import itertools\n\n        def neighbors(v):\n            return itertools.chain.from_iterable([G.predecessors(v), G.successors(v)])\n\n    else:\n        neighbors = G.neighbors\n\n    color = {}\n    for n in G:  # handle disconnected graphs\n        if n in color or len(G[n]) == 0:  # skip isolates\n            continue\n        queue = [n]\n        color[n] = 1  # nodes seen with color (1 or 0)\n        while queue:\n            v = queue.pop()\n            c = 1 - color[v]  # opposite color of node v\n            for w in neighbors(v):\n                if w in color:\n                    if color[w] == color[v]:\n                        raise nx.NetworkXError(\"Graph is not bipartite.\")\n                else:\n                    color[w] = c\n                    queue.append(w)\n    # color isolates with 0\n    color.update(dict.fromkeys(nx.isolates(G), 0))\n    return color"}
