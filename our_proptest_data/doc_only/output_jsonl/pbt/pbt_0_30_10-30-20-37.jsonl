{"function_name": "numpy.resize", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_total_elements_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)\n    resized = np.resize(a, new_shape)\n    assert resized.size == new_shape[0] * new_shape[1]\n# End program\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_repeated_elements_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)\n    resized = np.resize(a, new_shape)\n    if resized.size > a.size:\n        repeated_elements = (a * (resized.size // a.size + 1))[:resized.size]\n        assert np.array_equal(resized, repeated_elements)\n# End program\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_data_type_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)\n    resized = np.resize(a, new_shape)\n    assert resized.dtype == a.dtype\n# End program\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_c_order_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)\n    resized = np.resize(a, new_shape)\n    flat_a = a.flatten(order='C')\n    assert np.array_equal(resized.flatten(order='C'), flat_a[:resized.size])\n# End program\n\n@given(st.lists(st.integers(), min_size=1), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_truncated_elements_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)\n    if np.prod(new_shape) < a.size:  # Only test if we are truncating\n        resized = np.resize(a, new_shape)\n        assert np.array_equal(resized, a.flatten()[:resized.size])\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(a=st.lists(st.integers()), new_shape=st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_total_number_of_elements_property(a, new_shape):\n    array_a = np.array(a).reshape(-1)  # Flatten the input to ensure it's 1D\n    reshaped_array = np.resize(array_a, new_shape)\n    assert reshaped_array.size == np.prod(new_shape)\n# End program\n\n@given(a=st.lists(st.integers()), new_shape=st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_repeated_elements_property(a, new_shape):\n    array_a = np.array(a).reshape(-1)  # Flatten the input to ensure it's 1D\n    reshaped_array = np.resize(array_a, new_shape)\n    expected_size = np.prod(new_shape)\n    \n    if expected_size > array_a.size:\n        repeated_elements = (array_a.tolist() * ((expected_size // array_a.size) + 1))[:expected_size]\n        assert np.array_equal(reshaped_array, np.array(repeated_elements).reshape(new_shape))\n# End program\n\n@given(a=st.lists(st.integers()), new_shape=st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_data_type_property(a, new_shape):\n    array_a = np.array(a).reshape(-1)  # Flatten the input to ensure it's 1D\n    reshaped_array = np.resize(array_a, new_shape)\n    assert reshaped_array.dtype == array_a.dtype\n# End program\n\n@given(a=st.lists(st.integers()), new_shape=st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_c_order_filling_property(a, new_shape):\n    array_a = np.array(a).reshape(-1)  # Flatten the input to ensure it's 1D\n    reshaped_array = np.resize(array_a, new_shape)\n    \n    filled_elements = []\n    for i in range(np.prod(new_shape)):\n        filled_elements.append(array_a[i % array_a.size])\n    \n    assert np.array_equal(reshaped_array.flatten(), np.array(filled_elements))\n# End program\n\n@given(a=st.lists(st.integers()), new_shape=st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_truncation_property(a, new_shape):\n    array_a = np.array(a).reshape(-1)  # Flatten the input to ensure it's 1D\n    reshaped_array = np.resize(array_a, new_shape)\n    \n    if np.prod(new_shape) < array_a.size:\n        assert reshaped_array.size == np.prod(new_shape)\n        assert np.array_equal(reshaped_array.flatten(), array_a.flatten()[:reshaped_array.size])\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), \n       st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_total_number_of_elements_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)  # Convert to 1D array\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.size == np.prod(new_shape)\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), \n       st.tuples(st.integers(min_value=1), st.integers(min_value=1, max_value=10)))\ndef test_repeated_copies_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)  # Convert to 1D array\n    resized_array = np.resize(a, new_shape)\n    if resized_array.size > a.size:\n        expected_array = np.tile(a, (1, (resized_array.size + a.size - 1) // a.size))[:resized_array.size]\n        assert np.array_equal(resized_array, expected_array)\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), \n       st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_data_type_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)  # Convert to 1D array\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.dtype == a.dtype\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), \n       st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_c_order_filling_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)  # Convert to 1D array\n    resized_array = np.resize(a, new_shape)\n    expected_array = np.tile(a, (1, (resized_array.size + a.size - 1) // a.size))[:resized_array.size]\n    assert np.array_equal(resized_array, expected_array)\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), \n       st.tuples(st.integers(min_value=1), st.integers(min_value=1, max_value=10)))\ndef test_truncated_output_property(arr, new_shape):\n    a = np.array(arr).reshape(-1)  # Convert to 1D array\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.size <= a.size\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), st.tuples(st.integers(min_value=1, max_value=10), st.integers(min_value=1, max_value=10)))\ndef test_total_elements_property(data, new_shape):\n    a = np.array(data).reshape(-1)\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.size == np.prod(new_shape)\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), st.tuples(st.integers(min_value=1), st.integers(min_value=1, max_value=10)))\ndef test_repeated_copies_property(data, new_shape):\n    a = np.array(data).reshape(-1)\n    resized_array = np.resize(a, new_shape)\n    if resized_array.size > a.size:\n        assert all(resized_array[i] == a[i % a.size] for i in range(resized_array.size))\n\n@given(st.lists(st.integers(), min_size=1, max_size=100))\ndef test_data_type_property(data):\n    a = np.array(data)\n    resized_array = np.resize(a, (2, 3))\n    assert resized_array.dtype == a.dtype\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), st.tuples(st.integers(min_value=1, max_value=10), st.integers(min_value=1, max_value=10)))\ndef test_c_order_filling_property(data, new_shape):\n    a = np.array(data).reshape(-1)\n    resized_array = np.resize(a, new_shape)\n    flat_a = a.flatten(order='C')\n    assert np.all(resized_array.flatten(order='C')[:len(flat_a)] == flat_a)\n\n@given(st.lists(st.integers(), min_size=1, max_size=100), st.tuples(st.integers(min_value=1, max_value=10), st.integers(min_value=1, max_value=10)))\ndef test_truncation_property(data, new_shape):\n    a = np.array(data).reshape(-1)\n    resized_array = np.resize(a, new_shape)\n    if resized_array.size < a.size:\n        assert np.array_equal(resized_array, a[:resized_array.size])\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\n\n@given(st.lists(st.lists(st.integers())), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_total_elements_property(data, new_shape):\n    a = np.array(data)\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.size == np.prod(new_shape)\n\n@given(st.lists(st.lists(st.integers())), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_repeated_elements_property(data, new_shape):\n    a = np.array(data)\n    resized_array = np.resize(a, new_shape)\n    if a.size < np.prod(new_shape):\n        expected_elements = (a.flatten() * (np.prod(new_shape) // a.size + 1))[:np.prod(new_shape)]\n        assert np.array_equal(resized_array.flatten(), expected_elements)\n\n@given(st.lists(st.lists(st.integers())), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_data_type_property(data, new_shape):\n    a = np.array(data)\n    resized_array = np.resize(a, new_shape)\n    assert resized_array.dtype == a.dtype\n\n@given(st.lists(st.lists(st.integers())), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_c_order_filling_property(data, new_shape):\n    a = np.array(data)\n    resized_array = np.resize(a, new_shape)\n    assert np.array_equal(resized_array.flatten(), np.resize(a.flatten(), np.prod(new_shape)))\n\n@given(st.lists(st.lists(st.integers())), st.tuples(st.integers(min_value=1), st.integers(min_value=1)))\ndef test_truncation_property(data, new_shape):\n    a = np.array(data)\n    if a.size > np.prod(new_shape):\n        resized_array = np.resize(a, new_shape)\n        assert resized_array.size == np.prod(new_shape)\n        assert np.array_equal(resized_array.flatten(), a.flatten()[:np.prod(new_shape)])\n# End program\n```"], "properties": ["1. The total number of elements in the output array must equal the product of the dimensions specified in `new_shape`, regardless of the input array's size.\n\n2. If the `new_shape` specifies a size larger than the original array, the output array should contain repeated copies of the original array's elements, filling the additional space as necessary.\n\n3. The output array must maintain the same data type as the input array.\n\n4. The output array should be filled with elements in C-order, meaning that the elements are filled row-wise from the original array.\n\n5. If the `new_shape` specifies a size smaller than the original array, the output array should contain only the first elements of the original array, truncated to fit the new size."], "api_doc": "numpy.resize\nnumpy.resize(a, new_shape)[source]\nReturn a new array with the specified shape.\n\nIf the new array is larger than the original array, then the new array is filled with repeated copies of a. Note that this behavior is different from a.resize(new_shape) which fills with zeros instead of repeated copies of a.\n\nParameters\n:\na\narray_like\nArray to be resized.\n\nnew_shape\nint or tuple of int\nShape of resized array.\n\nReturns\n:\nreshaped_array\nndarray\nThe new array is formed from the data in the old array, repeated if necessary to fill out the required number of elements. The data are repeated iterating over the array in C-order.\n\nSee also\n\nnumpy.reshape\nReshape an array without changing the total size.\n\nnumpy.pad\nEnlarge and pad an array.\n\nnumpy.repeat\nRepeat elements of an array.\n\nndarray.resize\nresize an array in-place.\n\nNotes\n\nWhen the total size of the array does not change reshape should be used. In most other cases either indexing (to reduce the size) or padding (to increase the size) may be a more appropriate solution.\n\nWarning: This functionality does not consider axes separately, i.e. it does not apply interpolation/extrapolation. It fills the return array with the required number of elements, iterating over a in C-order, disregarding axes (and cycling back from the start if the new shape is larger). This functionality is therefore not suitable to resize images, or data where each axis represents a separate and distinct entity.\n\nExamples\n\nimport numpy as np\na = np.array([[0,1],[2,3]])\nnp.resize(a,(2,3))\narray([[0, 1, 2],\n       [3, 0, 1]])\nnp.resize(a,(1,4))\narray([[0, 1, 2, 3]])\nnp.resize(a,(2,4))\narray([[0, 1, 2, 3],\n       [0, 1, 2, 3]])", "api_code": "@array_function_dispatch(_resize_dispatcher)\ndef resize(a, new_shape):\n    if isinstance(new_shape, (int, nt.integer)):\n        new_shape = (new_shape,)\n\n    a = ravel(a)\n\n    new_size = 1\n    for dim_length in new_shape:\n        new_size *= dim_length\n        if dim_length < 0:\n            raise ValueError(\n                'all elements of `new_shape` must be non-negative'\n            )\n\n    if a.size == 0 or new_size == 0:\n        # First case must zero fill. The second would have repeats == 0.\n        return np.zeros_like(a, shape=new_shape)\n\n    repeats = -(-new_size // a.size)  # ceil division\n    a = concatenate((a,) * repeats)[:new_size]\n\n    return reshape(a, new_shape)"}
{"function_name": "networkx.algorithms.clique.find_cliques", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.lists(st.integers()), st.lists(st.integers()))\ndef test_output_cliques_are_subsets_of_nodes_property(graph_nodes, clique_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    if clique_nodes:\n        G.add_edges_from(st.sampled_from([(u, v) for u in clique_nodes for v in clique_nodes if u != v]).example())\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        assert all(node in graph_nodes for node in clique)\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_contain_at_least_one_node_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    if graph_nodes:\n        G.add_edges_from(st.sampled_from([(u, v) for u in graph_nodes for v in graph_nodes if u != v]).example())\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        assert len(clique) >= 1\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_are_maximal_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    if graph_nodes:\n        G.add_edges_from(st.sampled_from([(u, v) for u in graph_nodes for v in graph_nodes if u != v]).example())\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        for node in graph_nodes:\n            if node not in clique and G.has_edge(clique[0], node):\n                assert False  # The presence of an edge means the clique is not maximal\n\n@given(st.lists(st.integers()), st.lists(st.integers()))\ndef test_output_cliques_include_all_input_nodes_property(graph_nodes, clique_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    if clique_nodes:\n        G.add_edges_from(st.sampled_from([(u, v) for u in clique_nodes for v in clique_nodes if u != v]).example())\n    cliques = list(nx.find_cliques(G, nodes=clique_nodes))\n    for clique in cliques:\n        assert all(node in clique for node in clique_nodes)\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_are_unique_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    if graph_nodes:\n        G.add_edges_from(st.sampled_from([(u, v) for u in graph_nodes for v in graph_nodes if u != v]).example())\n    cliques = list(nx.find_cliques(G))\n    unique_cliques = set(tuple(sorted(clique)) for clique in cliques)\n    assert len(unique_cliques) == len(cliques)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom hypothesis import example\n\n@given(st.lists(st.integers()), st.lists(st.integers(), max_size=10))\ndef test_output_cliques_are_subsets_of_graph_nodes_property(graph_nodes, clique_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        assert all(node in graph_nodes for node in clique)\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_contain_at_least_one_node_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    cliques = list(nx.find_cliques(G))\n    if cliques:  # Check only if there are cliques found\n        assert all(len(clique) > 0 for clique in cliques)\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_are_maximal_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    for i in range(len(graph_nodes)):\n        for j in range(i + 1, len(graph_nodes)):\n            G.add_edge(graph_nodes[i], graph_nodes[j])  # Make it a complete graph\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        for node in graph_nodes:\n            if node not in clique:\n                assert not any(node in c for c in nx.find_cliques(G) if set(clique).issubset(c))\n\n@given(st.lists(st.integers()), st.lists(st.integers(), max_size=10))\ndef test_cliques_include_specified_nodes_property(graph_nodes, specified_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    for i in range(len(specified_nodes)):\n        for j in range(i + 1, len(specified_nodes)):\n            G.add_edge(specified_nodes[i], specified_nodes[j])  # Make it a complete subgraph\n    cliques = list(nx.find_cliques(G, nodes=specified_nodes))\n    for clique in cliques:\n        assert all(node in clique for node in specified_nodes)\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_are_unique_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    cliques = list(nx.find_cliques(G))\n    assert len(cliques) == len(set(map(tuple, cliques)))  # Convert to tuples to compare uniqueness\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.clique import find_cliques\n\n@given(st.data())\ndef test_find_cliques_subsets_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1, max_size=100)))\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        assert all(node in G.nodes for node in clique)\n\n@given(st.data())\ndef test_find_cliques_non_empty_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1, max_size=100)))\n    cliques = list(find_cliques(G))\n    if G.nodes:\n        assert all(len(clique) > 0 for clique in cliques)\n\n@given(st.data())\ndef test_find_cliques_maximal_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1, max_size=100)))\n    cliques = list(find_cliques(G))\n    for clique in cliques:\n        for node in G.nodes:\n            if node not in clique and all(neighbor in clique for neighbor in G.neighbors(node)):\n                assert False  # If we can add a node without breaking the clique property\n\n@given(st.data())\ndef test_find_cliques_include_nodes_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1, max_size=100)))\n    nodes = data.draw(st.lists(st.sampled_from(G.nodes), min_size=1, max_size=5))\n    if nx.is_clique(G, nodes):\n        cliques = list(find_cliques(G, nodes=nodes))\n        assert all(all(node in clique for node in nodes) for clique in cliques)\n\n@given(st.data())\ndef test_find_cliques_unique_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1, max_size=100)))\n    cliques = list(find_cliques(G))\n    assert len(cliques) == len(set(map(tuple, cliques)))  # Check for uniqueness of cliques\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.clique import find_cliques\n\n@given(st.data())\ndef test_find_cliques_subsets_property(data):\n    # Generate a random undirected graph\n    G = nx.generators.random_graphs.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    cliques = list(find_cliques(G))\n    \n    # Check that all cliques are subsets of nodes in G\n    for clique in cliques:\n        assert all(node in G.nodes for node in clique)\n\n@given(st.data())\ndef test_find_cliques_non_empty_property(data):\n    # Generate a random undirected graph\n    G = nx.generators.random_graphs.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    cliques = list(find_cliques(G))\n    \n    # Check that each clique contains at least one node if G is non-empty\n    if G.number_of_nodes() > 0:\n        assert all(len(clique) > 0 for clique in cliques)\n\n@given(st.data())\ndef test_find_cliques_maximal_property(data):\n    # Generate a random undirected graph\n    G = nx.generators.random_graphs.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    cliques = list(find_cliques(G))\n    \n    # Check that each clique is maximal\n    for clique in cliques:\n        for node in G.nodes:\n            if node not in clique and all(neighbor in clique for neighbor in G.neighbors(node)):\n                assert False  # Found a node that can be added to the clique, so it's not maximal\n\n@given(st.data())\ndef test_find_cliques_includes_specified_nodes_property(data):\n    # Generate a random undirected graph\n    G = nx.generators.random_graphs.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    nodes = data.draw(st.lists(st.sampled_from(G.nodes()), min_size=1, max_size=len(G.nodes())))\n    \n    if nx.is_clique(G.subgraph(nodes)):\n        cliques = list(find_cliques(G, nodes=nodes))\n        \n        # Check that all returned cliques include the specified nodes\n        for clique in cliques:\n            assert all(node in clique for node in nodes)\n\n@given(st.data())\ndef test_find_cliques_unique_cliques_property(data):\n    # Generate a random undirected graph\n    G = nx.generators.random_graphs.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=100)), 0.5)\n    cliques = list(find_cliques(G))\n    \n    # Check that there are no duplicate cliques\n    unique_cliques = {tuple(sorted(clique)) for clique in cliques}\n    assert len(unique_cliques) == len(cliques)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\n\n@given(st.lists(st.integers()), st.lists(st.integers(), unique=True))\ndef test_output_cliques_are_subsets_of_nodes_property(graph_nodes, clique_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    G.add_edges_from([(u, v) for u in graph_nodes for v in graph_nodes if u != v])\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        assert all(node in G.nodes for node in clique)\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_contain_at_least_one_node_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    G.add_edges_from([(u, v) for u in graph_nodes for v in graph_nodes if u != v])\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        assert len(clique) > 0\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_are_maximal_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    G.add_edges_from([(u, v) for u in graph_nodes for v in graph_nodes if u != v])\n    cliques = list(nx.find_cliques(G))\n    for clique in cliques:\n        for node in G.nodes:\n            if node not in clique and all(node in G.neighbors(n) for n in clique):\n                assert False\n\n@given(st.lists(st.integers()), st.lists(st.integers(), unique=True))\ndef test_cliques_include_all_provided_nodes_property(graph_nodes, specific_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    G.add_edges_from([(u, v) for u in graph_nodes for v in graph_nodes if u != v])\n    if set(specific_nodes).issubset(set(graph_nodes)):\n        cliques = list(nx.find_cliques(G, nodes=specific_nodes))\n        for clique in cliques:\n            assert all(node in clique for node in specific_nodes)\n\n@given(st.lists(st.integers()))\ndef test_output_cliques_are_unique_property(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    G.add_edges_from([(u, v) for u in graph_nodes for v in graph_nodes if u != v])\n    cliques = list(nx.find_cliques(G))\n    unique_cliques = set(tuple(sorted(clique)) for clique in cliques)\n    assert len(unique_cliques) == len(cliques)\n# End program\n```"], "properties": ["1. The output cliques must be subsets of the nodes present in the input graph \\( G \\).\n\n2. Each clique in the output must contain at least one node if the input graph \\( G \\) is non-empty.\n\n3. The output cliques must be maximal, meaning that no additional nodes from the graph \\( G \\) can be added to any clique without losing the property of being a complete subgraph.\n\n4. If a list of nodes is provided as input, every clique returned in the output must include all of those nodes.\n\n5. The output must not contain duplicate cliques; each maximal clique should be unique in the result."], "api_doc": "find_cliques\nfind_cliques(G, nodes=None)[source]\nReturns all maximal cliques in an undirected graph.\n\nFor each node n, a maximal clique for n is a largest complete subgraph containing n. The largest maximal clique is sometimes called the maximum clique.\n\nThis function returns an iterator over cliques, each of which is a list of nodes. It is an iterative implementation, so should not suffer from recursion depth issues.\n\nThis function accepts a list of nodes and only the maximal cliques containing all of these nodes are returned. It can considerably speed up the running time if some specific cliques are desired.\n\nParameters\n:\nG\nNetworkX graph\nAn undirected graph.\n\nnodes\nlist, optional (default=None)\nIf provided, only yield maximal cliques containing all nodes in nodes. If nodes isn\u2019t a clique itself, a ValueError is raised.\n\nReturns\n:\niterator\nAn iterator over maximal cliques, each of which is a list of nodes in G. If nodes is provided, only the maximal cliques containing all the nodes in nodes are returned. The order of cliques is arbitrary.\n\nRaises\n:\nValueError\nIf nodes is not a clique.\n\nSee also\n\nfind_cliques_recursive\nA recursive version of the same algorithm.\n\nNotes\n\nTo obtain a list of all maximal cliques, use list(find_cliques(G)). However, be aware that in the worst-case, the length of this list can be exponential in the number of nodes in the graph. This function avoids storing all cliques in memory by only keeping current candidate node lists in memory during its search.\n\nThis implementation is based on the algorithm published by Bron and Kerbosch (1973) [1], as adapted by Tomita, Tanaka and Takahashi (2006) [2] and discussed in Cazals and Karande (2008) [3]. It essentially unrolls the recursion used in the references to avoid issues of recursion stack depth (for a recursive implementation, see find_cliques_recursive()).\n\nThis algorithm ignores self-loops and parallel edges, since cliques are not conventionally defined with such edges.\n\nReferences\n\n[1]\nBron, C. and Kerbosch, J. \u201cAlgorithm 457: finding all cliques of an undirected graph\u201d. Communications of the ACM 16, 9 (Sep. 1973), 575\u2013577. <http://portal.acm.org/citation.cfm?doid=362342.362367>\n\n[2]\nEtsuji Tomita, Akira Tanaka, Haruhisa Takahashi, \u201cThe worst-case time complexity for generating all maximal cliques and computational experiments\u201d, Theoretical Computer Science, Volume 363, Issue 1, Computing and Combinatorics, 10th Annual International Conference on Computing and Combinatorics (COCOON 2004), 25 October 2006, Pages 28\u201342 <https://doi.org/10.1016/j.tcs.2006.06.015>\n\n[3]\nF. Cazals, C. Karande, \u201cA note on the problem of reporting maximal cliques\u201d, Theoretical Computer Science, Volume 407, Issues 1\u20133, 6 November 2008, Pages 564\u2013568, <https://doi.org/10.1016/j.tcs.2008.05.010>\n\nExamples\n\nfrom pprint import pprint  # For nice dict formatting\nG = nx.karate_club_graph()\nsum(1 for c in nx.find_cliques(G))  # The number of maximal cliques in G\n36\nmax(nx.find_cliques(G), key=len)  # The largest maximal clique in G\n[0, 1, 2, 3, 13]\nThe size of the largest maximal clique is known as the clique number of the graph, which can be found directly with:\n\nmax(len(c) for c in nx.find_cliques(G))\n5\nOne can also compute the number of maximal cliques in G that contain a given node. The following produces a dictionary keyed by node whose values are the number of maximal cliques in G that contain the node:\n\npprint({n: sum(1 for c in nx.find_cliques(G) if n in c) for n in G})\n{0: 13,\n 1: 6,\n 2: 7,\n 3: 3,\n 4: 2,\n 5: 3,\n 6: 3,\n 7: 1,\n 8: 3,\n 9: 2,\n 10: 2,\n 11: 1,\n 12: 1,\n 13: 2,\n 14: 1,\n 15: 1,\n 16: 1,\n 17: 1,\n 18: 1,\n 19: 2,\n 20: 1,\n 21: 1,\n 22: 1,\n 23: 3,\n 24: 2,\n 25: 2,\n 26: 1,\n 27: 3,\n 28: 2,\n 29: 2,\n 30: 2,\n 31: 4,\n 32: 9,\n 33: 14}\nOr, similarly, the maximal cliques in G that contain a given node. For example, the 4 maximal cliques that contain node 31:\n\n[c for c in nx.find_cliques(G) if 31 in c]\n[[0, 31], [33, 32, 31], [33, 28, 31], [24, 25, 31]]", "api_code": "@not_implemented_for(\"directed\")\n@nx._dispatchable\ndef find_cliques(G, nodes=None):\n    if len(G) == 0:\n        return\n\n    adj = {u: {v for v in G[u] if v != u} for u in G}\n\n    # Initialize Q with the given nodes and subg, cand with their nbrs\n    Q = nodes[:] if nodes is not None else []\n    cand = set(G)\n    for node in Q:\n        if node not in cand:\n            raise ValueError(f\"The given `nodes` {nodes} do not form a clique\")\n        cand &= adj[node]\n\n    if not cand:\n        yield Q[:]\n        return\n\n    subg = cand.copy()\n    stack = []\n    Q.append(None)\n\n    u = max(subg, key=lambda u: len(cand & adj[u]))\n    ext_u = cand - adj[u]\n\n    try:\n        while True:\n            if ext_u:\n                q = ext_u.pop()\n                cand.remove(q)\n                Q[-1] = q\n                adj_q = adj[q]\n                subg_q = subg & adj_q\n                if not subg_q:\n                    yield Q[:]\n                else:\n                    cand_q = cand & adj_q\n                    if cand_q:\n                        stack.append((subg, cand, ext_u))\n                        Q.append(None)\n                        subg = subg_q\n                        cand = cand_q\n                        u = max(subg, key=lambda u: len(cand & adj[u]))\n                        ext_u = cand - adj[u]\n            else:\n                Q.pop()\n                subg, cand, ext_u = stack.pop()\n    except IndexError:\n        pass"}
{"function_name": "statistics.correlation", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1))\ndef test_statistics_correlation_output_range():\n    x = st.lists(st.floats(), min_size=2).example()\n    y = st.lists(st.floats(), min_size=2).example()\n    result = statistics.correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(), min_size=2, unique=True))\ndef test_statistics_correlation_identical_inputs():\n    identical_list = st.lists(st.floats()).example()\n    result = statistics.correlation(identical_list, identical_list)\n    assert result == 1.0\n\n@given(st.lists(st.floats(), min_size=2).map(lambda x: [x[0]] * len(x)))\ndef test_statistics_correlation_constant_input():\n    constant_list = st.lists(st.floats(), min_size=2).map(lambda x: [x[0]] * len(x)).example()\n    try:\n        statistics.correlation(constant_list, constant_list)\n        assert False, \"Expected StatisticsError\"\n    except statistics.StatisticsError:\n        pass\n\n@given(st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1))\ndef test_statistics_correlation_ranked_perfect_alignment():\n    x = st.lists(st.floats(), min_size=2, unique=True).example()\n    y = sorted(x)  # Ensures a perfect monotonic relationship\n    result = statistics.correlation(x, y, method='ranked')\n    assert result == 1.0\n\n@given(st.lists(st.floats(), min_size=2, unique=True))\ndef test_statistics_correlation_consistency():\n    x = st.lists(st.floats(), min_size=2).example()\n    y = st.lists(st.floats(), min_size=2).example()\n    result1 = statistics.correlation(x, y)\n    result2 = statistics.correlation(x, y)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import correlation, StatisticsError\n\n@given(st.lists(st.floats(), min_size=2, unique=True), \n       st.lists(st.floats(), min_size=2, unique=True))\ndef test_output_within_bounds_property(x, y):\n    result = correlation(x, y)\n    assert -1.0 <= result <= 1.0\n\n@given(st.lists(st.floats(), min_size=2, unique=True))\ndef test_identical_lists_property(x):\n    result = correlation(x, x)\n    assert result == 1.0\n\n@given(st.lists(st.floats(), min_size=2))\ndef test_constant_list_property(x):\n    if len(set(x)) == 1:  # All elements are the same\n        try:\n            correlation(x, [1.0] * len(x))\n            assert False  # Should not reach here\n        except StatisticsError:\n            assert True  # Expected behavior\n\n@given(st.lists(st.floats(), min_size=2), \n       st.lists(st.floats(), min_size=2))\ndef test_ranked_monotonic_relationship_property(x, y):\n    result = correlation(x, y, method='ranked')\n    if sorted(x) == sorted(y):  # Perfectly aligned ranks\n        assert result == 1.0\n\n@given(st.lists(st.floats(), min_size=2), \n       st.lists(st.floats(), min_size=2))\ndef test_consistency_of_output_property(x, y):\n    result1 = correlation(x, y)\n    result2 = correlation(x, y)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1),\n                st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1))\ndef test_statistics_correlation_output_range_property(x, y):\n    result = statistics.correlation(x, y)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(), min_size=2).filter(lambda x: len(set(x)) == 1))\ndef test_statistics_correlation_identical_inputs_property(x):\n    result = statistics.correlation(x, x)\n    assert result == 1.0\n\n@given(st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1))\ndef test_statistics_correlation_constant_input_property(x):\n    with pytest.raises(statistics.StatisticsError):\n        statistics.correlation(x, [1] * len(x))\n\n@given(st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1),\n                st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1))\ndef test_statistics_correlation_ranked_perfectly_aligned_property(x, y):\n    result = statistics.correlation(x, y, method='ranked')\n    assert result == 1.0\n\n@given(st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1),\n                st.lists(st.floats(), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1))\ndef test_statistics_correlation_consistency_property(x, y):\n    result1 = statistics.correlation(x, y)\n    result2 = statistics.correlation(x, y)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000))\ndef test_statistics_correlation_output_range_property(data):\n    result = statistics.correlation(data, data)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000))\ndef test_statistics_correlation_identical_inputs_property(data):\n    if len(set(data)) > 1:  # Ensure not constant\n        result = statistics.correlation(data, data)\n        assert result == 1.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000))\ndef test_statistics_correlation_constant_input_property(data):\n    if len(set(data)) == 1:  # Constant input\n        try:\n            statistics.correlation(data, data)\n            assert False  # Should raise an exception\n        except statistics.StatisticsError:\n            pass  # Expected behavior\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000))\ndef test_statistics_correlation_ranked_perfect_property(data):\n    if len(set(data)) > 1:  # Ensure not constant\n        result = statistics.correlation(data, sorted(data), method='ranked')\n        assert result == 1.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=1000))\ndef test_statistics_correlation_consistency_property(data):\n    result1 = statistics.correlation(data, data)\n    result2 = statistics.correlation(data, data)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1))\ndef test_statistics_correlation_output_range_property(data):\n    result = statistics.correlation(data, data)\n    assert -1 <= result <= 1\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1))\ndef test_statistics_correlation_identical_lists_property(data):\n    result = statistics.correlation(data, data)\n    assert result == 1.0\n\n@given(st.lists(st.floats(min_value=1e10, max_value=1e10), min_size=2))\ndef test_statistics_correlation_constant_list_property(data):\n    with pytest.raises(statistics.StatisticsError):\n        statistics.correlation(data, data)\n\n@given(st.lists(st.integers(), min_size=2), st.lists(st.integers(), min_size=2))\ndef test_statistics_correlation_ranked_monotonic_property(list1, list2):\n    result = statistics.correlation(list1, list2, method='ranked')\n    # For simplicity, assuming here that if both lists are sorted, the correlation should be 1.0\n    if sorted(list1) == list1 and sorted(list2) == list2:\n        assert result == 1.0\n    elif sorted(list1, reverse=True) == list1 and sorted(list2, reverse=True) == list2:\n        assert result == 1.0\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, unique=True))\ndef test_statistics_correlation_consistency_property(data):\n    result1 = statistics.correlation(data, data)\n    result2 = statistics.correlation(data, data)\n    assert result1 == result2\n# End program\n```"], "properties": ["1. The output correlation coefficient must always be a floating-point number between -1 and +1, inclusive.\n\n2. If both input lists are identical (and not constant), the output correlation coefficient must be 1.0, indicating a perfect positive correlation.\n\n3. If one of the input lists is constant (e.g., all elements are the same), the function should raise a StatisticsError.\n\n4. If the method is set to 'ranked', the output correlation coefficient must reflect the strength of a monotonic relationship, and it should be equal to 1.0 if the ranks of the two inputs are perfectly aligned.\n\n5. The output correlation coefficient must remain consistent (i.e., produce the same value) for the same pair of input lists across multiple calls, regardless of any additional factors."], "api_doc": "statistics.correlation(x, y, /, *, method='linear')\nReturn the Pearson\u2019s correlation coefficient for two inputs. Pearson\u2019s correlation coefficient r takes values between -1 and +1. It measures the strength and direction of a linear relationship.\n\nIf method is \u201cranked\u201d, computes Spearman\u2019s rank correlation coefficient for two inputs. The data is replaced by ranks. Ties are averaged so that equal values receive the same rank. The resulting coefficient measures the strength of a monotonic relationship.\n\nSpearman\u2019s correlation coefficient is appropriate for ordinal data or for continuous data that doesn\u2019t meet the linear proportion requirement for Pearson\u2019s correlation coefficient.\n\nBoth inputs must be of the same length (no less than two), and need not to be constant, otherwise StatisticsError is raised.\n\nExample with Kepler\u2019s laws of planetary motion:\n\n>>>\n# Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and  Neptune\norbital_period = [88, 225, 365, 687, 4331, 10_756, 30_687, 60_190]    # days\ndist_from_sun = [58, 108, 150, 228, 778, 1_400, 2_900, 4_500] # million km\n\n# Show that a perfect monotonic relationship exists\ncorrelation(orbital_period, dist_from_sun, method='ranked')\n1.0\n\n# Observe that a linear relationship is imperfect\nround(correlation(orbital_period, dist_from_sun), 4)\n0.9882\n\n# Demonstrate Kepler's third law: There is a linear correlation\n# between the square of the orbital period and the cube of the\n# distance from the sun.\nperiod_squared = [p * p for p in orbital_period]\ndist_cubed = [d * d * d for d in dist_from_sun]\nround(correlation(period_squared, dist_cubed), 4)\n1.0\nAdded in version 3.10.\n\nChanged in version 3.12: Added support for Spearman\u2019s rank correlation coefficient.", "api_code": "def correlation(x, y, /, *, method='linear'):\n    n = len(x)\n    if len(y) != n:\n        raise StatisticsError('correlation requires that both inputs have same number of data points')\n    if n < 2:\n        raise StatisticsError('correlation requires at least two data points')\n    if method not in {'linear', 'ranked'}:\n        raise ValueError(f'Unknown method: {method!r}')\n    if method == 'ranked':\n        start = (n - 1) / -2            # Center rankings around zero\n        x = _rank(x, start=start)\n        y = _rank(y, start=start)\n    else:\n        xbar = fsum(x) / n\n        ybar = fsum(y) / n\n        x = [xi - xbar for xi in x]\n        y = [yi - ybar for yi in y]\n    sxy = sumprod(x, y)\n    sxx = sumprod(x, x)\n    syy = sumprod(y, y)\n    try:\n        return sxy / _sqrtprod(sxx, syy)\n    except ZeroDivisionError:\n        raise StatisticsError('at least one of the inputs is constant')"}
{"function_name": "networkx.algorithms.approximation.clustering_coefficient.average_clustering", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.data())\ndef test_average_clustering_coefficient_range_property():\n    G = st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=0)).example()\n    result = approximation.average_clustering(G, trials=1000)\n    assert 0 <= result <= 1\n\n@given(st.data())\ndef test_average_clustering_no_edges_property():\n    G = nx.Graph()\n    result = approximation.average_clustering(G, trials=1000)\n    assert result == 0\n\n@given(st.data())\ndef test_average_clustering_consistency_property():\n    G = nx.erdos_renyi_graph(10, 0.5)\n    result1 = approximation.average_clustering(G, trials=1000)\n    result2 = approximation.average_clustering(G, trials=1000)\n    assert result1 == result2\n\n@given(st.data())\ndef test_average_clustering_empty_graph_property():\n    G = nx.Graph()\n    result = approximation.average_clustering(G, trials=1000)\n    assert result == 0\n\n@given(st.data())\ndef test_average_clustering_edge_addition_property():\n    G = nx.erdos_renyi_graph(10, 0.5)\n    initial_result = approximation.average_clustering(G, trials=1000)\n\n    # Add an edge\n    G.add_edge(0, 1)\n    new_result = approximation.average_clustering(G, trials=1000)\n    assert new_result >= initial_result\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.data())\ndef test_average_clustering_coefficient_float_range_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.erdos_renyi_graph, st.integers(min_value=0, max_value=100), st.floats(min_value=0, max_value=1)),\n        st.builds(nx.complete_graph, st.integers(min_value=1, max_value=10)),\n        st.builds(nx.empty_graph, st.integers(min_value=0, max_value=10))\n    ))\n    result = approximation.average_clustering(G)\n    assert isinstance(result, float) and 0 <= result <= 1\n\n@given(st.data())\ndef test_average_clustering_coefficient_no_edges_property(data):\n    G = nx.empty_graph(5)  # A graph with no edges\n    result = approximation.average_clustering(G)\n    assert result == 0.0\n\n@given(st.data())\ndef test_average_clustering_coefficient_consistency_property(data):\n    G = data.draw(st.builds(nx.erdos_renyi_graph, st.integers(min_value=1, max_value=100), st.floats(min_value=0, max_value=1)))\n    result1 = approximation.average_clustering(G, trials=10000)\n    result2 = approximation.average_clustering(G, trials=10000)\n    assert abs(result1 - result2) < 0.01  # Check for stability\n\n@given(st.data())\ndef test_average_clustering_coefficient_empty_graph_property(data):\n    G = nx.empty_graph(0)  # An empty graph\n    result = approximation.average_clustering(G)\n    assert result == 0.0\n\n@given(st.data())\ndef test_average_clustering_coefficient_edge_addition_property(data):\n    G = nx.erdos_renyi_graph(10, 0.5, seed=42)\n    initial_clustering = approximation.average_clustering(G)\n    \n    # Add an edge and compute the new clustering\n    G.add_edge(0, 1)\n    new_clustering = approximation.average_clustering(G)\n    \n    assert new_clustering >= initial_clustering\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.integers(min_value=0, max_value=100), st.floats(0, 1), st.integers(min_value=0, max_value=1000))\ndef test_average_clustering_output_range_property(num_nodes, edge_prob, trials):\n    G = nx.erdos_renyi_graph(num_nodes, edge_prob)\n    result = approximation.average_clustering(G, trials=trials)\n    assert 0 <= result <= 1\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=0, max_size=0))\ndef test_average_clustering_empty_graph_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    result = approximation.average_clustering(G)\n    assert result == 0\n\n@given(st.integers(min_value=1, max_value=100), st.floats(0, 1), st.integers(min_value=100, max_value=1000))\ndef test_average_clustering_consistency_property(num_nodes, edge_prob, trials):\n    G = nx.erdos_renyi_graph(num_nodes, edge_prob)\n    result1 = approximation.average_clustering(G, trials=trials)\n    result2 = approximation.average_clustering(G, trials=trials)\n    assert abs(result1 - result2) < 0.01  # Check for consistency within a small tolerance\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=0), st.integers(min_value=0, max_value=1000))\ndef test_average_clustering_empty_graph_property(edges, trials):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    if G.number_of_nodes() == 0:\n        result = approximation.average_clustering(G, trials=trials)\n        assert result == 0\n\n@given(st.integers(min_value=1, max_value=100), st.floats(0, 1), st.integers(min_value=0, max_value=10))\ndef test_average_clustering_edge_addition_property(num_nodes, edge_prob, trials):\n    G = nx.erdos_renyi_graph(num_nodes, edge_prob)\n    original_result = approximation.average_clustering(G, trials=trials)\n    G.add_edge(0, 1)  # Add an edge\n    new_result = approximation.average_clustering(G, trials=trials)\n    assert new_result >= original_result  # Adding edges should not decrease the clustering coefficient\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.data())\ndef test_average_clustering_output_range_property(data):\n    G = data.draw(st.builds(nx.erdos_renyi_graph, st.integers(min_value=0, max_value=100), st.floats(min_value=0.0, max_value=1.0)))\n    result = approximation.average_clustering(G, trials=1000)\n    assert 0.0 <= result <= 1.0\n\n@given(st.data())\ndef test_average_clustering_no_edges_property(data):\n    G = nx.Graph()\n    result = approximation.average_clustering(G, trials=1000)\n    assert result == 0.0\n\n@given(st.data())\ndef test_average_clustering_consistency_property(data):\n    G = data.draw(st.builds(nx.erdos_renyi_graph, st.integers(min_value=5, max_value=20), st.floats(min_value=0.0, max_value=1.0)))\n    result1 = approximation.average_clustering(G, trials=1000)\n    result2 = approximation.average_clustering(G, trials=1000)\n    assert abs(result1 - result2) < 0.1  # Allow some tolerance for approximation\n\n@given(st.data())\ndef test_average_clustering_empty_graph_property(data):\n    G = nx.Graph()\n    result = approximation.average_clustering(G, trials=1000)\n    assert result == 0.0\n\n@given(st.data())\ndef test_average_clustering_structure_variation_property(data):\n    G = data.draw(st.builds(nx.erdos_renyi_graph, st.integers(min_value=5, max_value=20), st.floats(min_value=0.0, max_value=1.0)))\n    original_clustering = approximation.average_clustering(G, trials=1000)\n    \n    # Add an edge\n    if G.number_of_edges() > 0:\n        u, v = list(G.nodes())[:2]\n        G.add_edge(u, v)\n        new_clustering = approximation.average_clustering(G, trials=1000)\n        assert new_clustering >= original_clustering\n\n    # Remove an edge\n    if G.number_of_edges() > 1:\n        u, v = list(G.edges())[0]\n        G.remove_edge(u, v)\n        new_clustering = approximation.average_clustering(G, trials=1000)\n        assert new_clustering <= original_clustering\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import approximation\n\n@given(st.data())\ndef test_average_clustering_coefficient_range_property():\n    G = st.builds(nx.erdos_renyi_graph, st.integers(min_value=0, max_value=100), st.floats(min_value=0.0, max_value=1.0)).example()\n    c = approximation.average_clustering(G)\n    assert 0 <= c <= 1\n\n@given(st.data())\ndef test_average_clustering_coefficient_empty_graph_property():\n    G = nx.Graph()  # Empty graph\n    c = approximation.average_clustering(G)\n    assert c == 0\n\n@given(st.data())\ndef test_average_clustering_coefficient_consistency_property():\n    G = st.builds(nx.erdos_renyi_graph, st.integers(min_value=1, max_value=100), st.floats(min_value=0.0, max_value=1.0)).example()\n    c1 = approximation.average_clustering(G, trials=10000)\n    c2 = approximation.average_clustering(G, trials=10000)\n    assert abs(c1 - c2) < 0.01  # Checking for consistency within a reasonable margin\n\n@given(st.data())\ndef test_average_clustering_coefficient_no_edges_property():\n    G = nx.Graph()  # No edges\n    G.add_node(1)\n    G.add_node(2)\n    c = approximation.average_clustering(G)\n    assert c == 0\n\n@given(st.data())\ndef test_average_clustering_coefficient_structure_property():\n    G = nx.erdos_renyi_graph(10, 0.5)\n    original_c = approximation.average_clustering(G)\n    G.add_edge(1, 2)  # Adding an edge\n    new_c = approximation.average_clustering(G)\n    assert new_c >= original_c  # Adding edges should not decrease the average clustering coefficient\n# End program\n```"], "properties": ["1. The output average clustering coefficient should always be a float value between 0 and 1, inclusive, as it represents a proportion of triangles in the graph.\n\n2. If the input graph \\( G \\) has no edges, the output should be 0, as there are no triangles to form.\n\n3. The output should remain consistent for the same input graph \\( G \\) when the number of trials is sufficiently large, indicating that the approximation converges to a stable value.\n\n4. If the input graph \\( G \\) is empty (i.e., has no nodes), the output should be 0, as there are no local clusterings to compute.\n\n5. The output should vary with changes in the structure of the input graph \\( G \\); specifically, adding edges should not decrease the average clustering coefficient, and removing edges should not increase it."], "api_doc": "average_clustering\naverage_clustering(G, trials=1000, seed=None)[source]\nEstimates the average clustering coefficient of G.\n\nThe local clustering of each node in G is the fraction of triangles that actually exist over all possible triangles in its neighborhood. The average clustering coefficient of a graph G is the mean of local clusterings.\n\nThis function finds an approximate average clustering coefficient for G by repeating n times (defined in trials) the following experiment: choose a node at random, choose two of its neighbors at random, and check if they are connected. The approximate coefficient is the fraction of triangles found over the number of trials [1].\n\nParameters\n:\nG\nNetworkX graph\ntrials\ninteger\nNumber of trials to perform (default 1000).\n\nseed\ninteger, random_state, or None (default)\nIndicator of random number generation state. See Randomness.\n\nReturns\n:\nc\nfloat\nApproximated average clustering coefficient.\n\nRaises\n:\nNetworkXNotImplemented\nIf G is directed.\n\nReferences\n\n[1]\nSchank, Thomas, and Dorothea Wagner. Approximating clustering coefficient and transitivity. Universit\u00e4t Karlsruhe, Fakult\u00e4t f\u00fcr Informatik, 2004. https://doi.org/10.5445/IR/1000001239\n\nExamples\n\nfrom networkx.algorithms import approximation\nG = nx.erdos_renyi_graph(10, 0.2, seed=10)\napproximation.average_clustering(G, trials=1000, seed=10)\n0.214", "api_code": "@not_implemented_for(\"directed\")\n@py_random_state(2)\n@nx._dispatchable(name=\"approximate_average_clustering\")\ndef average_clustering(G, trials=1000, seed=None):\n    n = len(G)\n    triangles = 0\n    nodes = list(G)\n    for i in [int(seed.random() * n) for i in range(trials)]:\n        nbrs = list(G[nodes[i]])\n        if len(nbrs) < 2:\n            continue\n        u, v = seed.sample(nbrs, 2)\n        if u in G[v]:\n            triangles += 1\n    return triangles / trials"}
{"function_name": "statistics.pstdev", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_statistics_pstdev_non_negative_property(data):\n    result = statistics.pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_statistics_pstdev_constant_list_property(data):\n    constant_data = [data[0]] * len(data)\n    result = statistics.pstdev(constant_data)\n    assert result == 0\n\n@given(st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=1, max_size=1000))\ndef test_statistics_pstdev_increase_with_variability_property(data):\n    varied_data = data + [data[0] + 10]  # Adding a value to increase variability\n    result_original = statistics.pstdev(data)\n    result_varied = statistics.pstdev(varied_data)\n    assert result_varied >= result_original\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_statistics_pstdev_no_change_with_constant_addition_property(data):\n    modified_data = [x + 5 for x in data]  # Adding a constant to all elements\n    result_original = statistics.pstdev(data)\n    result_modified = statistics.pstdev(modified_data)\n    assert result_original == result_modified\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_statistics_pstdev_square_root_of_variance_property(data):\n    result_pstdev = statistics.pstdev(data)\n    result_pvariance = statistics.pvariance(data)\n    assert result_pstdev == result_pvariance ** 0.5\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_statistics_pstdev_non_negative_property(data):\n    result = statistics.pstdev(data)\n    assert result >= 0\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_statistics_pstdev_constant_list_property(data):\n    if len(data) > 0:\n        constant_value = data[0]\n        constant_list = [constant_value] * len(data)\n        result = statistics.pstdev(constant_list)\n        assert result == 0\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_statistics_pstdev_increasing_variability_property(data):\n    if len(data) > 1:\n        original_variance = statistics.pstdev(data)\n        expanded_data = data + [max(data) + 10]  # Adding an outlier to increase variability\n        result = statistics.pstdev(expanded_data)\n        assert result >= original_variance\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_statistics_pstdev_same_variance_property(data):\n    if len(data) > 0:\n        constant_value = 5.0\n        modified_data = [x + constant_value for x in data]\n        result_original = statistics.pstdev(data)\n        result_modified = statistics.pstdev(modified_data)\n        assert result_original == result_modified\n# End program\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=1000))\ndef test_statistics_pstdev_equivalence_with_pvariance_property(data):\n    result_pstdev = statistics.pstdev(data)\n    result_pvariance = statistics.pvariance(data) ** 0.5\n    assert result_pstdev == result_pvariance\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats()))\ndef test_statistics_pstdev_non_negative_property(data):\n    result = statistics.pstdev(data)\n    assert result >= 0\n# End program\n\n@given(st.lists(st.floats(min_value=1.0, max_value=1.0), min_size=1, max_size=5))\ndef test_statistics_pstdev_constant_list_property(data):\n    result = statistics.pstdev(data)\n    assert result == 0\n# End program\n\n@given(st.lists(st.floats(), min_size=2))\ndef test_statistics_pstdev_increasing_variability_property(data):\n    original_variance = statistics.pstdev(data)\n    expanded_data = data + [max(data) + 1]  # Adding a value to increase variability\n    new_variance = statistics.pstdev(expanded_data)\n    assert new_variance >= original_variance\n# End program\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_statistics_pstdev_constant_shift_property(data):\n    constant_shift = 5.0\n    result_original = statistics.pstdev(data)\n    result_shifted = statistics.pstdev([x + constant_shift for x in data])\n    assert result_original == result_shifted\n# End program\n\n@given(st.lists(st.floats()))\ndef test_statistics_pstdev_equivalence_with_pvariance_property(data):\n    if len(data) > 1:  # pvariance requires at least two elements\n        result_pstdev = statistics.pstdev(data)\n        result_pvariance = statistics.pvariance(data) ** 0.5\n        assert result_pstdev == result_pvariance\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(), max_size=1000))\ndef test_statistics_pstdev_non_negative():\n    data = st.data().draw(st.lists(st.floats(), max_size=1000))\n    result = statistics.pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_statistics_pstdev_constant_list():\n    constant_value = st.data().draw(st.floats())\n    data = [constant_value] * 10  # a constant list\n    result = statistics.pstdev(data)\n    assert result == 0\n\n@given(st.lists(st.floats(), min_size=2, max_size=1000))\ndef test_statistics_pstdev_variability_increases():\n    data = st.data().draw(st.lists(st.floats(), min_size=2, max_size=1000))\n    result1 = statistics.pstdev(data)\n\n    # Expand the range by adding more variability\n    extended_data = data + [min(data) - 10, max(data) + 10]\n    result2 = statistics.pstdev(extended_data)\n\n    assert result2 >= result1\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000), st.floats())\ndef test_statistics_pstdev_same_variance():\n    data = st.data().draw(st.lists(st.floats(), min_size=1, max_size=1000))\n    constant_addition = st.data().draw(st.floats())\n    adjusted_data = [x + constant_addition for x in data]\n    \n    result1 = statistics.pstdev(data)\n    result2 = statistics.pstdev(adjusted_data)\n\n    assert result1 == result2\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_statistics_pstdev_equivalence_with_pvariance():\n    data = st.data().draw(st.lists(st.floats(), min_size=1, max_size=1000))\n    result_pstdev = statistics.pstdev(data)\n    result_pvariance = statistics.pvariance(data)\n\n    assert result_pstdev == result_pvariance ** 0.5\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=100))\ndef test_statistics_pstdev_non_negative_property(data):\n    result = statistics.pstdev(data)\n    assert result >= 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=100))\ndef test_statistics_pstdev_constant_list_property(data):\n    if len(set(data)) == 1:  # All elements are the same\n        result = statistics.pstdev(data)\n        assert result == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=100))\ndef test_statistics_pstdev_variability_increases_property(data):\n    if len(data) > 1:\n        result1 = statistics.pstdev(data)\n        result2 = statistics.pstdev(data + [max(data) + 1])  # Adding a value to increase variability\n        assert result2 >= result1\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=100))\ndef test_statistics_pstdev_modify_variance_property(data):\n    if len(data) > 1:\n        result1 = statistics.pstdev(data)\n        modified_data = [x + 3 for x in data]  # Adding a constant to all elements\n        result2 = statistics.pstdev(modified_data)\n        assert result1 == result2\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=100))\ndef test_statistics_pstdev_equivalence_with_pvariance_property(data):\n    result_pstdev = statistics.pstdev(data)\n    result_pvariance = statistics.pvariance(data) ** 0.5  # Square root of pvariance\n    assert result_pstdev == result_pvariance\n# End program\n```"], "properties": ["1. The output of the function is always a non-negative number, as standard deviation cannot be negative.\n2. If the input data is a constant list (all elements are the same), the output should be zero, indicating no variability.\n3. The output should increase if the variability of the input data increases (i.e., if the range of the input data expands).\n4. The output should remain the same if the input list is modified by adding or removing elements that do not change the overall variance (e.g., adding a constant value to all elements).\n5. The output should be equal to the square root of the output from the `pvariance()` function when the same input data is provided."], "api_doc": "statistics.pstdev(data, mu=None)\nReturn the population standard deviation (the square root of the population variance). See pvariance() for arguments and other details.\n\n>>>\npstdev([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])\n0.986893273527251", "api_code": "def pstdev(data, mu=None):\n    T, ss, c, n = _ss(data, mu)\n    if n < 1:\n        raise StatisticsError('pstdev requires at least one data point')\n    mss = ss / n\n    if issubclass(T, Decimal):\n        return _decimal_sqrt_of_frac(mss.numerator, mss.denominator)\n    return _float_sqrt_of_frac(mss.numerator, mss.denominator)"}
{"function_name": "datetime.date.weekday", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int) and 0 <= result <= 6\n\n@given(st.dates())\ndef test_monday_always_returns_0_property(date):\n    if date.weekday() == 0:  # Check if it's Monday\n        assert date.weekday() == 0\n\n@given(st.dates())\ndef test_sunday_always_returns_6_property(date):\n    if date.weekday() == 6:  # Check if it's Sunday\n        assert date.weekday() == 6\n\n@given(st.dates())\ndef test_same_day_of_week_is_consistent_property(date):\n    day_of_week = date.weekday()\n    # Generate another date with the same day of the week\n    same_day_date = date + datetime.timedelta(days=(7 * st.integers(min_value=0, max_value=10).example()))\n    assert same_day_date.weekday() == day_of_week\n\n@given(st.dates())\ndef test_one_week_apart_same_output_property(date):\n    result = date.weekday()\n    week_apart_date = date + datetime.timedelta(days=7)\n    assert week_apart_date.weekday() == result\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int) and 0 <= result <= 6\n\n@given(st.dates())\ndef test_monday_outputs_0_property(date):\n    if date.weekday() == 0:  # If the date is a Monday\n        assert date.weekday() == 0\n\n@given(st.dates())\ndef test_sunday_outputs_6_property(date):\n    if date.weekday() == 6:  # If the date is a Sunday\n        assert date.weekday() == 6\n\n@given(st.dates())\ndef test_same_day_of_week_is_consistent_property(date):\n    day_of_week = date.weekday()\n    assert day_of_week == day_of_week  # Consistency check for the same weekday\n\n@given(st.dates())\ndef test_one_week_apart_same_output_property(date):\n    one_week_later = date + datetime.timedelta(weeks=1)\n    assert date.weekday() == one_week_later.weekday()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int) and 0 <= result <= 6\n\n@given(st.dates())\ndef test_monday_output_is_0_property(date):\n    if date.weekday() == 0:  # If it's a Monday\n        assert date.weekday() == 0\n\n@given(st.dates())\ndef test_sunday_output_is_6_property(date):\n    if date.weekday() == 6:  # If it's a Sunday\n        assert date.weekday() == 6\n\n@given(st.dates())\ndef test_same_day_of_week_consistency_property(date):\n    if date.weekday() in range(7):  # Check for any valid weekday\n        assert datetime.date(date.year, date.month, date.day).weekday() == date.weekday()\n\n@given(st.dates())\ndef test_one_week_apart_consistency_property(date):\n    one_week_later = date + datetime.timedelta(weeks=1)\n    assert date.weekday() == one_week_later.weekday()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int) and 0 <= result <= 6\n\n@given(st.dates())\ndef test_monday_output_is_0_property(date):\n    if date.weekday() == 0:  # If the date is a Monday\n        assert date.weekday() == 0\n\n@given(st.dates())\ndef test_sunday_output_is_6_property(date):\n    if date.weekday() == 6:  # If the date is a Sunday\n        assert date.weekday() == 6\n\n@given(st.dates())\ndef test_same_day_of_week_is_consistent_property(date):\n    weekday = date.weekday()\n    assert all((date.replace(month=(date.month + i - 1) % 12 + 1) if (date.month + i - 1) % 12 + 1 != 0 else date.replace(year=date.year + 1, month=1) if (date.month + i - 1) % 12 + 1 == 1 else date).weekday() == weekday for i in range(-3, 4))\n\n@given(st.dates())\ndef test_week_apart_output_is_same_property(date):\n    assert date.weekday() == (date + datetime.timedelta(weeks=1)).weekday()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates())\ndef test_output_is_integer_between_0_and_6_property(date):\n    result = date.weekday()\n    assert isinstance(result, int) and 0 <= result <= 6\n\n@given(st.dates())\ndef test_monday_output_is_0_property(date):\n    if date.weekday() == 0:  # If it's Monday\n        assert date.weekday() == 0\n\n@given(st.dates())\ndef test_sunday_output_is_6_property(date):\n    if date.weekday() == 6:  # If it's Sunday\n        assert date.weekday() == 6\n\n@given(st.dates())\ndef test_same_day_of_week_is_consistent_property(date):\n    weekday = date.weekday()\n    assert weekday == datetime.date(date.year, date.month, date.day).weekday()\n\n@given(st.dates())\ndef test_output_for_dates_one_week_apart_remains_same_property(date):\n    same_weekday_date = date + datetime.timedelta(days=7)\n    assert date.weekday() == same_weekday_date.weekday()\n# End program\n```"], "properties": ["1. The output of the function is always an integer between 0 and 6, inclusive.\n2. The output for a date representing a Monday is always 0.\n3. The output for a date representing a Sunday is always 6.\n4. The output for a date representing the same day of the week (e.g., all Mondays) is consistent regardless of the month or year.\n5. The output for a date that is one week apart (i.e., adding or subtracting 7 days) remains the same."], "api_doc": "date.weekday()\nReturn the day of the week as an integer, where Monday is 0 and Sunday is 6. For example, date(2002, 12, 4).weekday() == 2, a Wednesday. See also isoweekday().", "api_code": "def weekday(self):\n    return (self.toordinal() + 6) % 7"}
{"function_name": "decimal.Decimal.from_float", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_is_decimal_instance_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats())\ndef test_output_preserves_precision_property(f):\n    result = Decimal.from_float(f)\n    expected = Decimal(str(f))\n    assert result == expected\n\n@given(st.one_of(st.floats(allow_nan=True), st.floats(allow_infinity=True)))\ndef test_special_float_values_property(f):\n    result = Decimal.from_float(f)\n    if math.isnan(f):\n        assert result.is_nan()\n    elif math.isinf(f):\n        assert result == Decimal('Infinity') if f > 0 else Decimal('-Infinity')\n\n@given(st.floats())\ndef test_output_not_equal_to_float_repr_property(f):\n    if f != 0:\n        result = Decimal.from_float(f)\n        float_repr = Decimal(str(f))\n        assert result != float_repr\n\n@given(st.floats())\ndef test_deterministic_output_property(f):\n    result1 = Decimal.from_float(f)\n    result2 = Decimal.from_float(f)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_is_decimal_instance_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_precision_property(f):\n    result = Decimal.from_float(f)\n    # Check if the output is the nearest decimal representation\n    expected = Decimal(str(f))\n    assert result == expected\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_special_float_values_property(f):\n    result = Decimal.from_float(f)\n    if math.isnan(f):\n        assert result.is_nan()\n    elif math.isinf(f):\n        assert (result == Decimal('Infinity') and f > 0) or (result == Decimal('-Infinity') and f < 0)\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_not_equal_for_unrepresentable_floats_property(f):\n    if f != Decimal(str(f)):\n        result = Decimal.from_float(f)\n        assert result != Decimal(str(f))\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_deterministic_output_property(f):\n    result1 = Decimal.from_float(f)\n    result2 = Decimal.from_float(f)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_is_decimal_instance_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_preserves_precision_property(f):\n    result = Decimal.from_float(f)\n    expected = Decimal(str(f))\n    assert result == expected\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_handles_special_values_property(f):\n    if math.isnan(f):\n        assert Decimal.from_float(f) == Decimal('NaN')\n    elif f == float('inf'):\n        assert Decimal.from_float(f) == Decimal('Infinity')\n    elif f == float('-inf'):\n        assert Decimal.from_float(f) == Decimal('-Infinity')\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_not_equal_for_unrepresentable_floats_property(f):\n    if f != Decimal(str(f)):\n        result = Decimal.from_float(f)\n        assert result != Decimal(str(f))\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_is_deterministic_property(f):\n    result1 = Decimal.from_float(f)\n    result2 = Decimal.from_float(f)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_output_is_instance_of_decimal_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_preserves_precision_property(f):\n    result = Decimal.from_float(f)\n    expected = Decimal(f)\n    assert result == expected  # Check if the nearest decimal value is correct\n\n@given(st.one_of(st.just(float('nan')), st.just(float('inf')), st.just(float('-inf'))))\ndef test_special_float_values_property(f):\n    result = Decimal.from_float(f)\n    if f is float('nan'):\n        assert result.is_nan()\n    elif f is float('inf'):\n        assert result == Decimal('Infinity')\n    elif f is float('-inf'):\n        assert result == Decimal('-Infinity')\n\n@given(st.floats(allow_nan=False, allow_infinity=False).filter(lambda x: x != 0))\ndef test_output_is_not_exact_decimal_representation_property(f):\n    result = Decimal.from_float(f)\n    expected_decimal = Decimal(f)\n    assert result != expected_decimal or result == Decimal('0')  # If not zero, they should differ\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_deterministic_output_property(f):\n    result1 = Decimal.from_float(f)\n    result2 = Decimal.from_float(f)\n    assert result1 == result2  # The function should be deterministic\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_from_float_instance_property(f):\n    result = Decimal.from_float(f)\n    assert isinstance(result, Decimal)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_from_float_precision_property(f):\n    result = Decimal.from_float(f)\n    assert result == Decimal(str(f))\n\n@given(st.floats(allow_nan=True, allow_infinity=True))\ndef test_decimal_from_float_special_values_property(f):\n    result = Decimal.from_float(f)\n    if math.isnan(f):\n        assert result.is_nan()\n    elif math.isinf(f):\n        assert result.is_infinite()\n        assert result == Decimal('Infinity') if f > 0 else Decimal('-Infinity')\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_decimal_from_float_non_exact_property(f):\n    result = Decimal.from_float(f)\n    assert result != Decimal(str(f)) if f != round(f, 0) else result == Decimal(str(f))\n\n@given(st.floats(allow_nan=False, allow_infinity=False, min_value=-1e100, max_value=1e100))\ndef test_decimal_from_float_deterministic_property(f):\n    result1 = Decimal.from_float(f)\n    result2 = Decimal.from_float(f)\n    assert result1 == result2\n# End program\n```"], "properties": ["1. The output is an instance of the `Decimal` class regardless of the input being a float or an int.\n2. The output accurately represents the nearest decimal value to the input float, ensuring that the precision of the float is preserved as closely as possible.\n3. The output correctly handles special float values, such as returning 'NaN' for input `float('nan')`, 'Infinity' for `float('inf')`, and '-Infinity' for `float('-inf')`.\n4. The output is not equal to the decimal representation of the input float when the float cannot be exactly represented in binary, demonstrating that the output may contain more precision than the input.\n5. The output remains consistent across multiple calls with the same float input, ensuring that the function is deterministic."], "api_doc": "classmethod from_float(f)\nAlternative constructor that only accepts instances of float or int.\n\nNote Decimal.from_float(0.1) is not the same as Decimal('0.1'). Since 0.1 is not exactly representable in binary floating point, the value is stored as the nearest representable value which is 0x1.999999999999ap-4. That equivalent value in decimal is 0.1000000000000000055511151231257827021181583404541015625.\n\nNote From Python 3.2 onwards, a Decimal instance can also be constructed directly from a float.\n>>>\nDecimal.from_float(0.1)\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\nDecimal.from_float(float('nan'))\nDecimal('NaN')\nDecimal.from_float(float('inf'))\nDecimal('Infinity')\nDecimal.from_float(float('-inf'))\nDecimal('-Infinity')\nAdded in version 3.1.", "api_code": "@classmethod\ndef from_float(cls, f):\n    if isinstance(f, int):                # handle integer inputs\n        sign = 0 if f >= 0 else 1\n        k = 0\n        coeff = str(abs(f))\n    elif isinstance(f, float):\n        if _math.isinf(f) or _math.isnan(f):\n            return cls(repr(f))\n        if _math.copysign(1.0, f) == 1.0:\n            sign = 0\n        else:\n            sign = 1\n        n, d = abs(f).as_integer_ratio()\n        k = d.bit_length() - 1\n        coeff = str(n*5**k)\n    else:\n        raise TypeError(\"argument must be int or float.\")\n\n    result = _dec_from_triple(sign, coeff, -k)\n    if cls is Decimal:\n        return result\n    else:\n        return cls(result)"}
{"function_name": "decimal.Decimal.shift", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n# Set a high precision to avoid overflow issues\ngetcontext().prec = 100\n\n@given(st.integers(min_value=1, max_value=10**10), st.integers(min_value=0, max_value=10))\ndef test_positive_integer_shift_left_property(x, shift_amount):\n    result = Decimal(x).shift(shift_amount)\n    assert result >= Decimal(x)\n\n@given(st.integers(min_value=1, max_value=10**10), st.integers(min_value=1, max_value=10))\ndef test_positive_integer_shift_right_property(x, shift_amount):\n    result = Decimal(x).shift(-shift_amount)\n    assert result <= Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=-1), st.integers(min_value=0, max_value=10))\ndef test_negative_integer_shift_left_property(x, shift_amount):\n    result = Decimal(x).shift(shift_amount)\n    assert result <= Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=-1), st.integers(min_value=1, max_value=10))\ndef test_negative_integer_shift_right_property(x, shift_amount):\n    result = Decimal(x).shift(-shift_amount)\n    assert result >= Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=10**10), st.integers(min_value=-10, max_value=10))\ndef test_sign_and_exponent_property(x, shift_amount):\n    original_decimal = Decimal(x)\n    result = original_decimal.shift(shift_amount)\n    assert result.copy_sign(original_decimal) == original_decimal  # Sign remains the same\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n# Set a reasonable precision to avoid overflow issues during testing\ngetcontext().prec = 50\n\n@given(st.integers(min_value=1, max_value=10**10), st.integers(min_value=0, max_value=10))\ndef test_positive_integer_shift_left_property(value, shift_amount):\n    result = Decimal(value).shift(shift_amount)\n    assert result >= Decimal(value)\n\n@given(st.integers(min_value=1, max_value=10**10), st.integers(min_value=1, max_value=10))\ndef test_positive_integer_shift_right_property(value, shift_amount):\n    result = Decimal(value).shift(-shift_amount)\n    assert result <= Decimal(value)\n\n@given(st.integers(min_value=-10**10, max_value=-1), st.integers(min_value=0, max_value=10))\ndef test_negative_integer_shift_left_property(value, shift_amount):\n    result = Decimal(value).shift(shift_amount)\n    assert result <= Decimal(value)\n\n@given(st.integers(min_value=-10**10, max_value=-1), st.integers(min_value=1, max_value=10))\ndef test_negative_integer_shift_right_property(value, shift_amount):\n    result = Decimal(value).shift(-shift_amount)\n    assert result >= Decimal(value)\n\n@given(st.decimals(min_value=-10**10, max_value=10**10), st.integers(min_value=-10, max_value=10))\ndef test_sign_and_exponent_property(value, shift_amount):\n    original_decimal = Decimal(value)\n    result = original_decimal.shift(shift_amount)\n    assert result.sign() == original_decimal.sign()\n    assert result == original_decimal * (10 ** shift_amount)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n# Set a high precision for the Decimal context to avoid overflow issues\ngetcontext().prec = 50\n\n@given(st.integers(min_value=-10**10, max_value=10**10), st.integers())\ndef test_positive_integer_shift_left_property(x, shift_amount):\n    if shift_amount < 0:\n        shift_amount = 0  # Ensure only left shifts are considered\n    result = Decimal(x).shift(shift_amount)\n    assert result >= Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=10**10), st.integers(min_value=1))\ndef test_positive_integer_shift_right_property(x, shift_amount):\n    result = Decimal(x).shift(-shift_amount)\n    assert result <= Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=-1), st.integers())\ndef test_negative_integer_shift_left_property(x, shift_amount):\n    if shift_amount < 0:\n        shift_amount = 0  # Ensure only left shifts are considered\n    result = Decimal(x).shift(shift_amount)\n    assert result <= Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=-1), st.integers(min_value=1))\ndef test_negative_integer_shift_right_property(x, shift_amount):\n    result = Decimal(x).shift(-shift_amount)\n    assert result >= Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=10**10), st.integers())\ndef test_sign_and_exponent_property(x, shift_amount):\n    original = Decimal(x)\n    result = original.shift(shift_amount)\n    assert result.copy_sign(original) == original\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n# Set a higher precision to handle large numbers and avoid overflow\ngetcontext().prec = 50\n\n@given(st.integers(min_value=1, max_value=10**20), st.integers(min_value=0, max_value=20))\ndef test_positive_integer_shift_left_property(x, n):\n    \"\"\"Shifting a positive integer to the left results in a number greater than or equal to the original.\"\"\"\n    original = Decimal(x)\n    shifted = original.shift(n)\n    assert shifted >= original\n\n@given(st.integers(min_value=1, max_value=10**20), st.integers(min_value=1, max_value=20))\ndef test_positive_integer_shift_right_property(x, n):\n    \"\"\"Shifting a positive integer to the right results in a number less than or equal to the original.\"\"\"\n    original = Decimal(x)\n    shifted = original.shift(-n)\n    assert shifted <= original\n\n@given(st.integers(min_value=-10**20, max_value=-1), st.integers(min_value=0, max_value=20))\ndef test_negative_integer_shift_left_property(x, n):\n    \"\"\"Shifting a negative integer to the left results in a number less than or equal to the original.\"\"\"\n    original = Decimal(x)\n    shifted = original.shift(n)\n    assert shifted <= original\n\n@given(st.integers(min_value=-10**20, max_value=-1), st.integers(min_value=1, max_value=20))\ndef test_negative_integer_shift_right_property(x, n):\n    \"\"\"Shifting a negative integer to the right results in a number greater than or equal to the original.\"\"\"\n    original = Decimal(x)\n    shifted = original.shift(-n)\n    assert shifted >= original\n\n@given(st.decimals(), st.integers(min_value=-20, max_value=20))\ndef test_sign_and_exponent_property(x, n):\n    \"\"\"The output of the shift operation should have the same sign and exponent as the first operand.\"\"\"\n    original = Decimal(x)\n    shifted = original.shift(n)\n    assert shifted.sign() == original.sign()\n    assert shifted.as_tuple().exponent == original.as_tuple().exponent\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport decimal\n\n@given(st.integers(min_value=1, max_value=10**10), st.integers(min_value=1, max_value=10))\ndef test_shifting_positive_integer_left_property(x, n):\n    result = decimal.Decimal(x).shift(n)\n    assert result >= decimal.Decimal(x)\n\n@given(st.integers(min_value=1, max_value=10**10), st.integers(min_value=1, max_value=10))\ndef test_shifting_positive_integer_right_property(x, n):\n    result = decimal.Decimal(x).shift(-n)\n    assert result <= decimal.Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=-1), st.integers(min_value=1, max_value=10))\ndef test_shifting_negative_integer_left_property(x, n):\n    result = decimal.Decimal(x).shift(n)\n    assert result <= decimal.Decimal(x)\n\n@given(st.integers(min_value=-10**10, max_value=-1), st.integers(min_value=1, max_value=10))\ndef test_shifting_negative_integer_right_property(x, n):\n    result = decimal.Decimal(x).shift(-n)\n    assert result >= decimal.Decimal(x)\n\n@given(st.integers(), st.integers(min_value=-10, max_value=10))\ndef test_sign_and_exponent_property(x, n):\n    original = decimal.Decimal(x)\n    result = original.shift(n)\n    assert result.copy_sign(original) == original\n# End program\n```"], "properties": ["1. Shifting a positive integer to the left by a non-negative integer results in a number that is greater than or equal to the original number, assuming no overflow occurs.\n\n2. Shifting a positive integer to the right by a positive integer results in a number that is less than or equal to the original number, assuming the shift does not exceed the number of digits in the integer.\n\n3. Shifting a negative integer to the left by a non-negative integer results in a number that is less than or equal to the original number, assuming no overflow occurs.\n\n4. Shifting a negative integer to the right by a positive integer results in a number that is greater than or equal to the original number, assuming the shift does not exceed the number of digits in the integer.\n\n5. The output of the shift operation should always have the same sign and exponent as the first operand, regardless of the shifting amount."], "api_doc": "shift(other, context=None)\nReturn the result of shifting the digits of the first operand by an amount specified by the second operand. The second operand must be an integer in the range -precision through precision. The absolute value of the second operand gives the number of places to shift. If the second operand is positive then the shift is to the left; otherwise the shift is to the right. Digits shifted into the coefficient are zeros. The sign and exponent of the first operand are unchanged.", "api_code": "def shift(self, other, context=None):\n    if context is None:\n        context = getcontext()\n\n    other = _convert_other(other, raiseit=True)\n\n    ans = self._check_nans(other, context)\n    if ans:\n        return ans\n\n    if other._exp != 0:\n        return context._raise_error(InvalidOperation)\n    if not (-context.prec <= int(other) <= context.prec):\n        return context._raise_error(InvalidOperation)\n\n    if self._isinfinity():\n        return Decimal(self)\n\n    # get values, pad if necessary\n    torot = int(other)\n    rotdig = self._int\n    topad = context.prec - len(rotdig)\n    if topad > 0:\n        rotdig = '0'*topad + rotdig\n    elif topad < 0:\n        rotdig = rotdig[-topad:]\n\n    # let's shift!\n    if torot < 0:\n        shifted = rotdig[:torot]\n    else:\n        shifted = rotdig + '0'*torot\n        shifted = shifted[-context.prec:]\n\n    return _dec_from_triple(self._sign,\n                                shifted.lstrip('0') or '0', self._exp)"}
{"function_name": "decimal.Decimal.exp", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n# Setting a high precision to accommodate large Decimal operations\ngetcontext().prec = 50\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_exp_output_is_positive_property(x):\n    result = Decimal(x).exp()\n    assert result > 0\n\n@given(st.just(0))\ndef test_exp_of_zero_property(x):\n    result = Decimal(x).exp()\n    assert result == Decimal(1)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_exp_is_strictly_increasing_property(x1, x2):\n    if x1 < x2:\n        result1 = Decimal(x1).exp()\n        result2 = Decimal(x2).exp()\n        assert result1 < result2\n\n@given(st.decimals(min_value=-1e100, max_value=0))  # Limiting to avoid overflow\ndef test_exp_approaches_zero_as_input_negative_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result < 1  # For large negative x, exp(x) should be close to 0\n\n@given(st.decimals(min_value=0, max_value=1e100))  # Limiting to avoid overflow\ndef test_exp_grows_without_bound_as_input_positive_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result > 0  # For large positive x, exp(x) should grow large\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n# Set a high precision context to avoid overflow issues with large exponents\ngetcontext().prec = 50\n\n@given(st.decimals())\ndef test_output_is_positive_property(x):\n    result = Decimal(x).exp()\n    assert result > 0\n\n@given(st.decimals())\ndef test_output_equals_one_when_input_is_zero_property():\n    result = Decimal(0).exp()\n    assert result == Decimal(1)\n\n@given(st.decimals(), st.decimals())\ndef test_output_is_strictly_increasing_property(x1, x2):\n    if x1 < x2:\n        result1 = Decimal(x1).exp()\n        result2 = Decimal(x2).exp()\n        assert result1 < result2\n\n@given(st.decimals(min_value=-1000, max_value=0))\ndef test_output_approaches_zero_for_negative_infinity_property(x):\n    result = Decimal(x).exp()\n    if x < -100:  # Testing behavior for very negative values\n        assert result < 0.01  # Expecting a small positive result\n\n@given(st.decimals(min_value=0, max_value=1000))\ndef test_output_tends_to_infinity_for_positive_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result > 1e50  # Expecting a large positive result\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_positive_output_property(x):\n    result = Decimal(x).exp()\n    assert result > 0\n\n@given(st.just(0))\ndef test_zero_input_property(x):\n    result = Decimal(x).exp()\n    assert result == Decimal(1)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_strictly_increasing_property(x1, x2):\n    if x1 < x2:\n        result1 = Decimal(x1).exp()\n        result2 = Decimal(x2).exp()\n        assert result1 < result2\n\n@given(st.floats(allow_nan=False, allow_infinity=False).filter(lambda x: x < 0))\ndef test_approaches_zero_negative_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result < 1  # As x becomes very negative, e^x should approach 0.\n\n@given(st.floats(allow_nan=False, allow_infinity=False).filter(lambda x: x > 0))\ndef test_approaches_infinity_positive_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result > 0  # As x increases indefinitely, e^x should tend to infinity.\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_positive_property(x):\n    result = Decimal(x).exp()\n    assert result > 0\n\n@given(st.just(0.0))\ndef test_output_one_property(x):\n    result = Decimal(x).exp()\n    assert result == Decimal(1)\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_output_increasing_property(x1, x2):\n    if x1 < x2:\n        result1 = Decimal(x1).exp()\n        result2 = Decimal(x2).exp()\n        assert result1 < result2\n\n@given(st.floats(allow_nan=False, allow_infinity=True).filter(lambda x: x < 0))\ndef test_output_approaching_zero_property(x):\n    result = Decimal(x).exp()\n    assert result < 1  # Since e^x approaches 0 as x approaches negative infinity\n\n@given(st.floats(allow_nan=False, allow_infinity=True).filter(lambda x: x > 0))\ndef test_output_tending_to_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result > 0  # This is just to ensure that the output is growing for positive input\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_exponential_output_positive_property(x):\n    result = Decimal(x).exp()\n    assert result > 0\n\n@given(st.just(0))\ndef test_exponential_zero_property(x):\n    result = Decimal(x).exp()\n    assert result == Decimal('1')\n\n@given(st.floats(allow_nan=False, allow_infinity=False))\ndef test_exponential_increasing_property(x1, x2):\n    if x1 < x2:\n        result1 = Decimal(x1).exp()\n        result2 = Decimal(x2).exp()\n        assert result1 < result2\n\n@given(st.floats(min_value=-1e10, max_value=0))  # Constrain to negative values to avoid overflow\ndef test_exponential_negative_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result < 1  # As x approaches -\u221e, e^x approaches 0\n\n@given(st.floats(min_value=0, max_value=1e10))  # Constrain to positive values to avoid overflow\ndef test_exponential_positive_infinity_property(x):\n    result = Decimal(x).exp()\n    assert result > 0  # As x approaches +\u221e, e^x should grow without bound\n# End program\n```"], "properties": ["1. The output of the exponential function for any input \\( x \\) should always be a positive decimal number, as \\( e^x \\) is positive for all real numbers \\( x \\).\n\n2. The output should be equal to 1 when the input is 0, since \\( e^0 = 1 \\).\n\n3. The output should be strictly increasing; that is, if \\( x_1 < x_2 \\), then \\( e^{x_1} < e^{x_2} \\) for any real numbers \\( x_1 \\) and \\( x_2 \\).\n\n4. The output should approach 0 as the input approaches negative infinity, reflecting the behavior that \\( e^x \\) approaches 0 as \\( x \\) becomes very negative.\n\n5. The output should grow without bound as the input approaches positive infinity, meaning that \\( e^x \\) should tend to infinity as \\( x \\) increases indefinitely."], "api_doc": "exp(context=None)\nReturn the value of the (natural) exponential function e**x at the given number. The result is correctly rounded using the ROUND_HALF_EVEN rounding mode.\n\n>>>\nDecimal(1).exp()\nDecimal('2.718281828459045235360287471')\nDecimal(321).exp()\nDecimal('2.561702493119680037517373933E+139')", "api_code": "def exp(self, context=None):\n\n    if context is None:\n        context = getcontext()\n\n    # exp(NaN) = NaN\n    ans = self._check_nans(context=context)\n    if ans:\n        return ans\n\n    # exp(-Infinity) = 0\n    if self._isinfinity() == -1:\n        return _Zero\n\n    # exp(0) = 1\n    if not self:\n        return _One\n\n    # exp(Infinity) = Infinity\n    if self._isinfinity() == 1:\n        return Decimal(self)"}
{"function_name": "decimal.Decimal.as_integer_ratio", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation\nimport math\n\n# Property 1: The output pair (n, d) must consist of two integers.\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_output_pair_is_integers_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert isinstance(n, int) and isinstance(d, int)\n\n# Property 2: The denominator d must always be positive.\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_denominator_positive_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert d > 0\n\n# Property 3: The fraction represented by (n, d) must be in lowest terms.\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fraction_in_lowest_terms_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert math.gcd(n, d) == 1\n\n# Property 4: The output pair (n, d) must represent the exact value of the Decimal.\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_exact_representation_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert Decimal(n) / Decimal(d) == decimal_value\n\n# Property 5: The function must raise an OverflowError for infinity and ValueError for NaN.\n@given(st.one_of(st.decimals(allow_nan=True, allow_infinity=True), st.integers()))\ndef test_overflow_and_nan_property(value):\n    try:\n        result = Decimal(value).as_integer_ratio()\n        assert not (math.isinf(value) or math.isnan(value))\n    except OverflowError:\n        assert math.isinf(value)\n    except ValueError:\n        assert math.isnan(value)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, Overflow, InvalidOperation\nimport math\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_output_is_pair_of_integers_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert isinstance(n, int) and isinstance(d, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_denominator_is_positive_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert d > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fraction_is_lowest_terms_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert math.gcd(n, d) == 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fraction_exact_representation_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert Decimal(n) / Decimal(d) == decimal_value\n\n@given(st.one_of(st.decimals(allow_nan=True), st.decimals(allow_infinity=True)))\ndef test_raises_on_infinity_or_nan_property(decimal_value):\n    if decimal_value.is_nan() or decimal_value.is_infinite():\n        try:\n            decimal_value.as_integer_ratio()\n            assert False, \"Expected ValueError or OverflowError\"\n        except (ValueError, OverflowError):\n            pass  # Expected behavior\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation\nfrom math import gcd\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_output_is_integer_pair(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert isinstance(n, int) and isinstance(d, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_denominator_positive(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert d > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_in_lowest_terms(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert gcd(n, d) == 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_as_integer_ratio_exact_representation(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert Decimal(n) / Decimal(d) == decimal_value\n\n@given(st.decimals(allow_nan=True, allow_infinity=True))\ndef test_as_integer_ratio_raises_on_special_values(decimal_value):\n    try:\n        decimal_value.as_integer_ratio()\n    except (OverflowError, ValueError):\n        pass\n    else:\n        assert not (decimal_value.is_nan() or decimal_value.is_infinite())\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation\nimport math\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_output_is_pair_of_integers_property(value):\n    n, d = value.as_integer_ratio()\n    assert isinstance(n, int) and isinstance(d, int)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_denominator_is_positive_property(value):\n    n, d = value.as_integer_ratio()\n    assert d > 0\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fraction_in_lowest_terms_property(value):\n    n, d = value.as_integer_ratio()\n    assert math.gcd(n, d) == 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_exact_representation_property(value):\n    n, d = value.as_integer_ratio()\n    assert Decimal(n) / Decimal(d) == value\n\n@given(st.one_of(st.integers(), st.floats(allow_nan=False, allow_infinity=True)))\ndef test_overflow_and_nan_cases_property(value):\n    try:\n        if isinstance(value, float) and (math.isinf(value) or math.isnan(value)):\n            with pytest.raises((OverflowError, ValueError)):\n                Decimal(value).as_integer_ratio()\n    except InvalidOperation:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\nimport math\n\n@given(st.decimals(min_value=-Decimal('1e1000'), max_value=Decimal('1e1000')))\ndef test_output_is_pair_of_integers_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert isinstance(n, int) and isinstance(d, int)\n\n@given(st.decimals(min_value=-Decimal('1e1000'), max_value=Decimal('1e1000')))\ndef test_denominator_is_positive_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert d > 0\n\n@given(st.decimals(min_value=-Decimal('1e1000'), max_value=Decimal('1e1000')))\ndef test_fraction_is_in_lowest_terms_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert math.gcd(n, d) == 1\n\n@given(st.decimals(min_value=-Decimal('1e1000'), max_value=Decimal('1e1000')))\ndef test_fraction_exact_representation_property(decimal_value):\n    n, d = decimal_value.as_integer_ratio()\n    assert Decimal(n) / Decimal(d) == decimal_value\n\n@given(st.one_of(st.decimals(allow_nan=False, allow_infinity=False), \n                 st.decimals(allow_nan=True), \n                 st.decimals(allow_infinity=True)))\ndef test_raises_errors_on_special_values_property(decimal_value):\n    if decimal_value.is_nan():\n        try:\n            decimal_value.as_integer_ratio()\n        except ValueError:\n            pass  # Expected behavior\n        else:\n            assert False, \"Expected ValueError for NaN\"\n    elif decimal_value.is_infinite():\n        try:\n            decimal_value.as_integer_ratio()\n        except OverflowError:\n            pass  # Expected behavior\n        else:\n            assert False, \"Expected OverflowError for infinity\"\n# End program\n```"], "properties": ["1. The output pair (n, d) must consist of two integers, where n is the numerator and d is the denominator.\n\n2. The denominator d must always be positive, regardless of the input Decimal value.\n\n3. The fraction represented by the output pair (n, d) must be in lowest terms, meaning that the greatest common divisor (GCD) of n and d must be 1.\n\n4. If the input Decimal is a finite number, the output pair (n, d) must represent the exact value of the Decimal as a fraction.\n\n5. The function must raise an OverflowError if the input Decimal is an infinity and a ValueError if the input is NaN (not a number)."], "api_doc": "as_integer_ratio()\nReturn a pair (n, d) of integers that represent the given Decimal instance as a fraction, in lowest terms and with a positive denominator:\n\n>>>\nDecimal('-3.14').as_integer_ratio()\n(-157, 50)\nThe conversion is exact. Raise OverflowError on infinities and ValueError on NaNs.\n\nAdded in version 3.6.", "api_code": "def as_integer_ratio(self):\n    return (self._numerator, self._denominator)"}
{"function_name": "statistics.median", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_median_non_empty_output_property(data):\n    result = statistics.median(data)\n    assert isinstance(result, (int, float))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_median_odd_length_property(data):\n    if len(data) % 2 == 1:\n        sorted_data = sorted(data)\n        result = statistics.median(sorted_data)\n        assert result == sorted_data[len(sorted_data) // 2]\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_median_even_length_property(data):\n    if len(data) % 2 == 0:\n        sorted_data = sorted(data)\n        result = statistics.median(sorted_data)\n        assert result == (sorted_data[len(sorted_data) // 2 - 1] + sorted_data[len(sorted_data) // 2]) / 2\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False)))\ndef test_median_sorted_property(data):\n    sorted_data = sorted(data)\n    result_unsorted = statistics.median(data)\n    result_sorted = statistics.median(sorted_data)\n    assert result_unsorted == result_sorted\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_median_empty_input_property(data):\n    if len(data) == 0:\n        try:\n            statistics.median(data)\n            assert False, \"Expected StatisticsError for empty input\"\n        except statistics.StatisticsError:\n            pass  # Expected behavior\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.integers(min_value=-1000000, max_value=1000000), min_size=1))\ndef test_median_non_empty_output_is_number_property(data):\n    result = statistics.median(data)\n    assert isinstance(result, (int, float))\n\n@given(st.lists(st.integers(min_value=-1000000, max_value=1000000), min_size=1))\ndef test_median_odd_length_is_middle_element_property(data):\n    if len(data) % 2 == 1:\n        sorted_data = sorted(data)\n        result = statistics.median(sorted_data)\n        assert result == sorted_data[len(sorted_data) // 2]\n\n@given(st.lists(st.integers(min_value=-1000000, max_value=1000000), min_size=2))\ndef test_median_even_length_is_average_of_middle_elements_property(data):\n    if len(data) % 2 == 0:\n        sorted_data = sorted(data)\n        mid_index = len(sorted_data) // 2\n        expected_result = (sorted_data[mid_index - 1] + sorted_data[mid_index]) / 2\n        result = statistics.median(sorted_data)\n        assert result == expected_result\n\n@given(st.lists(st.integers(min_value=-1000000, max_value=1000000)))\ndef test_median_sorted_input_is_same_property(data):\n    sorted_data = sorted(data)\n    result_unsorted = statistics.median(data)\n    result_sorted = statistics.median(sorted_data)\n    assert result_unsorted == result_sorted\n\n@given(st.lists(st.integers(min_value=-1000000, max_value=1000000), min_size=0))\ndef test_median_empty_input_raises_statistics_error_property(data):\n    if len(data) == 0:\n        try:\n            statistics.median(data)\n            assert False, \"Expected StatisticsError not raised\"\n        except statistics.StatisticsError:\n            pass  # Test passes\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_median_non_empty_output_is_number(data):\n    result = statistics.median(data)\n    assert isinstance(result, (int, float))\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_median_odd_length_equals_middle_element(data):\n    data_sorted = sorted(data)\n    if len(data_sorted) % 2 == 1:\n        middle_index = len(data_sorted) // 2\n        result = statistics.median(data_sorted)\n        assert result == data_sorted[middle_index]\n\n@given(st.lists(st.integers(), min_size=2))\ndef test_median_even_length_average_of_middle_elements(data):\n    data_sorted = sorted(data)\n    if len(data_sorted) % 2 == 0:\n        middle_index = len(data_sorted) // 2\n        result = statistics.median(data_sorted)\n        average_of_middle = (data_sorted[middle_index - 1] + data_sorted[middle_index]) / 2\n        assert result == average_of_middle\n\n@given(st.lists(st.integers()))\ndef test_median_sorted_output_is_same(data):\n    data_sorted = sorted(data)\n    result_unsorted = statistics.median(data)\n    result_sorted = statistics.median(data_sorted)\n    assert result_unsorted == result_sorted\n\n@given(st.lists(st.integers(), min_size=0))\ndef test_median_empty_input_raises_statistics_error(data):\n    if len(data) == 0:\n        try:\n            statistics.median(data)\n            assert False, \"Expected StatisticsError\"\n        except statistics.StatisticsError:\n            pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_median_non_empty_output():\n    data = st.data().draw(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\n    result = statistics.median(data)\n    assert isinstance(result, (int, float))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_median_odd_length():\n    data = st.data().draw(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1, max_size=100))\n    if len(data) % 2 == 1:\n        sorted_data = sorted(data)\n        result = statistics.median(data)\n        assert result == sorted_data[len(sorted_data) // 2]\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2))\ndef test_median_even_length():\n    data = st.data().draw(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\n    if len(data) % 2 == 0:\n        sorted_data = sorted(data)\n        result = statistics.median(data)\n        assert result == (sorted_data[len(sorted_data) // 2 - 1] + sorted_data[len(sorted_data) // 2]) / 2\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_median_sorted_output():\n    data = st.data().draw(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\n    sorted_data = sorted(data)\n    result_unsorted = statistics.median(data)\n    result_sorted = statistics.median(sorted_data)\n    assert result_unsorted == result_sorted\n\n@given()\ndef test_median_empty_input():\n    try:\n        statistics.median([])\n        assert False  # Should not reach this line\n    except statistics.StatisticsError:\n        assert True  # Expected outcome\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_median_returns_number_property(data):\n    result = statistics.median(data)\n    assert isinstance(result, (int, float))\n\n@given(st.lists(st.integers(), min_size=1))\ndef test_median_odd_length_property(data):\n    if len(data) % 2 == 1:\n        sorted_data = sorted(data)\n        mid_index = len(sorted_data) // 2\n        result = statistics.median(sorted_data)\n        assert result == sorted_data[mid_index]\n\n@given(st.lists(st.integers(), min_size=2))\ndef test_median_even_length_property(data):\n    if len(data) % 2 == 0:\n        sorted_data = sorted(data)\n        mid_index1 = len(sorted_data) // 2 - 1\n        mid_index2 = len(sorted_data) // 2\n        expected_result = (sorted_data[mid_index1] + sorted_data[mid_index2]) / 2\n        result = statistics.median(sorted_data)\n        assert result == expected_result\n\n@given(st.lists(st.integers()))\ndef test_median_sorted_property(data):\n    sorted_data = sorted(data)\n    result_unsorted = statistics.median(data)\n    result_sorted = statistics.median(sorted_data)\n    assert result_unsorted == result_sorted\n\n@given(st.lists(st.integers(), min_size=0))\ndef test_median_empty_list_property(data):\n    if len(data) == 0:\n        try:\n            statistics.median(data)\n            assert False  # Should raise an exception\n        except StatisticsError:\n            assert True  # Correct behavior\n```\n"], "properties": ["1. The median of a non-empty list of numbers should always be a number (either an integer or a float).\n\n2. If the input list has an odd number of elements, the median should be equal to the middle element when the list is sorted.\n\n3. If the input list has an even number of elements, the median should be the average of the two middle elements when the list is sorted.\n\n4. The median should remain unchanged if the input list is sorted in ascending or descending order.\n\n5. The median should raise a StatisticsError when the input list is empty."], "api_doc": "statistics.median(data)\nReturn the median (middle value) of numeric data, using the common \u201cmean of middle two\u201d method. If data is empty, StatisticsError is raised. data can be a sequence or iterable.\n\nThe median is a robust measure of central location and is less affected by the presence of outliers. When the number of data points is odd, the middle data point is returned:\n\n>>>\nmedian([1, 3, 5])\n3\nWhen the number of data points is even, the median is interpolated by taking the average of the two middle values:\n\n>>>\nmedian([1, 3, 5, 7])\n4.0\nThis is suited for when your data is discrete, and you don\u2019t mind that the median may not be an actual data point.\n\nIf the data is ordinal (supports order operations) but not numeric (doesn\u2019t support addition), consider using median_low() or median_high() instead.", "api_code": "def median(data):\n    data = sorted(data)\n    n = len(data)\n    if n == 0:\n        raise StatisticsError(\"no median for empty data\")\n    if n % 2 == 1:\n        return data[n // 2]\n    else:\n        i = n // 2\n        return (data[i - 1] + data[i]) / 2"}
{"function_name": "statistics.linear_regression", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\nimport math\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100),\n       st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_slope_is_finite_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Check for valid input\n        slope, intercept = statistics.linear_regression(x, y)\n        assert math.isfinite(slope)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100),\n       st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_intercept_is_finite_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Check for valid input\n        slope, intercept = statistics.linear_regression(x, y)\n        assert math.isfinite(intercept)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100),\n       st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_intercept_zero_when_proportional_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Check for valid input\n        slope, intercept = statistics.linear_regression(x, y, proportional=True)\n        assert intercept == 0.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100),\n       st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_slope_changes_predictably_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Check for valid input\n        original_slope, _ = statistics.linear_regression(x, y)\n        scaled_x = [value * 2 for value in x]  # Scale x values\n        scaled_slope, _ = statistics.linear_regression(scaled_x, y)\n        assert scaled_slope == original_slope * 2  # Slope should scale by the same factor\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100),\n       st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_statistics_error_on_invalid_inputs_property(x, y):\n    if len(x) != len(y) or len(set(x)) <= 1:  # Invalid input cases\n        try:\n            statistics.linear_regression(x, y)\n            assert False  # If no exception, the test fails\n        except statistics.StatisticsError:\n            assert True  # If exception is raised, the test passes\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nimport math\n\n@given(st.lists(st.floats(), min_size=2, max_size=100), st.lists(st.floats(), min_size=2, max_size=100))\ndef test_slope_is_finite_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Ensure x is not constant\n        slope, _ = statistics.linear_regression(x, y)\n        assert math.isfinite(slope)\n\n@given(st.lists(st.floats(), min_size=2, max_size=100), st.lists(st.floats(), min_size=2, max_size=100))\ndef test_intercept_is_finite_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Ensure x is not constant\n        _, intercept = statistics.linear_regression(x, y)\n        assert math.isfinite(intercept)\n\n@given(st.lists(st.floats(), min_size=2, max_size=100), st.lists(st.floats(), min_size=2, max_size=100))\ndef test_intercept_zero_when_proportional_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Ensure x is not constant\n        slope, intercept = statistics.linear_regression(x, y, proportional=True)\n        assert intercept == 0.0\n\n@given(st.lists(st.floats(), min_size=2, max_size=100), st.lists(st.floats(), min_size=2, max_size=100))\ndef test_slope_predictable_change_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Ensure x is not constant\n        original_slope, _ = statistics.linear_regression(x, y)\n        scaled_x = [value * 2 for value in x]\n        scaled_slope, _ = statistics.linear_regression(scaled_x, y)\n        assert scaled_slope == 2 * original_slope  # Scaling x should double the slope\n\n@given(st.lists(st.floats(), min_size=2, max_size=100), st.lists(st.floats(), min_size=2, max_size=100))\ndef test_statistics_error_on_constant_x_property(x, y):\n    if len(set(x)) == 1 or len(x) != len(y):\n        try:\n            statistics.linear_regression(x, y)\n        except statistics.StatisticsError:\n            assert True\n        else:\n            assert False\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nimport math\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10)))\ndef test_slope_is_finite_property(data):\n    x, y = data\n    slope, _ = statistics.linear_regression(x, y)\n    assert not math.isnan(slope) and not math.isinf(slope)\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10)))\ndef test_intercept_is_finite_property(data):\n    x, y = data\n    _, intercept = statistics.linear_regression(x, y)\n    assert not math.isnan(intercept) and not math.isinf(intercept)\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10)))\ndef test_intercept_zero_when_proportional_property(data):\n    x, y = data\n    slope, intercept = statistics.linear_regression(x, y, proportional=True)\n    assert intercept == 0.0\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10).filter(lambda lst: len(set(lst)) > 1), \n                st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10)))\ndef test_slope_behavior_with_scaling_property(data):\n    x, y = data\n    original_slope, _ = statistics.linear_regression(x, y)\n    scaled_x = [i * 2 for i in x]\n    scaled_slope, _ = statistics.linear_regression(scaled_x, y)\n    assert scaled_slope == original_slope * 2\n\n@given(st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10).filter(lambda lst: len(set(lst)) == 1),\n                st.lists(st.floats(min_value=-1e10, max_value=1e10), min_size=2, max_size=10)))\ndef test_statistics_error_for_constant_x_property(data):\n    x = data[0]  # constant value\n    y = data[1]\n    try:\n        statistics.linear_regression(x, y)\n        assert False, \"Expected StatisticsError was not raised.\"\n    except statistics.StatisticsError:\n        pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1), \n                st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=2, unique=True)))\ndef test_slope_is_finite_property(data):\n    x, y = data\n    slope, intercept = statistics.linear_regression(x, y)\n    assert not (slope != slope or slope == float('inf') or slope == float('-inf'))\n\n@given(st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1), \n                st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=2, unique=True)))\ndef test_intercept_is_finite_property(data):\n    x, y = data\n    slope, intercept = statistics.linear_regression(x, y)\n    assert not (intercept != intercept or intercept == float('inf') or intercept == float('-inf'))\n\n@given(st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1), \n                st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=2, unique=True)))\ndef test_intercept_zero_if_proportional_property(data):\n    x, y = data\n    slope, intercept = statistics.linear_regression(x, y, proportional=True)\n    assert intercept == 0.0\n\n@given(st.lists(st.floats(min_value=1e-6, max_value=1e6), min_size=2, unique=True).filter(lambda x: len(set(x)) > 1), \n                st.lists(st.floats(min_value=1e-6, max_value=1e6), min_size=2, unique=True)))\ndef test_slope_changes_predictably_property(data):\n    x, y = data\n    original_slope, _ = statistics.linear_regression(x, y)\n    scaled_x = [i * 2 for i in x]\n    new_slope, _ = statistics.linear_regression(scaled_x, y)\n    assert new_slope == 2 * original_slope\n\n@given(st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=2, unique=True).filter(lambda x: len(set(x)) == 1), \n                st.lists(st.floats(min_value=-1e6, max_value=1e6), min_size=2, unique=True)))\ndef test_statistics_error_if_constant_x_property(data):\n    x, y = data\n    try:\n        statistics.linear_regression(x, y)\n        assert False  # Should raise an error\n    except statistics.StatisticsError:\n        assert True\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nimport math\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_slope_is_finite_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Ensure valid input\n        slope, _ = statistics.linear_regression(x, y)\n        assert math.isfinite(slope)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_intercept_is_finite_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Ensure valid input\n        _, intercept = statistics.linear_regression(x, y)\n        assert math.isfinite(intercept)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_intercept_zero_when_proportional_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Ensure valid input\n        slope, intercept = statistics.linear_regression(x, y, proportional=True)\n        assert intercept == 0.0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_slope_changes_with_scaling_property(x, y):\n    if len(x) == len(y) and len(set(x)) > 1:  # Ensure valid input\n        original_slope, _ = statistics.linear_regression(x, y)\n        scaled_x = [xi * 2 for xi in x]  # Scale x values\n        scaled_slope, _ = statistics.linear_regression(scaled_x, y)\n        assert scaled_slope == 2 * original_slope  # Slope should double\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100), \n               st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=2, max_size=100))\ndef test_statistics_error_when_constant_x_property(x, y):\n    if len(set(x)) == 1:  # Ensure x is constant\n        try:\n            statistics.linear_regression(x, y)\n            assert False  # Should raise an error\n        except statistics.StatisticsError:\n            pass  # Expected behavior\n\n# End program\n```"], "properties": ["1. The slope must be a finite number (not NaN or infinite) when both input lists have valid values and are not constant.\n2. The intercept must be a finite number (not NaN or infinite) when both input lists have valid values and are not constant.\n3. If the `proportional` parameter is set to `True`, the intercept must always equal 0.0 regardless of the input values.\n4. The slope should change in a predictable manner when the input values are altered while maintaining their relationship (e.g., scaling the x values should proportionally scale the slope).\n5. The function should raise a StatisticsError if the input lists are of unequal lengths or if the independent variable x is constant."], "api_doc": "statistics.linear_regression(x, y, /, *, proportional=False)\nReturn the slope and intercept of simple linear regression parameters estimated using ordinary least squares. Simple linear regression describes the relationship between an independent variable x and a dependent variable y in terms of this linear function:\n\ny = slope * x + intercept + noise\n\nwhere slope and intercept are the regression parameters that are estimated, and noise represents the variability of the data that was not explained by the linear regression (it is equal to the difference between predicted and actual values of the dependent variable).\n\nBoth inputs must be of the same length (no less than two), and the independent variable x cannot be constant; otherwise a StatisticsError is raised.\n\nFor example, we can use the release dates of the Monty Python films to predict the cumulative number of Monty Python films that would have been produced by 2019 assuming that they had kept the pace.\n\n>>>\nyear = [1971, 1975, 1979, 1982, 1983]\nfilms_total = [1, 2, 3, 4, 5]\nslope, intercept = linear_regression(year, films_total)\nround(slope * 2019 + intercept)\n16\nIf proportional is true, the independent variable x and the dependent variable y are assumed to be directly proportional. The data is fit to a line passing through the origin. Since the intercept will always be 0.0, the underlying linear function simplifies to:\n\ny = slope * x + noise\n\nContinuing the example from correlation(), we look to see how well a model based on major planets can predict the orbital distances for dwarf planets:\n\n>>>\nmodel = linear_regression(period_squared, dist_cubed, proportional=True)\nslope = model.slope\n\n# Dwarf planets:   Pluto,  Eris,    Makemake, Haumea, Ceres\norbital_periods = [90_560, 204_199, 111_845, 103_410, 1_680]  # days\npredicted_dist = [math.cbrt(slope * (p * p)) for p in orbital_periods]\nlist(map(round, predicted_dist))\n[5912, 10166, 6806, 6459, 414]\n\n[5_906, 10_152, 6_796, 6_450, 414]  # actual distance in million km\n[5906, 10152, 6796, 6450, 414]\nAdded in version 3.10.\n\nChanged in version 3.11: Added support for proportional.", "api_code": "def linear_regression(x, y, /, *, proportional=False):\n    n = len(x)\n    if len(y) != n:\n        raise StatisticsError('linear regression requires that both inputs have same number of data points')\n    if n < 2:\n        raise StatisticsError('linear regression requires at least two data points')\n    if not proportional:\n        xbar = fsum(x) / n\n        ybar = fsum(y) / n\n        x = [xi - xbar for xi in x]  # List because used three times below\n        y = (yi - ybar for yi in y)  # Generator because only used once below\n    sxy = sumprod(x, y) + 0.0        # Add zero to coerce result to a float\n    sxx = sumprod(x, x)\n    try:\n        slope = sxy / sxx   # equivalent to:  covariance(x, y) / variance(x)\n    except ZeroDivisionError:\n        raise StatisticsError('x is constant')\n    intercept = 0.0 if proportional else ybar - slope * xbar\n    return LinearRegression(slope=slope, intercept=intercept)"}
{"function_name": "networkx.algorithms.approximation.clique.large_clique_size", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import clique\n\n@given(st.data())\ndef test_large_clique_size_non_negative():\n    G = st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)).example()\n    k = clique.large_clique_size(G)\n    assert k >= 0  # Output must be a non-negative integer\n\n@given(st.data())\ndef test_large_clique_size_does_not_exceed_node_count():\n    G = st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)).example()\n    node_count = G.number_of_nodes()\n    k = clique.large_clique_size(G)\n    assert k <= node_count  # Output must not exceed total number of nodes\n\n@given(st.data())\ndef test_large_clique_size_empty_graph():\n    G = nx.Graph()  # Create an empty graph\n    k = clique.large_clique_size(G)\n    assert k == 0  # Output should be zero for empty graph\n\n@given(st.data())\ndef test_large_clique_size_bounded_by_max_degree():\n    G = st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)).example()\n    max_degree = max(dict(G.degree()).values(), default=0)\n    k = clique.large_clique_size(G)\n    assert k <= max_degree + 1  # Output must be less than or equal to max degree + 1\n\n@given(st.data())\ndef test_large_clique_size_disconnected_graph():\n    G = st.builds(nx.Graph, st.lists(st.tuples(st.integers(), st.integers()), min_size=1)).example()\n    components = list(nx.connected_components(G))\n    largest_clique_size = max(clique.large_clique_size(G.subgraph(comp)) for comp in components)\n    k = clique.large_clique_size(G)\n    assert k == largest_clique_size  # Output should reflect the largest clique in any component\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import large_clique_size\n\n@given(st.data())\ndef test_large_clique_size_non_negative():\n    G = st.one_of(\n        st.builds(nx.complete_graph, st.integers(min_value=1, max_value=10)),\n        st.builds(nx.empty_graph, st.integers(min_value=0, max_value=10))\n    ).example()\n    k = large_clique_size(G)\n    assert k >= 0\n\n@given(st.data())\ndef test_large_clique_size_within_node_count():\n    num_nodes = st.integers(min_value=1, max_value=1000).example()\n    G = nx.complete_graph(num_nodes)\n    k = large_clique_size(G)\n    assert k <= num_nodes\n\n@given(st.data())\ndef test_large_clique_size_empty_graph():\n    G = nx.empty_graph(0)\n    k = large_clique_size(G)\n    assert k == 0\n\n@given(st.data())\ndef test_large_clique_size_bounded_by_max_degree():\n    num_nodes = st.integers(min_value=1, max_value=10).example()\n    G = nx.random_graphs.barabasi_albert_graph(num_nodes, 1)\n    max_degree = max(dict(G.degree()).values())\n    k = large_clique_size(G)\n    assert k <= max_degree + 1\n\n@given(st.data())\ndef test_large_clique_size_connected_components():\n    G = nx.disjoint_union(nx.complete_graph(3), nx.complete_graph(4))\n    k = large_clique_size(G)\n    assert k == 4  # The largest clique is from the component with 4 nodes\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import large_clique_size\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=100))\ndef test_large_clique_size_non_negative_integer(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    clique_size = large_clique_size(G)\n    assert clique_size >= 0\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=100))\ndef test_large_clique_size_not_exceeding_node_count(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    clique_size = large_clique_size(G)\n    assert clique_size <= len(graph_nodes)\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=0))\ndef test_large_clique_size_empty_graph(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    clique_size = large_clique_size(G)\n    assert clique_size == 0\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, max_size=100))\ndef test_large_clique_size_bounded_by_max_degree(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    G.add_edges_from([(i, j) for i in range(len(graph_nodes)) for j in range(i + 1, len(graph_nodes))])\n    max_degree = max(dict(G.degree()).values())\n    clique_size = large_clique_size(G)\n    assert clique_size <= max_degree + 1\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, max_size=100))\ndef test_large_clique_size_disconnected_graph(graph_nodes):\n    G = nx.Graph()\n    G.add_nodes_from(graph_nodes)\n    # Create disconnected components\n    for i in range(0, len(graph_nodes), 2):\n        if i + 1 < len(graph_nodes):\n            G.add_edge(graph_nodes[i], graph_nodes[i + 1])\n    clique_size = large_clique_size(G)\n    assert clique_size <= max(2, len(graph_nodes) // 2)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import large_clique_size\n\n@given(st.lists(st.integers(min_value=1, max_value=100), min_size=0, max_size=100))\ndef test_large_clique_size_non_negative_integer(graph_data):\n    G = nx.Graph()\n    G.add_nodes_from(graph_data)\n    clique_size = large_clique_size(G)\n    assert isinstance(clique_size, int) and clique_size >= 0\n\n@given(st.lists(st.integers(min_value=1, max_value=100), min_size=0, max_size=100))\ndef test_large_clique_size_not_exceed_total_nodes(graph_data):\n    G = nx.Graph()\n    G.add_nodes_from(graph_data)\n    clique_size = large_clique_size(G)\n    assert clique_size <= len(graph_data)\n\n@given(st.lists(st.integers(min_value=1, max_value=100), min_size=0, max_size=0))\ndef test_large_clique_size_empty_graph(empty_graph_data):\n    G = nx.Graph()\n    clique_size = large_clique_size(G)\n    assert clique_size == 0\n\n@given(st.lists(st.integers(min_value=1, max_value=100), min_size=1, max_size=100))\ndef test_large_clique_size_bounded_by_degree(graph_data):\n    G = nx.Graph()\n    G.add_nodes_from(graph_data)\n    G.add_edges_from([(graph_data[i], graph_data[j]) for i in range(len(graph_data)) for j in range(i+1, len(graph_data))])\n    max_degree = max(dict(G.degree()).values())\n    clique_size = large_clique_size(G)\n    assert clique_size <= max_degree + 1\n\n@given(st.lists(st.integers(min_value=1, max_value=100), min_size=1, max_size=100))\ndef test_large_clique_size_disconnected_components(graph_data):\n    G = nx.Graph()\n    G.add_nodes_from(graph_data)\n    G.add_edges_from([(graph_data[i], graph_data[j]) for i in range(0, len(graph_data)//2) for j in range(len(graph_data)//2, len(graph_data))])\n    clique_size = large_clique_size(G)\n    # Since the graph is disconnected, the size should reflect the largest clique in the components.\n    # We are not asserting a specific value here as it depends on the specific connections.\n    assert clique_size >= 0\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import large_clique_size\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=100).map(nx.path_graph))\ndef test_large_clique_size_non_negative_integer(G):\n    k = large_clique_size(G)\n    assert k >= 0\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=100).map(nx.path_graph))\ndef test_large_clique_size_not_exceeding_node_count(G):\n    k = large_clique_size(G)\n    assert k <= G.number_of_nodes()\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=0, max_size=0).map(nx.path_graph))\ndef test_large_clique_size_empty_graph(G):\n    k = large_clique_size(G)\n    assert k == 0\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, max_size=100).map(nx.path_graph))\ndef test_large_clique_size_not_exceeding_max_degree(G):\n    k = large_clique_size(G)\n    max_degree = max(dict(G.degree()).values(), default=0)\n    assert k <= max_degree + 1\n\n@given(st.lists(st.integers(min_value=0, max_value=100), min_size=1, max_size=100).map(nx.connected_watts_strogatz_graph))\ndef test_large_clique_size_disconnected_components(G):\n    k = large_clique_size(G)\n    largest_clique_in_components = max(large_clique_size(G.subgraph(c)) for c in nx.connected_components(G))\n    assert k <= largest_clique_in_components\n\n# End program\n```"], "properties": ["1. The output size of the large clique must be a non-negative integer, as it represents the number of nodes in the clique.\n\n2. The output size of the large clique must not exceed the total number of nodes in the input graph \\( G \\).\n\n3. If the input graph \\( G \\) is empty (i.e., has no nodes), the output should be zero, indicating that there are no cliques.\n\n4. The output size of the large clique must be less than or equal to the maximum degree of the graph plus one, as this is a theoretical upper bound on the size of a clique.\n\n5. If the input graph \\( G \\) is disconnected (i.e., contains multiple components), the output size should reflect the largest clique found in any of the connected components of the graph."], "api_doc": "large_clique_size\nlarge_clique_size(G)[source]\nFind the size of a large clique in a graph.\n\nA clique is a subset of nodes in which each pair of nodes is adjacent. This function is a heuristic for finding the size of a large clique in the graph.\n\nParameters\n:\nG\nNetworkX graph\nReturns\n:\nk: integer\nThe size of a large clique in the graph.\n\nRaises\n:\nNetworkXNotImplemented\nIf the graph is directed or is a multigraph.\n\nSee also\n\nnetworkx.algorithms.approximation.clique.max_clique()\nA function that returns an approximate maximum clique with a guarantee on the approximation ratio.\n\nnetworkx.algorithms.clique\nFunctions for finding the exact maximum clique in a graph.\n\nNotes\n\nThis implementation is from [1]. Its worst case time complexity is \n, where n is the number of nodes in the graph and d is the maximum degree.\n\nThis function is a heuristic, which means it may work well in practice, but there is no rigorous mathematical guarantee on the ratio between the returned number and the actual largest clique size in the graph.\n\nReferences\n\n[1]\nPattabiraman, Bharath, et al. \u201cFast Algorithms for the Maximum Clique Problem on Massive Graphs with Applications to Overlapping Community Detection.\u201d Internet Mathematics 11.4-5 (2015): 421\u2013448. <https://doi.org/10.1080/15427951.2014.986778>\n\nExamples\n\nG = nx.path_graph(10)\nnx.approximation.large_clique_size(G)\n2", "api_code": "@not_implemented_for(\"directed\")\n@not_implemented_for(\"multigraph\")\n@nx._dispatchable\ndef large_clique_size(G):\n    degrees = G.degree\n\n    def _clique_heuristic(G, U, size, best_size):\n        if not U:\n            return max(best_size, size)\n        u = max(U, key=degrees)\n        U.remove(u)\n        N_prime = {v for v in G[u] if degrees[v] >= best_size}\n        return _clique_heuristic(G, U & N_prime, size + 1, best_size)\n\n    best_size = 0\n    nodes = (u for u in G if degrees[u] >= best_size)\n    for u in nodes:\n        neighbors = {v for v in G[u] if degrees[v] >= best_size}\n        best_size = _clique_heuristic(G, neighbors, 1, best_size)\n    return best_size"}
{"function_name": "datetime.datetime.combine", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, date, time, timedelta\n\n@given(st.dates(), st.times())\ndef test_datetime_combine_date_property(input_date, input_time):\n    result = datetime.combine(input_date, input_time)\n    assert result.year == input_date.year\n    assert result.month == input_date.month\n    assert result.day == input_date.day\n\n@given(st.dates(), st.times())\ndef test_datetime_combine_time_property(input_date, input_time):\n    result = datetime.combine(input_date, input_time)\n    assert result.hour == input_time.hour\n    assert result.minute == input_time.minute\n    assert result.second == input_time.second\n    assert result.microsecond == input_time.microsecond\n\n@given(st.dates(), st.times(tzinfo=st.tzinfo()))\ndef test_datetime_combine_tzinfo_argument_property(input_date, input_time):\n    tzinfo = input_time.tzinfo\n    result = datetime.combine(input_date, input_time, tzinfo=tzinfo)\n    assert result.tzinfo == tzinfo\n\n@given(st.datetimes(), st.times())\ndef test_datetime_combine_datetime_argument_property(input_datetime, input_time):\n    result = datetime.combine(input_datetime, input_time)\n    assert result.year == input_datetime.year\n    assert result.month == input_datetime.month\n    assert result.day == input_datetime.day\n\n@given(st.dates(), st.times(tzinfo=st.one_of(st.none(), st.tzinfo())))\ndef test_datetime_combine_timezone_aware_property(input_date, input_time):\n    tzinfo = input_time.tzinfo if input_time.tzinfo is not None else None\n    result = datetime.combine(input_date, input_time, tzinfo=tzinfo)\n    if tzinfo is not None:\n        assert result.tzinfo is not None\n    else:\n        assert result.tzinfo is None\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates(), st.times())\ndef test_datetime_combine_date_components_property(date, time):\n    result = datetime.datetime.combine(date, time)\n    assert result.year == date.year\n    assert result.month == date.month\n    assert result.day == date.day\n\n@given(st.dates(), st.times())\ndef test_datetime_combine_time_components_property(date, time):\n    result = datetime.datetime.combine(date, time)\n    assert result.hour == time.hour\n    assert result.minute == time.minute\n    assert result.second == time.second\n    assert result.microsecond == time.microsecond\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.times()))\ndef test_datetime_combine_tzinfo_property(date, time, tzinfo):\n    result = datetime.datetime.combine(date, time, tzinfo)\n    if tzinfo is None:\n        assert result.tzinfo == time.tzinfo\n    else:\n        assert result.tzinfo == tzinfo\n\n@given(st.datetimes())\ndef test_datetime_combine_datetime_ignored_property(dt):\n    result = datetime.datetime.combine(dt.date(), dt.time())\n    assert result.year == dt.year\n    assert result.month == dt.month\n    assert result.day == dt.day\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.just(datetime.timezone.utc)))\ndef test_datetime_combine_timezone_aware_property(date, time, tzinfo):\n    result = datetime.datetime.combine(date, time, tzinfo)\n    if tzinfo is None and time.tzinfo is None:\n        assert result.tzinfo is None\n    elif tzinfo is not None:\n        assert result.tzinfo is not None\n    else:\n        assert result.tzinfo == time.tzinfo\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, date, time, timedelta\n\n@given(st.dates(), st.times())\ndef test_datetime_combine_date_components_property(d, t):\n    combined = datetime.combine(d, t)\n    assert combined.year == d.year\n    assert combined.month == d.month\n    assert combined.day == d.day\n\n@given(st.dates(), st.times())\ndef test_datetime_combine_time_components_property(d, t):\n    combined = datetime.combine(d, t)\n    assert combined.hour == t.hour\n    assert combined.minute == t.minute\n    assert combined.second == t.second\n    assert combined.microsecond == t.microsecond\n\n@given(st.dates(), st.times(tzinfo=st.just(None)), st.tzinfos())\ndef test_datetime_combine_tzinfo_property(d, t, tzinfo):\n    combined = datetime.combine(d, t, tzinfo=tzinfo)\n    if tzinfo is None:\n        assert combined.tzinfo == t.tzinfo\n    else:\n        assert combined.tzinfo == tzinfo\n\n@given(st.datetimes())\ndef test_datetime_combine_ignore_time_components_property(dt):\n    d = dt.date()\n    t = dt.time()\n    combined = datetime.combine(d, t)\n    assert combined.year == d.year\n    assert combined.month == d.month\n    assert combined.day == d.day\n    assert combined.hour == t.hour\n    assert combined.minute == t.minute\n    assert combined.second == t.second\n    assert combined.microsecond == t.microsecond\n\n@given(st.dates(), st.times(tzinfo=st.just(None)), st.tzinfos())\ndef test_datetime_combine_timezone_aware_property(d, t, tzinfo):\n    combined = datetime.combine(d, t, tzinfo)\n    if tzinfo is not None:\n        assert combined.tzinfo is not None\n    else:\n        assert combined.tzinfo is None\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.just(datetime.timezone.utc)))\ndef test_date_components_property(date, time, tzinfo):\n    result = datetime.datetime.combine(date, time, tzinfo)\n    assert result.year == date.year\n    assert result.month == date.month\n    assert result.day == date.day\n\n@given(st.dates(), st.times())\ndef test_time_components_property(date, time):\n    result = datetime.datetime.combine(date, time)\n    assert result.hour == time.hour\n    assert result.minute == time.minute\n    assert result.second == time.second\n    assert result.microsecond == time.microsecond\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.just(datetime.timezone.utc)))\ndef test_tzinfo_from_time_property(date, time, tzinfo):\n    result = datetime.datetime.combine(date, time, tzinfo)\n    if tzinfo is None:\n        assert result.tzinfo == time.tzinfo\n    else:\n        assert result.tzinfo == tzinfo\n\n@given(st.datetimes(), st.times(), st.one_of(st.none(), st.just(datetime.timezone.utc)))\ndef test_datetime_object_ignored_time_property(datetime_obj, time, tzinfo):\n    result = datetime.datetime.combine(datetime_obj, time, tzinfo)\n    assert result.year == datetime_obj.year\n    assert result.month == datetime_obj.month\n    assert result.day == datetime_obj.day\n    assert result.hour == time.hour\n    assert result.minute == time.minute\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.just(datetime.timezone.utc)))\ndef test_timezone_aware_property(date, time, tzinfo):\n    result = datetime.datetime.combine(date, time, tzinfo)\n    if tzinfo is not None:\n        assert result.tzinfo is not None\n    else:\n        assert result.tzinfo is None\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport datetime\n\n@given(st.dates(), st.times())\ndef test_datetime_combine_date_components_property(date, time):\n    result = datetime.datetime.combine(date, time)\n    assert result.year == date.year\n    assert result.month == date.month\n    assert result.day == date.day\n\n@given(st.dates(), st.times())\ndef test_datetime_combine_time_components_property(date, time):\n    result = datetime.datetime.combine(date, time)\n    assert result.hour == time.hour\n    assert result.minute == time.minute\n    assert result.second == time.second\n    assert result.microsecond == time.microsecond\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.timezones()))\ndef test_datetime_combine_tzinfo_property(date, time, tzinfo):\n    result = datetime.datetime.combine(date, time, tzinfo=tzinfo)\n    if tzinfo is None:\n        assert result.tzinfo == time.tzinfo\n    else:\n        assert result.tzinfo == tzinfo\n\n@given(st.datetimes(), st.times())\ndef test_datetime_combine_datetime_argument_property(dt, time):\n    result = datetime.datetime.combine(dt, time)\n    assert result.year == dt.year\n    assert result.month == dt.month\n    assert result.day == dt.day\n\n@given(st.dates(), st.times(), st.one_of(st.none(), st.timezones()))\ndef test_datetime_combine_timezone_aware_property(date, time, tzinfo):\n    result = datetime.datetime.combine(date, time, tzinfo=tzinfo)\n    if tzinfo is not None:\n        assert result.tzinfo is not None\n    else:\n        assert result.tzinfo is None\n# End program\n```"], "properties": ["1. The output datetime object's year, month, and day must match the year, month, and day of the input date object.\n\n2. The output datetime object's hour, minute, second, and microsecond must match the corresponding properties of the input time object.\n\n3. If the tzinfo argument is not provided, the tzinfo attribute of the output datetime object must be equal to the tzinfo attribute of the input time object.\n\n4. If the input date argument is a datetime object, the output must retain the date components of that datetime object, disregarding any time components or tzinfo attributes from it.\n\n5. The output datetime object must be timezone-aware if the tzinfo argument is provided, otherwise it must be timezone-naive if the input time object is timezone-naive."], "api_doc": "classmethod datetime.combine(date, time, tzinfo=time.tzinfo)\nReturn a new datetime object whose date components are equal to the given date object\u2019s, and whose time components are equal to the given time object\u2019s. If the tzinfo argument is provided, its value is used to set the tzinfo attribute of the result, otherwise the tzinfo attribute of the time argument is used. If the date argument is a datetime object, its time components and tzinfo attributes are ignored.\n\nFor any datetime object d, d == datetime.combine(d.date(), d.time(), d.tzinfo).\n\nChanged in version 3.6: Added the tzinfo argument.", "api_code": "@classmethod\ndef combine(cls, date, time, tzinfo=True):\n    if not isinstance(date, _date_class):\n        raise TypeError(\"date argument must be a date instance\")\n    if not isinstance(time, _time_class):\n        raise TypeError(\"time argument must be a time instance\")\n    if tzinfo is True:\n        tzinfo = time.tzinfo\n    return cls(date.year, date.month, date.day,\n                time.hour, time.minute, time.second, time.microsecond,\n                tzinfo, fold=time.fold)"}
{"function_name": "decimal.Decimal.fma", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.floats(min_value=-1e+100, max_value=1e+100), \n       st.floats(min_value=-1e+100, max_value=1e+100), \n       st.floats(min_value=-1e+100, max_value=1e+100))\ndef test_fma_output_equivalence_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    assert result == Decimal(a) * Decimal(b) + Decimal(c)\n\n@given(st.floats(min_value=-1e+100, max_value=1e+100), \n       st.floats(min_value=-1e+100, max_value=1e+100))\ndef test_fma_output_type_property(a, b):\n    result = Decimal(a).fma(Decimal(b), Decimal(0))\n    assert isinstance(result, Decimal)\n\n@given(st.floats(min_value=-1e+100, max_value=1e+100))\ndef test_fma_with_zero_other_property(a):\n    result = Decimal(a).fma(0, Decimal(5))\n    assert result == Decimal(a) + Decimal(5)\n\n@given(st.floats(min_value=-1e+100, max_value=1e+100))\ndef test_fma_with_zero_third_property(a):\n    result = Decimal(a).fma(Decimal(3), 0)\n    assert result == Decimal(a) * Decimal(3)\n\n@given(st.floats(min_value=-1e+100, max_value=1e+100), \n       st.floats(min_value=-1e+100, max_value=1e+100))\ndef test_fma_commutativity_property(a, b):\n    result1 = Decimal(a).fma(Decimal(b), Decimal(5))\n    result2 = Decimal(b).fma(Decimal(a), Decimal(5))\n    assert result1 == result2\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fma_output_equals_product_plus_third_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    expected = Decimal(a) * Decimal(b) + Decimal(c)\n    assert result == expected\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fma_output_is_decimal_property(a, b, c):\n    result = Decimal(a).fma(Decimal(b), Decimal(c))\n    assert isinstance(result, Decimal)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fma_zero_other_property(a):\n    result = Decimal(a).fma(Decimal(0), Decimal(5))\n    assert result == Decimal(a) + Decimal(5)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fma_zero_third_property(a):\n    result = Decimal(a).fma(Decimal(3), Decimal(0))\n    assert result == Decimal(a) * Decimal(3)\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_fma_commutative_property(b, c):\n    a = Decimal(2)  # Fixed value to test commutativity\n    result1 = Decimal(a).fma(Decimal(b), Decimal(c))\n    result2 = Decimal(a).fma(Decimal(c), Decimal(b))\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n# Set a high precision context to handle large inputs and avoid overflow\ngetcontext().prec = 50\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_output_equivalence_property(x, y, z):\n    \"\"\"The output should be equal to self * other + third.\"\"\"\n    result = Decimal(x).fma(Decimal(y), Decimal(z))\n    expected = Decimal(x) * Decimal(y) + Decimal(z)\n    assert result == expected\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_output_type_property(x, y, z):\n    \"\"\"The output should be a Decimal type.\"\"\"\n    result = Decimal(x).fma(Decimal(y), Decimal(z))\n    assert isinstance(result, Decimal)\n\n@given(st.decimals(), st.decimals())\ndef test_fma_zero_other_property(x, z):\n    \"\"\"If other is zero, the output should equal self + third.\"\"\"\n    result = Decimal(x).fma(Decimal(0), Decimal(z))\n    expected = Decimal(x) + Decimal(z)\n    assert result == expected\n\n@given(st.decimals(), st.decimals())\ndef test_fma_zero_third_property(x, y):\n    \"\"\"If third is zero, the output should equal self * other.\"\"\"\n    result = Decimal(x).fma(Decimal(y), Decimal(0))\n    expected = Decimal(x) * Decimal(y)\n    assert result == expected\n\n@given(st.decimals(), st.decimals(), st.decimals())\ndef test_fma_commutative_property(y, z, x):\n    \"\"\"The output should be consistent with the commutative property of addition.\"\"\"\n    result1 = Decimal(x).fma(Decimal(y), Decimal(z))\n    result2 = Decimal(x).fma(Decimal(z), Decimal(y))\n    assert result1 == result2\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_output_equals_product_plus_third_property(self, other, third):\n    result = self.fma(other, third)\n    assert result == self * other + third\n# End program\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_output_type_property(self, other, third):\n    result = self.fma(other, third)\n    assert isinstance(result, Decimal)\n# End program\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_zero_other_property(self, self, third):\n    result = self.fma(0, third)\n    assert result == self + third\n# End program\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_zero_third_property(self, self, other):\n    result = self.fma(other, 0)\n    assert result == self * other\n# End program\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_commutative_property(self, self, other):\n    third = Decimal('1')  # Using a constant third for clarity\n    result1 = self.fma(other, third)\n    result2 = self.fma(third, other)\n    assert result1 == result2\n# End program\n\n@given(st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False), \n       st.decimals(allow_nan=False, allow_infinity=False))\ndef test_no_rounding_property(self, self, other, third):\n    context = Decimal.getcontext()\n    original_rounding = context.rounding\n    context.rounding = 'ROUND_DOWN'  # Change rounding for the test\n    result_with_rounding = self.fma(other, third)\n    context.rounding = original_rounding  # Reset to original\n    assert result_with_rounding == self * other + third\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, getcontext\n\n# Set a high precision to handle large numbers\ngetcontext().prec = 100\n\n@given(st.decimals(min_value=-1e100, max_value=1e100), \n       st.decimals(min_value=-1e100, max_value=1e100), \n       st.decimals(min_value=-1e100, max_value=1e100))\ndef test_output_equals_product_plus_third_property(first, second, third):\n    result = Decimal(first).fma(second, third)\n    expected = Decimal(first) * Decimal(second) + Decimal(third)\n    assert result == expected\n\n@given(st.decimals(min_value=-1e100, max_value=1e100), \n       st.decimals(min_value=-1e100, max_value=1e100))\ndef test_output_is_decimal_type_property(first, second):\n    result = Decimal(first).fma(second, Decimal(0))\n    assert isinstance(result, Decimal)\n\n@given(st.decimals(min_value=-1e100, max_value=1e100))\ndef test_output_with_zero_other_property(first):\n    result = Decimal(first).fma(Decimal(0), Decimal(5))\n    expected = Decimal(first) + Decimal(5)\n    assert result == expected\n\n@given(st.decimals(min_value=-1e100, max_value=1e100))\ndef test_output_with_zero_third_property(first):\n    result = Decimal(first).fma(Decimal(3), Decimal(0))\n    expected = Decimal(first) * Decimal(3)\n    assert result == expected\n\n@given(st.decimals(min_value=-1e100, max_value=1e100), \n       st.decimals(min_value=-1e100, max_value=1e100))\ndef test_commutative_property_of_addition(first, third):\n    result1 = Decimal(first).fma(third, Decimal(0))\n    result2 = Decimal(third).fma(first, Decimal(0))\n    assert result1 == result2\n# End program\n```"], "properties": ["1. The output should be equal to the sum of the product of the first two arguments and the third argument, i.e., `self * other + third`.\n\n2. The output should be a `Decimal` type, ensuring the result maintains the precision characteristics of the `Decimal` class.\n\n3. If either `other` or `third` is zero, the output should equal `self + third` (if `other` is 0) or `self * other` (if `third` is 0).\n\n4. The output should be consistent with the commutative property of addition, meaning `fma(other, third)` should yield the same result as `fma(third, other)` when the roles of the second and third arguments are swapped.\n\n5. The output should not be affected by the rounding mode of the context, as the operation is designed to perform the multiplication without rounding the intermediate product."], "api_doc": "fma(other, third, context=None)\nFused multiply-add. Return self*other+third with no rounding of the intermediate product self*other.\n\n>>>\nDecimal(2).fma(3, 5)\nDecimal('11')", "api_code": "def fma(self, other, third, context=None):\n    if self._is_special or other._is_special:\n        if context is None:\n            context = getcontext()\n        if self._exp == 'N':\n            return context._raise_error(InvalidOperation, 'sNaN', self)\n        if other._exp == 'N':\n            return context._raise_error(InvalidOperation, 'sNaN', other)\n        if self._exp == 'n':\n            product = self\n        elif other._exp == 'n':\n            product = other\n        elif self._exp == 'F':\n            if not other:\n                return context._raise_error(InvalidOperation,\n                                            'INF * 0 in fma')\n            product = _SignedInfinity[self._sign ^ other._sign]\n        elif other._exp == 'F':\n            if not self:\n                return context._raise_error(InvalidOperation,\n                                            '0 * INF in fma')\n            product = _SignedInfinity[self._sign ^ other._sign]\n    else:\n        product = _dec_from_triple(self._sign ^ other._sign,\n                                    str(int(self._int) * int(other._int)),\n                                    self._exp + other._exp)\n\n    return product.__add__(third, context)"}
{"function_name": "pandas.DataFrame.sort_values", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_sorting_order_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers())],\n                                 rows=st.lists(st.record({'col1': st.one_of(st.text(), st.none()),\n                                                           'col2': st.integers()}), min_size=1, max_size=100)))\n    sorted_df = df.sort_values(by='col2')\n    assert sorted_df['col2'].is_monotonic_increasing\n\n@given(st.data())\ndef test_na_position_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers())],\n                                 rows=st.lists(st.record({'col1': st.one_of(st.text(), st.none()),\n                                                           'col2': st.integers()}), min_size=1, max_size=100)))\n    na_position = data.draw(st.one_of(st.just('first'), st.just('last')))\n    sorted_df = df.sort_values(by='col1', na_position=na_position)\n    \n    if na_position == 'first':\n        assert sorted_df['col1'].isna().all() or sorted_df['col1'].first_valid_index() != 0\n    else:\n        assert sorted_df['col1'].isna().all() or sorted_df['col1'].last_valid_index() != len(sorted_df) - 1\n\n@given(st.data())\ndef test_inplace_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers())],\n                                 rows=st.lists(st.record({'col1': st.one_of(st.text(), st.none()),\n                                                           'col2': st.integers()}), min_size=1, max_size=100)))\n\n    original_df = df.copy()\n    df.sort_values(by='col2', inplace=True)\n    assert df is original_df  # In-place modification should not return a new DataFrame\n    assert df.equals(original_df.sort_values(by='col2'))  # Should match the sorted version\n\n@given(st.data())\ndef test_column_retention_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers())],\n                                 rows=st.lists(st.record({'col1': st.one_of(st.text(), st.none()),\n                                                           'col2': st.integers()}), min_size=1, max_size=100)))\n    sorted_df = df.sort_values(by='col1')\n    assert all(col in sorted_df.columns for col in df.columns)\n\n@given(st.data())\ndef test_multiple_column_sorting_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers()),\n                                          st.column('col3', elements=st.text())],\n                                 rows=st.lists(st.record({'col1': st.one_of(st.text(), st.none()),\n                                                           'col2': st.integers(),\n                                                           'col3': st.text()}), min_size=1, max_size=100)))\n    sorted_df = df.sort_values(by=['col1', 'col2'])\n    assert sorted_df['col1'].is_monotonic_increasing or sorted_df['col1'].isna().all()\n    for group in sorted_df.groupby('col1'):\n        assert group[1]['col2'].is_monotonic_increasing\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n# Property 1: Test sorted output based on specified columns\n@given(st.data())\ndef test_sort_values_sorted_order_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                           st.column('col2', elements=st.integers())],\n                                 index=st.integers(min_value=1, max_value=100)))\n    sorted_df = df.sort_values(by='col2')\n    assert sorted_df['col2'].is_monotonic\n\n# Property 2: Test NaN positioning based on na_position parameter\n@given(st.data())\ndef test_sort_values_nan_position_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                           st.column('col2', elements=st.integers())],\n                                 index=st.integers(min_value=1, max_value=100)))\n    na_position_first_df = df.sort_values(by='col1', na_position='first')\n    na_position_last_df = df.sort_values(by='col1', na_position='last')\n    \n    assert na_position_first_df['col1'].isnull().all() == (na_position_first_df['col1'].iloc[0] is None)\n    assert na_position_last_df['col1'].isnull().all() == (na_position_last_df['col1'].iloc[-1] is None)\n\n# Property 3: Test inplace behavior\n@given(st.data())\ndef test_sort_values_inplace_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                           st.column('col2', elements=st.integers())],\n                                 index=st.integers(min_value=1, max_value=100)))\n    original_df = df.copy()\n    \n    df.sort_values(by='col2', inplace=True)\n    assert df.equals(df.sort_values(by='col2'))  # Verifying sorting worked\n    assert not original_df.equals(df)  # Original should not be equal\n\n    df = original_df  # Reset\n    sorted_df = df.sort_values(by='col2', inplace=False)\n    assert not original_df.equals(sorted_df)  # New DataFrame should differ\n\n# Property 4: Test output retains original columns and types\n@given(st.data())\ndef test_sort_values_retain_columns_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                           st.column('col2', elements=st.floats()),\n                                           st.column('col3', elements=st.dates())],\n                                 index=st.integers(min_value=1, max_value=100)))\n    sorted_df = df.sort_values(by='col2')\n    \n    assert set(sorted_df.columns) == set(df.columns)\n    assert all(sorted_df[col].dtype == df[col].dtype for col in df.columns)\n\n# Property 5: Test hierarchical sorting by multiple columns\n@given(st.data())\ndef test_sort_values_hierarchical_sort_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.text()),\n                                           st.column('col2', elements=st.integers()),\n                                           st.column('col3', elements=st.integers())],\n                                 index=st.integers(min_value=1, max_value=100)))\n    sorted_df = df.sort_values(by=['col1', 'col2'])\n    \n    assert all(sorted_df['col1'].iloc[i] <= sorted_df['col1'].iloc[i + 1] for i in range(len(sorted_df) - 1))\n    assert all(sorted_df.loc[sorted_df['col1'] == sorted_df['col1'].iloc[i], 'col2'].iloc[j] <= sorted_df.loc[sorted_df['col1'] == sorted_df['col1'].iloc[i], 'col2'].iloc[j + 1] \n           for i in range(len(sorted_df) - 1) for j in range(len(sorted_df.loc[sorted_df['col1'] == sorted_df['col1'].iloc[i]]) - 1))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_sorted_order_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers())],\n                                 rows=st.lists(st.tuples(), min_size=1, max_size=1000),\n                                 index=False))\n    sorted_df = df.sort_values(by='col2')\n    assert sorted_df['col2'].is_monotonic_increasing\n\n@given(st.data())\ndef test_nan_position_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers())],\n                                 rows=st.lists(st.tuples(), min_size=1, max_size=1000),\n                                 index=False))\n    sorted_with_nan_last = df.sort_values(by='col2', na_position='last')\n    assert not sorted_with_nan_last['col2'].isnull().any() or sorted_with_nan_last['col2'].isnull().all()\n\n@given(st.data())\ndef test_inplace_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers())],\n                                 rows=st.lists(st.tuples(), min_size=1, max_size=1000),\n                                 index=False))\n    original_df = df.copy()\n    df.sort_values(by='col2', inplace=True)\n    assert df.equals(original_df) is False\n\n@given(st.data())\ndef test_column_types_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.none())),\n                                          st.column('col2', elements=st.integers()),\n                                          st.column('col3', elements=st.floats())],\n                                 rows=st.lists(st.tuples(), min_size=1, max_size=1000),\n                                 index=False))\n    sorted_df = df.sort_values(by='col2')\n    assert sorted_df.dtypes.equals(df.dtypes)\n\n@given(st.data())\ndef test_multi_column_sort_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.text()),\n                                          st.column('col2', elements=st.integers())],\n                                 rows=st.lists(st.tuples(), min_size=1, max_size=1000),\n                                 index=False))\n    sorted_df = df.sort_values(by=['col1', 'col2'])\n    assert all(sorted_df['col1'].iloc[i] <= sorted_df['col1'].iloc[i + 1] for i in range(len(sorted_df) - 1))\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_sorted_order_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', st.one_of(st.text(), st.none())),\n        st.column('col2', st.integers(min_value=0, max_value=100)),\n    ], rows=st.lists(st.fixed_dictionaries({\n        'col1': st.one_of(st.text(), st.none()),\n        'col2': st.integers(min_value=0, max_value=100),\n    }))))\n    \n    sorted_df = df.sort_values(by='col1')\n    assert sorted_df['col1'].is_monotonic_increasing\n\n@given(st.data())\ndef test_na_position_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', st.one_of(st.text(), st.none())),\n        st.column('col2', st.integers(min_value=0, max_value=100)),\n    ], rows=st.lists(st.fixed_dictionaries({\n        'col1': st.one_of(st.text(), st.none()),\n        'col2': st.integers(min_value=0, max_value=100),\n    }))))\n    \n    sorted_df_last = df.sort_values(by='col1', na_position='last')\n    assert sorted_df_last['col1'].isna().sum() == df['col1'].isna().sum()\n    assert sorted_df_last['col1'].iloc[-1] is np.nan\n\n@given(st.data())\ndef test_inplace_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', st.one_of(st.text(), st.none())),\n        st.column('col2', st.integers(min_value=0, max_value=100)),\n    ], rows=st.lists(st.fixed_dictionaries({\n        'col1': st.one_of(st.text(), st.none()),\n        'col2': st.integers(min_value=0, max_value=100),\n    }))))\n    \n    original_df = df.copy()\n    df.sort_values(by='col1', inplace=True)\n    assert df.equals(df.sort_values(by='col1', inplace=False))\n    assert not original_df.equals(df)\n\n@given(st.data())\ndef test_column_retention_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', st.one_of(st.text(), st.none())),\n        st.column('col2', st.integers(min_value=0, max_value=100)),\n    ], rows=st.lists(st.fixed_dictionaries({\n        'col1': st.one_of(st.text(), st.none()),\n        'col2': st.integers(min_value=0, max_value=100),\n    }))))\n    \n    sorted_df = df.sort_values(by='col1')\n    assert set(df.columns) == set(sorted_df.columns)\n\n@given(st.data())\ndef test_multiple_column_sort_property(data):\n    df = data.draw(st.dataframes(columns=[\n        st.column('col1', st.one_of(st.text(), st.none())),\n        st.column('col2', st.integers(min_value=0, max_value=100)),\n        st.column('col3', st.integers(min_value=0, max_value=100)),\n    ], rows=st.lists(st.fixed_dictionaries({\n        'col1': st.one_of(st.text(), st.none()),\n        'col2': st.integers(min_value=0, max_value=100),\n        'col3': st.integers(min_value=0, max_value=100),\n    }))))\n    \n    sorted_df = df.sort_values(by=['col1', 'col2'])\n    assert all(sorted_df['col1'].is_monotonic_increasing)\n    assert all(sorted_df.groupby('col1')['col2'].is_monotonic_increasing)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport pandas as pd\nimport numpy as np\n\n@given(st.data())\ndef test_sorted_order_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())), \n                                           st.column('col2', elements=st.integers())]))\n    sorted_df = df.sort_values(by='col1')\n    assert sorted_df['col1'].is_monotonic_increasing\n\n@given(st.data())\ndef test_na_position_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())), \n                                           st.column('col2', elements=st.integers())]))\n    sorted_df_last = df.sort_values(by='col1', na_position='last')\n    sorted_df_first = df.sort_values(by='col1', na_position='first')\n    assert sorted_df_last['col1'].isnull().sum() == df['col1'].isnull().sum()\n    assert sorted_df_first['col1'].isnull().sum() == df['col1'].isnull().sum()\n\n@given(st.data())\ndef test_inplace_modification_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())), \n                                           st.column('col2', elements=st.integers())]))\n    original_df = df.copy()\n    df.sort_values(by='col1', inplace=True)\n    assert df.equals(original_df) == False  # Original DataFrame should be modified\n\n@given(st.data())\ndef test_column_retention_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.one_of(st.text(), st.nothing())), \n                                           st.column('col2', elements=st.integers())]))\n    sorted_df = df.sort_values(by='col1')\n    assert set(sorted_df.columns) == set(df.columns)  # All original columns should be retained\n\n@given(st.data())\ndef test_multiple_columns_sort_property(data):\n    df = data.draw(st.dataframes(columns=[st.column('col1', elements=st.text()), \n                                           st.column('col2', elements=st.integers()), \n                                           st.column('col3', elements=st.integers())]))\n    sorted_df = df.sort_values(by=['col1', 'col2'])\n    assert sorted_df['col1'].is_monotonic_increasing  # Check primary sort order\n    assert all(sorted_df['col1'].iloc[i] <= sorted_df['col1'].iloc[i+1] for i in range(len(sorted_df)-1))  # Secondary sort order check\n\n# End program\n```"], "properties": ["1. The output DataFrame should be sorted in the specified order based on the values of the specified columns or index levels, maintaining the relative order of rows with equal values (if using a stable sorting algorithm).\n\n2. Rows containing NaN values in the sorted column(s) should be positioned according to the `na_position` parameter, either at the beginning or the end of the output DataFrame.\n\n3. If `inplace=True`, the original DataFrame should be modified directly, and the function should return None, while if `inplace=False`, the original DataFrame should remain unchanged, and a new sorted DataFrame should be returned.\n\n4. The output DataFrame should retain all original columns and their respective data types, regardless of the sorting operation performed.\n\n5. When sorting by multiple columns, the output should reflect the hierarchical sort order defined by the list of columns, where the first column in the list takes precedence over subsequent columns for sorting."], "api_doc": "pandas.DataFrame.sort_values\nDataFrame.sort_values(by, *, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]\nSort by the values along either axis.\n\nParameters\n:\nby\nstr or list of str\nName or list of names to sort by.\n\nif axis is 0 or \u2018index\u2019 then by may contain index levels and/or column labels.\n\nif axis is 1 or \u2018columns\u2019 then by may contain column levels and/or index labels.\n\naxis\n\u201c{0 or \u2018index\u2019, 1 or \u2018columns\u2019}\u201d, default 0\nAxis to be sorted.\n\nascending\nbool or list of bool, default True\nSort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.\n\ninplace\nbool, default False\nIf True, perform operation in-place.\n\nkind\n{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.\n\nna_position\n{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\nPuts NaNs at the beginning if first; last puts NaNs at the end.\n\nignore_index\nbool, default False\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nkey\ncallable, optional\nApply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.\n\nReturns\n:\nDataFrame or None\nDataFrame with sorted values or None if inplace=True.\n\nSee also\n\nDataFrame.sort_index\nSort a DataFrame by the index.\n\nSeries.sort_values\nSimilar method for a Series.\n\nExamples\n\ndf = pd.DataFrame({\n    'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n    'col2': [2, 1, 9, 8, 7, 4],\n    'col3': [0, 1, 9, 4, 2, 3],\n    'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n})\ndf\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nSort by col1\n\ndf.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort by multiple columns\n\ndf.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort Descending\n\ndf.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D\nPutting NAs first\n\ndf.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\nSorting with a key function\n\ndf.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nNatural sort with the key argument, using the natsort <https://github.com/SethMMorton/natsort> package.\n\ndf = pd.DataFrame({\n   \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n   \"value\": [10, 20, 30, 40, 50]\n})\ndf\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\nfrom natsort import index_natsorted\ndf.sort_values(\n    by=\"time\",\n    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n)\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20", "api_code": "def sort_values(\n    self,\n    by: IndexLabel,\n    *,\n    axis: Axis = 0,\n    ascending: bool | list[bool] | tuple[bool, ...] = True,\n    inplace: bool = False,\n    kind: SortKind = \"quicksort\",\n    na_position: str = \"last\",\n    ignore_index: bool = False,\n    key: ValueKeyFunc | None = None,\n) -> DataFrame | None:\n    inplace = validate_bool_kwarg(inplace, \"inplace\")\n    axis = self._get_axis_number(axis)\n    ascending = validate_ascending(ascending)\n    if not isinstance(by, list):\n        by = [by]\n    # error: Argument 1 to \"len\" has incompatible type \"Union[bool, List[bool]]\";\n    # expected \"Sized\"\n    if is_sequence(ascending) and (\n        len(by) != len(ascending)  # type: ignore[arg-type]\n    ):\n        # error: Argument 1 to \"len\" has incompatible type \"Union[bool,\n        # List[bool]]\"; expected \"Sized\"\n        raise ValueError(\n            f\"Length of ascending ({len(ascending)})\"  # type: ignore[arg-type]\n            f\" != length of by ({len(by)})\"\n        )\n    if len(by) > 1:\n        keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n\n        # need to rewrap columns in Series to apply key function\n        if key is not None:\n            # error: List comprehension has incompatible type List[Series];\n            # expected List[ndarray]\n            keys = [\n                Series(k, name=name)  # type: ignore[misc]\n                for (k, name) in zip(keys, by)\n            ]\n\n        indexer = lexsort_indexer(\n            keys, orders=ascending, na_position=na_position, key=key\n        )\n    elif len(by):\n        # len(by) == 1\n\n        k = self._get_label_or_level_values(by[0], axis=axis)\n\n        # need to rewrap column in Series to apply key function\n        if key is not None:\n            # error: Incompatible types in assignment (expression has type\n            # \"Series\", variable has type \"ndarray\")\n            k = Series(k, name=by[0])  # type: ignore[assignment]\n\n        if isinstance(ascending, (tuple, list)):\n            ascending = ascending[0]\n\n        indexer = nargsort(\n            k, kind=kind, ascending=ascending, na_position=na_position, key=key\n        )\n    else:\n        if inplace:\n            return self._update_inplace(self)\n        else:\n            return self.copy(deep=None)\n\n    if is_range_indexer(indexer, len(indexer)):\n        result = self.copy(deep=(not inplace and not using_copy_on_write()))\n        if ignore_index:\n            result.index = default_index(len(result))\n\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result\n\n    new_data = self._mgr.take(\n        indexer, axis=self._get_block_manager_axis(axis), verify=False\n    )\n\n    if ignore_index:\n        new_data.set_axis(\n            self._get_block_manager_axis(axis), default_index(len(indexer))\n        )\n\n    result = self._constructor_from_mgr(new_data, axes=new_data.axes)\n    if inplace:\n        return self._update_inplace(result)\n    else:\n        return result.__finalize__(self, method=\"sort_values\")"}
{"function_name": "networkx.algorithms.assortativity.average_neighbor_degree", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.assortativity import average_neighbor_degree\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1, max_size=10)))\n    nodes = list(G.nodes())\n    output = average_neighbor_degree(G)\n    assert all(node in output for node in nodes)\n\n@given(st.data())\ndef test_average_degree_non_negative_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1, max_size=10)))\n    output = average_neighbor_degree(G)\n    assert all(degree >= 0 for degree in output.values())\n\n@given(st.data())\ndef test_average_degree_symmetric_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=2, max_size=10)))\n    output = average_neighbor_degree(G)\n    for u, v in G.edges():\n        assert output[u] == output[v] if u in G.neighbors(v) else True\n\n@given(st.data())\ndef test_in_neighbors_average_degree_zero_property(data):\n    G = data.draw(st.builds(nx.DiGraph, nodes=st.lists(st.integers(), min_size=1, max_size=10)))\n    output = average_neighbor_degree(G, source='in')\n    for node in G.nodes():\n        if len(list(G.predecessors(node))) == 0:\n            assert output[node] == 0\n\n@given(st.data())\ndef test_consistency_of_output_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1, max_size=10)))\n    output1 = average_neighbor_degree(G)\n    output2 = average_neighbor_degree(G)\n    assert output1 == output2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.assortativity import average_neighbor_degree\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    G = data.draw(st.one_of(st.builds(nx.Graph), st.builds(nx.DiGraph)))\n    nodes = list(G.nodes)\n    output = average_neighbor_degree(G)\n    assert all(node in output for node in nodes)\n\n@given(st.data())\ndef test_average_degree_non_negative_property(data):\n    G = data.draw(st.one_of(st.builds(nx.Graph), st.builds(nx.DiGraph)))\n    output = average_neighbor_degree(G)\n    assert all(value >= 0 for value in output.values())\n\n@given(st.data())\ndef test_average_degree_symmetry_property(data):\n    G = data.draw(st.builds(nx.Graph))\n    if G.number_of_edges() > 0:\n        output = average_neighbor_degree(G)\n        for node in G.nodes:\n            for neighbor in G.neighbors(node):\n                assert output[node] == output[neighbor]\n\n@given(st.data())\ndef test_average_degree_zero_for_no_predecessors_property(data):\n    G = nx.DiGraph()\n    G.add_nodes_from(range(5))\n    output = average_neighbor_degree(G, source='in')\n    for node in G.nodes:\n        if G.in_degree(node) == 0:\n            assert output[node] == 0.0\n\n@given(st.data())\ndef test_consistent_output_property(data):\n    G = data.draw(st.one_of(st.builds(nx.Graph), st.builds(nx.DiGraph)))\n    params = {\n        'source': data.draw(st.sampled_from(['in', 'out', 'in+out'])),\n        'target': data.draw(st.sampled_from(['in', 'out', 'in+out'])),\n    }\n    output1 = average_neighbor_degree(G, **params)\n    output2 = average_neighbor_degree(G, **params)\n    assert output1 == output2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.assortativity import average_neighbor_degree\n\n@given(st.data())\ndef test_output_contains_all_nodes_property(data):\n    # Generate a random directed graph\n    G = nx.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=50)), \n                             directed=True)\n    result = average_neighbor_degree(G)\n    for node in G.nodes:\n        assert node in result\n\n@given(st.data())\ndef test_average_degree_non_negative_property(data):\n    # Generate a random directed graph\n    G = nx.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=50)), \n                             directed=True)\n    result = average_neighbor_degree(G)\n    for avg_degree in result.values():\n        assert avg_degree >= 0\n\n@given(st.data())\ndef test_average_degree_symmetry_property(data):\n    # Generate a random undirected graph\n    G = nx.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=50)), \n                                 directed=False)\n    result = average_neighbor_degree(G)\n    for node in G.nodes:\n        neighbors = list(G.neighbors(node))\n        for neighbor in neighbors:\n            assert result[node] == result[neighbor]\n\n@given(st.data())\ndef test_zero_average_degree_for_no_predecessors_property(data):\n    # Create a small directed graph with isolated nodes\n    G = nx.DiGraph()\n    G.add_nodes_from(range(5))\n    result = average_neighbor_degree(G, source=\"in\")\n    for node in G.nodes:\n        assert result[node] == 0.0\n\n@given(st.data())\ndef test_consistency_of_results_property(data):\n    # Generate a random directed graph\n    G = nx.gnp_random_graph(data.draw(st.integers(min_value=1, max_value=50)), \n                             directed=True)\n    result1 = average_neighbor_degree(G)\n    result2 = average_neighbor_degree(G)\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.assortativity import average_neighbor_degree\n\n# Property 1: The output dictionary should contain an entry for each node specified in the `nodes` parameter.\n@given(G=st.builds(nx.path_graph, st.integers(min_value=1, max_value=10)))\ndef test_output_contains_specified_nodes_property(G):\n    nodes = list(G.nodes())\n    avg_degree = average_neighbor_degree(G, nodes=nodes)\n    assert all(node in avg_degree for node in nodes)\n\n# Property 2: The average degree values in the output dictionary should be non-negative numbers.\n@given(G=st.builds(nx.path_graph, st.integers(min_value=1, max_value=10)))\ndef test_average_degree_non_negative_property(G):\n    avg_degree = average_neighbor_degree(G)\n    assert all(value >= 0 for value in avg_degree.values())\n\n# Property 3: For undirected graphs, the average neighbor degree should be symmetric.\n@given(G=st.builds(nx.path_graph, st.integers(min_value=1, max_value=10)))\ndef test_average_neighbor_degree_symmetric_property(G):\n    avg_degree = average_neighbor_degree(G)\n    for node in G.nodes():\n        for neighbor in G.neighbors(node):\n            assert avg_degree[node] == avg_degree[neighbor]\n\n# Property 4: For directed graphs and source set to \"in\", nodes with no predecessors should have an average neighbor degree of zero.\n@given(G=st.builds(nx.DiGraph, st.integers(min_value=1, max_value=10)))\ndef test_no_predecessors_zero_average_degree_property(G):\n    avg_degree = average_neighbor_degree(G, source=\"in\")\n    for node in G.nodes():\n        if G.in_degree(node) == 0:\n            assert avg_degree[node] == 0.0\n\n# Property 5: The average neighbor degree should be consistent when called multiple times with the same parameters.\n@given(G=st.builds(nx.path_graph, st.integers(min_value=1, max_value=10)))\ndef test_average_neighbor_degree_consistency_property(G):\n    avg_degree1 = average_neighbor_degree(G)\n    avg_degree2 = average_neighbor_degree(G)\n    assert avg_degree1 == avg_degree2\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.assortativity import average_neighbor_degree\n\n# Property 1: Output contains an entry for each specified node\n@given(graph=st.builds(nx.path_graph, st.integers(min_value=1, max_value=10)), nodes=st.lists(st.integers(min_value=0, max_value=9), min_size=1, max_size=5))\ndef test_output_contains_entry_for_each_specified_node(graph, nodes):\n    result = average_neighbor_degree(graph, nodes=nodes)\n    for node in nodes:\n        assert node in result\n\n# Property 2: Average degree values are non-negative\n@given(graph=st.builds(nx.path_graph, st.integers(min_value=1, max_value=10)))\ndef test_average_degree_values_are_non_negative(graph):\n    result = average_neighbor_degree(graph)\n    for avg_degree in result.values():\n        assert avg_degree >= 0\n\n# Property 3: Average neighbor degree is symmetric for undirected graphs\n@given(graph=st.builds(nx.path_graph, st.integers(min_value=1, max_value=10)))\ndef test_average_neighbor_degree_is_symmetric_for_undirected(graph):\n    result = average_neighbor_degree(graph)\n    for node_a in graph.nodes:\n        for node_b in graph.neighbors(node_a):\n            assert result[node_a] == result[node_b]\n\n# Property 4: Zero average neighbor degree for nodes with no predecessors in directed graphs\n@given(graph=st.builds(nx.DiGraph, st.integers(min_value=1, max_value=10)))\ndef test_zero_average_neighbor_degree_for_no_predecessors(graph):\n    result = average_neighbor_degree(graph, source=\"in\")\n    for node in graph.nodes:\n        if graph.in_degree(node) == 0:\n            assert result[node] == 0.0\n\n# Property 5: Consistency of output for the same parameters\n@given(graph=st.builds(nx.path_graph, st.integers(min_value=1, max_value=10)))\ndef test_consistency_of_output(graph):\n    result_1 = average_neighbor_degree(graph)\n    result_2 = average_neighbor_degree(graph)\n    assert result_1 == result_2\n\n# End program\n```"], "properties": ["1. The output dictionary should contain an entry for each node specified in the `nodes` parameter (or all nodes in the graph if `nodes` is not provided), with the node as the key.\n\n2. The average degree values in the output dictionary should be non-negative numbers, as degrees cannot be negative.\n\n3. For undirected graphs, the average neighbor degree should be symmetric, meaning that the average neighbor degree of node A should equal that of node B if A and B are neighbors.\n\n4. If the graph is directed and the source is set to \"in\", the average neighbor degree for nodes with no predecessors should be zero.\n\n5. The average neighbor degree should be consistent when called multiple times with the same parameters, returning the same results for the same graph structure and parameters."], "api_doc": "average_neighbor_degree\naverage_neighbor_degree(G, source='out', target='out', nodes=None, weight=None)[source]\nReturns the average degree of the neighborhood of each node.\n\nIn an undirected graph, the neighborhood N(i) of node i contains the nodes that are connected to i by an edge.\n\nFor directed graphs, N(i) is defined according to the parameter source:\n\nif source is \u2018in\u2019, then N(i) consists of predecessors of node i.\n\nif source is \u2018out\u2019, then N(i) consists of successors of node i.\n\nif source is \u2018in+out\u2019, then N(i) is both predecessors and successors.\n\nThe average neighborhood degree of a node i is\n\n \n \nwhere N(i) are the neighbors of node i and k_j is the degree of node j which belongs to N(i). For weighted graphs, an analogous measure can be defined [1],\n\n \n \nwhere s_i is the weighted degree of node i, w_{ij} is the weight of the edge that links i and j and N(i) are the neighbors of node i.\n\nParameters\n:\nG\nNetworkX graph\nsource\nstring (\u201cin\u201d|\u201dout\u201d|\u201din+out\u201d), optional (default=\u201dout\u201d)\nDirected graphs only. Use \u201cin\u201d- or \u201cout\u201d-neighbors of source node.\n\ntarget\nstring (\u201cin\u201d|\u201dout\u201d|\u201din+out\u201d), optional (default=\u201dout\u201d)\nDirected graphs only. Use \u201cin\u201d- or \u201cout\u201d-degree for target node.\n\nnodes\nlist or iterable, optional (default=G.nodes)\nCompute neighbor degree only for specified nodes.\n\nweight\nstring or None, optional (default=None)\nThe edge attribute that holds the numerical value used as a weight. If None, then each edge has weight 1.\n\nReturns\n:\nd: dict\nA dictionary keyed by node to the average degree of its neighbors.\n\nRaises\n:\nNetworkXError\nIf either source or target are not one of \u2018in\u2019, \u2018out\u2019, or \u2018in+out\u2019. If either source or target is passed for an undirected graph.\n\nSee also\n\naverage_degree_connectivity\nReferences\n\n[1]\nA. Barrat, M. Barth\u00e9lemy, R. Pastor-Satorras, and A. Vespignani, \u201cThe architecture of complex weighted networks\u201d. PNAS 101 (11): 3747\u20133752 (2004).\n\nExamples\n\nG = nx.path_graph(4)\nG.edges[0, 1][\"weight\"] = 5\nG.edges[2, 3][\"weight\"] = 3\nnx.average_neighbor_degree(G)\n{0: 2.0, 1: 1.5, 2: 1.5, 3: 2.0}\nnx.average_neighbor_degree(G, weight=\"weight\")\n{0: 2.0, 1: 1.1666666666666667, 2: 1.25, 3: 2.0}\nG = nx.DiGraph()\nnx.add_path(G, [0, 1, 2, 3])\nnx.average_neighbor_degree(G, source=\"in\", target=\"in\")\n{0: 0.0, 1: 0.0, 2: 1.0, 3: 1.0}\nnx.average_neighbor_degree(G, source=\"out\", target=\"out\")\n{0: 1.0, 1: 1.0, 2: 0.0, 3: 0.0}", "api_code": "@nx._dispatchable(edge_attrs=\"weight\")\ndef average_neighbor_degree(G, source=\"out\", target=\"out\", nodes=None, weight=None):\n    if G.is_directed():\n        if source == \"in\":\n            source_degree = G.in_degree\n        elif source == \"out\":\n            source_degree = G.out_degree\n        elif source == \"in+out\":\n            source_degree = G.degree\n        else:\n            raise nx.NetworkXError(\n                f\"source argument {source} must be 'in', 'out' or 'in+out'\"\n            )\n\n        if target == \"in\":\n            target_degree = G.in_degree\n        elif target == \"out\":\n            target_degree = G.out_degree\n        elif target == \"in+out\":\n            target_degree = G.degree\n        else:\n            raise nx.NetworkXError(\n                f\"target argument {target} must be 'in', 'out' or 'in+out'\"\n            )\n    else:\n        if source != \"out\" or target != \"out\":\n            raise nx.NetworkXError(\n                f\"source and target arguments are only supported for directed graphs\"\n            )\n        source_degree = target_degree = G.degree\n\n    # precompute target degrees -- should *not* be weighted degree\n    t_deg = dict(target_degree())\n\n    # Set up both predecessor and successor neighbor dicts leaving empty if not needed\n    G_P = G_S = {n: {} for n in G}\n    if G.is_directed():\n        # \"in\" or \"in+out\" cases: G_P contains predecessors\n        if \"in\" in source:\n            G_P = G.pred\n        # \"out\" or \"in+out\" cases: G_S contains successors\n        if \"out\" in source:\n            G_S = G.succ\n    else:\n        # undirected leave G_P empty but G_S is the adjacency\n        G_S = G.adj\n\n    # Main loop: Compute average degree of neighbors\n    avg = {}\n    for n, deg in source_degree(nodes, weight=weight):\n        # handle degree zero average\n        if deg == 0:\n            avg[n] = 0.0\n            continue\n\n        # we sum over both G_P and G_S, but one of the two is usually empty.\n        if weight is None:\n            avg[n] = (\n                sum(t_deg[nbr] for nbr in G_S[n]) + sum(t_deg[nbr] for nbr in G_P[n])\n            ) / deg\n        else:\n            avg[n] = (\n                sum(dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_S[n].items())\n                + sum(dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_P[n].items())\n            ) / deg\n    return avg"}
{"function_name": "networkx.algorithms.dag.is_aperiodic", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.dag import is_aperiodic\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_acyclic_graph_property(edges):\n    # Create a directed graph from the edges\n    G = nx.DiGraph(edges)\n    # Check if the graph is acyclic\n    if nx.is_directed_acyclic_graph(G):\n        assert is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_single_cycle_property(edges):\n    # Create a directed graph from the edges\n    G = nx.DiGraph(edges)\n    # Find cycles in the graph\n    cycles = list(nx.simple_cycles(G))\n    for cycle in cycles:\n        if len(cycle) > 1:\n            assert is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_coprime_cycles_property(edges):\n    # Create a directed graph from the edges\n    G = nx.DiGraph(edges)\n    # Find cycles in the graph\n    cycles = list(nx.simple_cycles(G))\n    lengths = [len(cycle) for cycle in cycles]\n    if len(lengths) > 1 and all(lengths[i] % lengths[j] != 0 for i in range(len(lengths)) for j in range(len(lengths)) if i != j):\n        assert is_aperiodic(G) is True\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_common_divisor_cycles_property(edges):\n    # Create a directed graph from the edges\n    G = nx.DiGraph(edges)\n    # Find cycles in the graph\n    cycles = list(nx.simple_cycles(G))\n    lengths = [len(cycle) for cycle in cycles]\n    if len(lengths) > 1 and any(lengths[i] % lengths[j] == 0 for i in range(len(lengths)) for j in range(len(lengths)) if i != j):\n        assert is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_edge_permutation_property(edges):\n    # Create a directed graph from the edges\n    G1 = nx.DiGraph(edges)\n    G2 = nx.DiGraph(sorted(edges))  # Sort edges to create a permutation\n    assert is_aperiodic(G1) == is_aperiodic(G2)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXError\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=1000, unique=True))\ndef test_acyclic_graph_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(G):\n        assert not nx.algorithms.dag.is_aperiodic(G)\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=1000, unique=True))\ndef test_single_cycle_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.number_of_nodes(G) >= 2 and len(edges) == 2:\n        assert nx.algorithms.dag.is_aperiodic(G) == False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=1000, unique=True))\ndef test_coprime_cycles_property(edges):\n    G = nx.DiGraph(edges)\n    if len(edges) > 2:\n        cycles = list(nx.simple_cycles(G))\n        cycle_lengths = [len(cycle) for cycle in cycles]\n        if len(cycle_lengths) > 1 and all(gcd(cycle_lengths) == 1 for cycle in cycles):\n            assert nx.algorithms.dag.is_aperiodic(G)\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=1000, unique=True))\ndef test_common_divisor_cycles_property(edges):\n    G = nx.DiGraph(edges)\n    if len(edges) > 2:\n        cycles = list(nx.simple_cycles(G))\n        cycle_lengths = [len(cycle) for cycle in cycles]\n        if len(cycle_lengths) > 1 and all(len(cycle_lengths) > 1 and gcd(cycle_lengths) > 1):\n            assert nx.algorithms.dag.is_aperiodic(G) == False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=1000, unique=True))\ndef test_order_independence_property(edges):\n    G1 = nx.DiGraph(edges)\n    G2 = nx.DiGraph(sorted(edges))\n    assert nx.algorithms.dag.is_aperiodic(G1) == nx.algorithms.dag.is_aperiodic(G2)\n# End program\n``` \n\nNote: The `gcd` function and the proper imports for it should be included in the actual testing code. You can implement the `gcd` function using `math.gcd` or any similar approach.", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXError\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_acyclic_graph_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(G):\n        assert nx.algorithms.dag.is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_single_cycle_property(edges):\n    if len(edges) == 2 and edges[0][0] == edges[1][1]:  # Ensuring a single cycle\n        G = nx.DiGraph(edges)\n        cycle_length = len(edges)\n        if cycle_length > 1:\n            assert nx.algorithms.dag.is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=3, max_size=100))\ndef test_coprime_cycles_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_strongly_connected(G):\n        cycle_lengths = {len(cycle) for cycle in nx.simple_cycles(G)}\n        if len(cycle_lengths) > 1 and all(cycle_lengths):\n            assert gcd(*cycle_lengths) == 1\n            assert nx.algorithms.dag.is_aperiodic(G) is True\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=4, max_size=100))\ndef test_common_divisor_cycles_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_strongly_connected(G):\n        cycle_lengths = {len(cycle) for cycle in nx.simple_cycles(G)}\n        if len(cycle_lengths) > 1 and all(cycle_lengths):\n            common_divisor = gcd(*cycle_lengths)\n            if common_divisor > 1:\n                assert nx.algorithms.dag.is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_order_independence_property(edges):\n    G1 = nx.DiGraph(edges)\n    G2 = nx.DiGraph(edges[::-1])  # Reverse the edges\n    assert nx.algorithms.dag.is_aperiodic(G1) == nx.algorithms.dag.is_aperiodic(G2)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.dag import is_aperiodic\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=20))\ndef test_acyclic_graph_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(G):\n        assert is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=2, max_size=20))\ndef test_single_cycle_property(edges):\n    G = nx.DiGraph(edges)\n    cycles = list(nx.simple_cycles(G))\n    if len(cycles) == 1 and len(cycles[0]) > 1:\n        assert is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=3, max_size=20))\ndef test_coprime_cycles_property(edges):\n    G = nx.DiGraph(edges)\n    cycles = list(nx.simple_cycles(G))\n    cycle_lengths = [len(cycle) for cycle in cycles]\n    if len(cycle_lengths) > 1 and all(nx.gcd(cycle_lengths) == 1):\n        assert is_aperiodic(G) is True\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=4, max_size=20))\ndef test_common_divisor_cycles_property(edges):\n    G = nx.DiGraph(edges)\n    cycles = list(nx.simple_cycles(G))\n    cycle_lengths = [len(cycle) for cycle in cycles]\n    if len(cycle_lengths) > 1 and not all(nx.gcd(cycle_lengths) == 1):\n        assert is_aperiodic(G) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=20))\ndef test_order_independence_property(edges):\n    G1 = nx.DiGraph(edges)\n    G2 = nx.DiGraph(sorted(edges))  # Sort edges to create a different order\n    assert is_aperiodic(G1) == is_aperiodic(G2)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.dag import is_aperiodic\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_acyclic_graph_property(edges):\n    DG = nx.DiGraph(edges)\n    if nx.is_directed_acyclic_graph(DG):\n        assert is_aperiodic(DG) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=2))\ndef test_single_cycle_property(edges):\n    if len(edges) == 2:\n        DG = nx.DiGraph(edges)\n        if (edges[0][1], edges[0][0]) in edges:\n            assert is_aperiodic(DG) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=3))\ndef test_coprime_cycles_property(edges):\n    DG = nx.DiGraph(edges)\n    cycle_lengths = []\n    for cycle in nx.simple_cycles(DG):\n        cycle_lengths.append(len(cycle))\n    if len(cycle_lengths) > 1 and all(gcd(cycle_lengths[0], length) == 1 for length in cycle_lengths[1:]):\n        assert is_aperiodic(DG) is True\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=4))\ndef test_common_divisor_cycles_property(edges):\n    DG = nx.DiGraph(edges)\n    cycle_lengths = []\n    for cycle in nx.simple_cycles(DG):\n        cycle_lengths.append(len(cycle))\n    if len(cycle_lengths) > 1:\n        common_divisor = gcd(cycle_lengths[0], cycle_lengths[1])\n        if all(length % common_divisor == 0 for length in cycle_lengths):\n            assert is_aperiodic(DG) is False\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_order_independence_property(edges):\n    DG1 = nx.DiGraph(edges)\n    DG2 = nx.DiGraph(sorted(edges))\n    assert is_aperiodic(DG1) == is_aperiodic(DG2)\n\n# End program\n``` \n\nNote: The `gcd` function used in the code should be imported from the `math` module. Additionally, the functions that analyze cycles (`nx.simple_cycles`) and the conditions for testing should be properly checked to ensure accuracy in the tests. Please ensure that you have the `networkx` library installed to run these tests."], "properties": ["1. If the input graph is acyclic, the output should always be False, as acyclic graphs do not contain cycles and therefore cannot be aperiodic.\n\n2. If the input graph contains a single cycle of length k, where k > 1, the output should be False, since k divides the length of the cycle.\n\n3. If the input graph contains multiple cycles with lengths that are coprime (i.e., their greatest common divisor is 1), the output should be True, indicating that the graph is aperiodic.\n\n4. If the input graph contains cycles where all cycle lengths share a common divisor greater than 1, the output should be False, as this indicates the presence of a common k that divides each cycle length.\n\n5. The output of the function should remain consistent regardless of the order of edges in the input directed graph, meaning that the function should yield the same result for any permutation of the edges that define the same graph structure."], "api_doc": "is_aperiodic\nis_aperiodic(G)[source]\nReturns True if G is aperiodic.\n\nA directed graph is aperiodic if there is no integer k > 1 that divides the length of every cycle in the graph.\n\nParameters\n:\nG\nNetworkX DiGraph\nA directed graph\n\nReturns\n:\nbool\nTrue if the graph is aperiodic False otherwise\n\nRaises\n:\nNetworkXError\nIf G is not directed\n\nNotes\n\nThis uses the method outlined in [1], which runs in \n time given \n edges in G. Note that a graph is not aperiodic if it is acyclic as every integer trivial divides length 0 cycles.\n\nReferences\n\n[1]\nJarvis, J. P.; Shier, D. R. (1996), \u201cGraph-theoretic analysis of finite Markov chains,\u201d in Shier, D. R.; Wallenius, K. T., Applied Mathematical Modeling: A Multidisciplinary Approach, CRC Press.\n\nExamples\n\nA graph consisting of one cycle, the length of which is 2. Therefore k = 2 divides the length of every cycle in the graph and thus the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 1)])\nnx.is_aperiodic(DG)\nFalse\nA graph consisting of two cycles: one of length 2 and the other of length 3. The cycle lengths are coprime, so there is no single value of k where k > 1 that divides each cycle length and therefore the graph is aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 3), (3, 1), (1, 4), (4, 1)])\nnx.is_aperiodic(DG)\nTrue\nA graph consisting of two cycles: one of length 2 and the other of length 4. The lengths of the cycles share a common factor k = 2, and therefore the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 1), (3, 4), (4, 5), (5, 6), (6, 3)])\nnx.is_aperiodic(DG)\nFalse\nAn acyclic graph, therefore the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 3)])\nnx.is_aperiodic(DG)\nFalse", "api_code": "@nx._dispatchable\ndef is_aperiodic(G):\n    if not G.is_directed():\n        raise nx.NetworkXError(\"is_aperiodic not defined for undirected graphs\")\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept(\"Graph has no nodes.\")\n    s = arbitrary_element(G)\n    levels = {s: 0}\n    this_level = [s]\n    g = 0\n    lev = 1\n    while this_level:\n        next_level = []\n        for u in this_level:\n            for v in G[u]:\n                if v in levels:  # Non-Tree Edge\n                    g = gcd(g, levels[u] - levels[v] + 1)\n                else:  # Tree Edge\n                    next_level.append(v)\n                    levels[v] = lev\n        this_level = next_level\n        lev += 1\n    if len(levels) == len(G):  # All nodes in tree\n        return g == 1\n    else:\n        return g == 1 and nx.is_aperiodic(G.subgraph(set(G) - set(levels)))"}
{"function_name": "statistics.geometric_mean", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(min_value=0.0001, max_value=1e10), min_size=1, max_size=5))\ndef test_output_is_positive_property(data):\n    result = statistics.geometric_mean(data)\n    assert result > 0\n\n@given(st.lists(st.floats(min_value=0.0001, max_value=1e10), min_size=1, max_size=5))\ndef test_output_less_than_or_equal_to_max_property(data):\n    result = statistics.geometric_mean(data)\n    assert result <= max(data)\n\n@given(st.lists(st.floats(min_value=0.0001, max_value=1e10), min_size=1, max_size=5))\ndef test_output_greater_than_or_equal_to_min_property(data):\n    result = statistics.geometric_mean(data)\n    assert result >= min(data)\n\n@given(st.floats(min_value=0.0001, max_value=1e10))\ndef test_output_equals_single_value_property(single_value):\n    result = statistics.geometric_mean([single_value])\n    assert result == single_value\n\n@given(st.lists(st.floats(min_value=0.0001, max_value=1e10), min_size=1, max_size=5))\ndef test_output_invariant_under_order_property(data):\n    result_original = statistics.geometric_mean(data)\n    result_sorted = statistics.geometric_mean(sorted(data))\n    assert result_original == result_sorted\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(min_value=0.1, max_value=1e10), min_size=1, max_size=5))\ndef test_geometric_mean_positive_output_property(data):\n    result = statistics.geometric_mean(data)\n    assert result > 0\n\n@given(st.lists(st.floats(min_value=0.1, max_value=1e10), min_size=1, max_size=5))\ndef test_geometric_mean_maximum_value_property(data):\n    result = statistics.geometric_mean(data)\n    assert result <= max(data)\n\n@given(st.lists(st.floats(min_value=0.1, max_value=1e10), min_size=1, max_size=5))\ndef test_geometric_mean_minimum_value_property(data):\n    result = statistics.geometric_mean(data)\n    assert result >= min(data)\n\n@given(st.floats(min_value=0.1, max_value=1e10))\ndef test_geometric_mean_single_value_property(value):\n    result = statistics.geometric_mean([value])\n    assert result == value\n\n@given(st.lists(st.floats(min_value=0.1, max_value=1e10), min_size=2, max_size=5))\ndef test_geometric_mean_order_invariance_property(data):\n    result1 = statistics.geometric_mean(data)\n    result2 = statistics.geometric_mean(list(reversed(data)))\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10), min_size=1, max_size=5))\ndef test_geometric_mean_positive_output_property(data):\n    result = statistics.geometric_mean(data)\n    assert result > 0\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10), min_size=1, max_size=5))\ndef test_geometric_mean_maximum_value_property(data):\n    result = statistics.geometric_mean(data)\n    assert result <= max(data)\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10), min_size=1, max_size=5))\ndef test_geometric_mean_minimum_value_property(data):\n    result = statistics.geometric_mean(data)\n    assert result >= min(data)\n\n@given(st.floats(min_value=1e-10, max_value=1e10))\ndef test_geometric_mean_single_value_property(single_value):\n    result = statistics.geometric_mean([single_value])\n    assert result == single_value\n\n@given(st.lists(st.floats(min_value=1e-10, max_value=1e10), min_size=1, max_size=5))\ndef test_geometric_mean_order_invariance_property(data):\n    result_1 = statistics.geometric_mean(data)\n    result_2 = statistics.geometric_mean(list(reversed(data)))\n    assert result_1 == result_2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import geometric_mean\nimport math\n\n@given(st.lists(st.floats(min_value=0, max_value=math.inf), min_size=1, max_size=10))\ndef test_output_is_positive_property(data):\n    result = geometric_mean(data)\n    assert result > 0\n\n@given(st.lists(st.floats(min_value=0, max_value=math.inf), min_size=1, max_size=10))\ndef test_output_le_maximum_value_property(data):\n    if data:\n        max_value = max(data)\n        result = geometric_mean(data)\n        assert result <= max_value\n\n@given(st.lists(st.floats(min_value=0, max_value=math.inf), min_size=1, max_size=10))\ndef test_output_ge_minimum_value_property(data):\n    if data:\n        min_value = min(data)\n        result = geometric_mean(data)\n        assert result >= min_value\n\n@given(st.floats(min_value=0, max_value=math.inf))\ndef test_output_equals_single_value_property(single_value):\n    result = geometric_mean([single_value])\n    assert result == single_value\n\n@given(st.lists(st.floats(min_value=0, max_value=math.inf), min_size=1, max_size=10))\ndef test_output_invariant_under_order_property(data):\n    result1 = geometric_mean(data)\n    result2 = geometric_mean(list(reversed(data)))\n    assert result1 == result2\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\n\n@given(st.lists(st.floats(min_value=0.1, max_value=1e10), min_size=1, max_size=100))\ndef test_output_is_positive_property(data):\n    result = statistics.geometric_mean(data)\n    assert result > 0\n\n@given(st.lists(st.floats(min_value=0.1, max_value=1e10), min_size=1, max_size=100))\ndef test_output_less_than_or_equal_to_max_property(data):\n    result = statistics.geometric_mean(data)\n    assert result <= max(data)\n\n@given(st.lists(st.floats(min_value=0.1, max_value=1e10), min_size=1, max_size=100))\ndef test_output_greater_than_or_equal_to_min_property(data):\n    result = statistics.geometric_mean(data)\n    assert result >= min(data)\n\n@given(st.floats(min_value=0.1, max_value=1e10))\ndef test_output_equals_single_value_property(value):\n    result = statistics.geometric_mean([value])\n    assert result == value\n\n@given(st.lists(st.floats(min_value=0.1, max_value=1e10), min_size=1, max_size=100))\ndef test_output_invariant_under_order_property(data):\n    result1 = statistics.geometric_mean(data)\n    result2 = statistics.geometric_mean(sorted(data))\n    assert result1 == result2\n# End program\n```"], "properties": ["1. The output should be a positive float value, as the geometric mean is only defined for positive numbers and cannot be zero or negative.\n\n2. The output should be less than or equal to the maximum value of the input dataset, since the geometric mean cannot exceed the largest number in the dataset.\n\n3. The output should be greater than or equal to the minimum value of the input dataset, as the geometric mean is always at least as large as the smallest positive number in the dataset.\n\n4. If the input dataset contains only one positive number, the output should equal that number, reflecting the property that the geometric mean of a single value is the value itself.\n\n5. The output should be invariant under the order of the input values, meaning that changing the order of the input data should not affect the result of the geometric mean calculation."], "api_doc": "statistics.geometric_mean(data)\nConvert data to floats and compute the geometric mean.\n\nThe geometric mean indicates the central tendency or typical value of the data using the product of the values (as opposed to the arithmetic mean which uses their sum).\n\nRaises a StatisticsError if the input dataset is empty, if it contains a zero, or if it contains a negative value. The data may be a sequence or iterable.\n\nNo special efforts are made to achieve exact results. (However, this may change in the future.)\n\n>>>\nround(geometric_mean([54, 24, 36]), 1)\n36.0\nAdded in version 3.8.", "api_code": "def geometric_mean(data):\n    n = 0\n    found_zero = False\n    def count_positive(iterable):\n        nonlocal n, found_zero\n        for n, x in enumerate(iterable, start=1):\n            if x > 0.0 or math.isnan(x):\n                yield x\n            elif x == 0.0:\n                found_zero = True\n            else:\n                raise StatisticsError('No negative inputs allowed', x)\n    total = fsum(map(log, count_positive(data)))\n    if not n:\n        raise StatisticsError('Must have a non-empty dataset')\n    if math.isnan(total):\n        return math.nan\n    if found_zero:\n        return math.nan if total == math.inf else 0.0\n    return exp(total / n)"}
{"function_name": "datetime.datetime.fromisoformat", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timezone, timedelta\nimport re\n\n# Helper function to generate valid ISO 8601 date strings\ndef valid_iso_datetime():\n    return st.one_of(\n        st.builds(lambda year, month, day: f\"{year}-{month:02d}-{day:02d}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31)),\n        st.builds(lambda year, month, day, hour, minute, second: \n                   f\"{year}-{month:02d}-{day:02d}T{hour:02d}:{minute:02d}:{second:02d}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31),\n                   st.integers(min_value=0, max_value=23),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=59)),\n        st.builds(lambda year, month, day, hour, minute, second, microsecond: \n                   f\"{year}-{month:02d}-{day:02d}T{hour:02d}:{minute:02d}:{second:02d}.{microsecond:06d}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31),\n                   st.integers(min_value=0, max_value=23),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=999999)),\n        st.builds(lambda year, month, day, tz_offset: \n                   f\"{year}-{month:02d}-{day:02d}T00:00:00{tz_offset}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31),\n                   st.sampled_from(['+00:00', '+01:00', '+02:00', '-01:00', '-02:00']))\n    )\n\n@given(valid_iso_datetime())\ndef test_output_validity_property(date_string):\n    \"\"\"The output datetime object should always represent a valid point in time.\"\"\"\n    try:\n        result = datetime.fromisoformat(date_string)\n        assert isinstance(result, datetime)\n    except ValueError:\n        assert True  # If a ValueError is raised, it is expected\n\n@given(valid_iso_datetime())\ndef test_output_date_attributes_property(date_string):\n    \"\"\"The year, month, and day attributes should match the parsed values from the input string.\"\"\"\n    result = datetime.fromisoformat(date_string)\n    match = re.match(r'(\\d{4})-(\\d{2})-(\\d{2})', date_string)\n    if match:\n        year, month, day = map(int, match.groups())\n        assert result.year == year\n        assert result.month == month\n        assert result.day == day\n\n@given(valid_iso_datetime())\ndef test_output_time_attributes_property(date_string):\n    \"\"\"The time attributes should match the values specified in the input string if present.\"\"\"\n    result = datetime.fromisoformat(date_string)\n    match = re.match(r'(\\d{4})-(\\d{2})-(\\d{2})T(\\d{2}):(\\d{2}):(\\d{2})(\\.\\d+)?', date_string)\n    if match:\n        hour, minute, second = map(int, match.groups()[3:6])\n        assert result.hour == hour\n        assert result.minute == minute\n        assert result.second == second\n\n@given(valid_iso_datetime())\ndef test_output_timezone_property(date_string):\n    \"\"\"The output should have the correct tzinfo attribute if the input includes a timezone offset.\"\"\"\n    result = datetime.fromisoformat(date_string)\n    tz_match = re.search(r'([+-]\\d{2}:\\d{2})$', date_string)\n    if tz_match:\n        assert result.tzinfo is not None\n\n@given(st.integers(min_value=1, max_value=9999))\ndef test_default_midnight_property(year):\n    \"\"\"The output datetime object should default to midnight (00:00:00) if no time is given.\"\"\"\n    date_string = f\"{year}-01-01\"\n    result = datetime.fromisoformat(date_string)\n    assert result.hour == 0\n    assert result.minute == 0\n    assert result.second == 0\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime\n\n@given(st.text())\ndef test_valid_datetime_object_property(date_string):\n    try:\n        result = datetime.fromisoformat(date_string)\n        assert isinstance(result, datetime)\n    except ValueError:\n        pass  # Expected for invalid inputs\n\n@given(st.text())\ndef test_attributes_match_input_property(date_string):\n    try:\n        result = datetime.fromisoformat(date_string)\n        if 'T' in date_string or ' ' in date_string:\n            date_part, time_part = date_string.split('T') if 'T' in date_string else date_string.split(' ')\n            year, month, day = map(int, date_part.split('-'))\n            hour, minute, second = (map(int, time_part.split(':')) + [0])[:3]\n            assert result.year == year\n            assert result.month == month\n            assert result.day == day\n            assert result.hour == hour\n            assert result.minute == minute\n            assert result.second == second\n    except ValueError:\n        pass  # Expected for invalid inputs\n\n@given(st.text())\ndef test_timezone_information_property(date_string):\n    try:\n        result = datetime.fromisoformat(date_string)\n        if '+' in date_string or '-' in date_string:\n            assert result.tzinfo is not None\n    except ValueError:\n        pass  # Expected for invalid inputs\n\n@given(st.text())\ndef test_time_defaults_to_midnight_property(date_string):\n    if 'T' not in date_string and ' ' not in date_string:\n        date_string += 'T00:00:00'  # Ensure time is added for the test\n    try:\n        result = datetime.fromisoformat(date_string)\n        if 'T' not in date_string and ' ' not in date_string:\n            assert result.hour == 0\n            assert result.minute == 0\n            assert result.second == 0\n    except ValueError:\n        pass  # Expected for invalid inputs\n\n@given(st.text())\ndef test_valid_timezone_offset_property(date_string):\n    try:\n        result = datetime.fromisoformat(date_string)\n        if 'Z' in date_string or '+' in date_string or '-' in date_string:\n            assert result.tzinfo is not None\n    except ValueError:\n        pass  # Expected for invalid inputs\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timedelta, timezone\nimport re\n\n# Generate valid ISO 8601 date strings\ndef valid_iso_8601():\n    return st.one_of(\n        st.builds(lambda y, m, d: f\"{y:04d}-{m:02d}-{d:02d}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31)),\n        st.builds(lambda y, m, d, h, mi, s, ms: f\"{y:04d}-{m:02d}-{d:02d}T{h:02d}:{mi:02d}:{s:02d}.{ms:06d}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31),\n                   st.integers(min_value=0, max_value=23),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=999999)),\n        st.builds(lambda y, m, d, tz: f\"{y:04d}-{m:02d}-{d:02d}Z\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31)),\n        st.builds(lambda y, m, d, h, mi, s, ms, offset: f\"{y:04d}-{m:02d}-{d:02d}T{h:02d}:{mi:02d}:{s:02d}.{ms:06d}{offset}\",\n                   st.integers(min_value=1, max_value=9999),\n                   st.integers(min_value=1, max_value=12),\n                   st.integers(min_value=1, max_value=31),\n                   st.integers(min_value=0, max_value=23),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=59),\n                   st.integers(min_value=0, max_value=999999),\n                   st.regex(r'([+-]\\d{2}:\\d{2}|Z)'))\n    )\n\n@given(valid_iso_8601())\ndef test_datetime_fromisoformat_validity_property(date_string):\n    # The output should always be a valid datetime object\n    result = datetime.fromisoformat(date_string)\n    assert isinstance(result, datetime)\n\n@given(valid_iso_8601())\ndef test_datetime_fromisoformat_date_attributes_property(date_string):\n    # Extract year, month, and day from input\n    match = re.match(r'(\\d{4})-(\\d{2})-(\\d{2})', date_string)\n    if match:\n        year, month, day = map(int, match.groups())\n        result = datetime.fromisoformat(date_string)\n        assert result.year == year\n        assert result.month == month\n        assert result.day == day\n\n@given(valid_iso_8601())\ndef test_datetime_fromisoformat_time_attributes_property(date_string):\n    # Extract time attributes if they exist\n    match = re.match(r'.*T(\\d{2}):(\\d{2}):(\\d{2})(\\.\\d{1,6})?$', date_string)\n    if match:\n        hour, minute, second = map(int, match.groups()[:3])\n        result = datetime.fromisoformat(date_string)\n        assert result.hour == hour\n        assert result.minute == minute\n        assert result.second == second\n\n@given(valid_iso_8601())\ndef test_datetime_fromisoformat_timezone_property(date_string):\n    # Extract timezone information if it exists\n    if 'Z' in date_string or re.search(r'[+-]\\d{2}:\\d{2}', date_string):\n        result = datetime.fromisoformat(date_string)\n        assert result.tzinfo is not None\n    else:\n        result = datetime.fromisoformat(date_string)\n        assert result.tzinfo is None\n\n@given(valid_iso_8601())\ndef test_datetime_fromisoformat_default_midnight_property(date_string):\n    # Check if the time defaults to midnight if not specified\n    if 'T' not in date_string:\n        result = datetime.fromisoformat(date_string)\n        assert result.hour == 0\n        assert result.minute == 0\n        assert result.second == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timezone, timedelta\nimport re\n\n@st.composite\ndef iso_datetime_strings(draw):\n    # Generate valid ISO 8601 date strings\n    year = draw(st.integers(min_value=1, max_value=9999))\n    month = draw(st.integers(min_value=1, max_value=12))\n    day = draw(st.integers(min_value=1, max_value=31))\n    hour = draw(st.integers(min_value=0, max_value=23))\n    minute = draw(st.integers(min_value=0, max_value=59))\n    second = draw(st.integers(min_value=0, max_value=59))\n    microsecond = draw(st.integers(min_value=0, max_value=999999))\n    \n    # Timezone offset\n    tz_offset = draw(st.one_of(\n        st.just('Z'),  # UTC\n        st.timezones()  # Random timezone offsets\n    ))\n    \n    if tz_offset == 'Z':\n        return f\"{year:04d}-{month:02d}-{day:02d}T{hour:02d}:{minute:02d}:{second:02d}.{microsecond:06d}Z\"\n    else:\n        return f\"{year:04d}-{month:02d}-{day:02d}T{hour:02d}:{minute:02d}:{second:02d}.{microsecond:06d}{tz_offset}\"\n\n@given(iso_datetime_strings())\ndef test_output_is_valid_datetime_property(date_string):\n    # Test that the output is a valid datetime object\n    dt = datetime.fromisoformat(date_string)\n    assert isinstance(dt, datetime)\n\n@given(iso_datetime_strings())\ndef test_date_attributes_match_input_property(date_string):\n    # Test that the year, month, and day attributes correspond to input\n    dt = datetime.fromisoformat(date_string)\n    year, month, day = map(int, re.findall(r'\\d+', date_string.split('T')[0]))\n    assert dt.year == year\n    assert dt.month == month\n    assert dt.day == day\n\n@given(iso_datetime_strings())\ndef test_time_attributes_match_input_property(date_string):\n    # Test that time attributes match if present\n    dt = datetime.fromisoformat(date_string)\n    time_part = date_string.split('T')[1] if 'T' in date_string else None\n    if time_part:\n        time_parts = re.findall(r'\\d+', time_part)\n        if len(time_parts) >= 3:  # hour, minute, second are present\n            hour, minute, second = map(int, time_parts[:3])\n            assert dt.hour == hour\n            assert dt.minute == minute\n            assert dt.second == second\n            if len(time_parts) == 4:  # microsecond is present\n                microsecond = int(time_parts[3])\n                assert dt.microsecond == microsecond\n\n@given(iso_datetime_strings())\ndef test_timezone_info_property(date_string):\n    # Test that tzinfo is set correctly if timezone is present\n    dt = datetime.fromisoformat(date_string)\n    if 'Z' in date_string or re.search(r'([+-]\\d{2}:\\d{2})', date_string):\n        assert dt.tzinfo is not None\n    else:\n        assert dt.tzinfo is None\n\n@given(iso_datetime_strings())\ndef test_default_to_midnight_property(date_string):\n    # Test that it defaults to midnight if no time is provided\n    if 'T' not in date_string and ' ' not in date_string:\n        dt = datetime.fromisoformat(date_string)\n        assert dt.hour == 0\n        assert dt.minute == 0\n        assert dt.second == 0\n        assert dt.microsecond == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import datetime, timezone, timedelta\nimport re\n\n# Helper function to generate valid ISO 8601 date strings\ndef valid_iso_format():\n    return st.one_of(\n        st.just('2011-11-04'),\n        st.just('20111104'),\n        st.just('2011-11-04T00:05:23'),\n        st.just('2011-11-04T00:05:23Z'),\n        st.just('20111104T000523'),\n        st.just('2011-W01-2T00:05:23.283'),\n        st.just('2011-11-04 00:05:23.283'),\n        st.just('2011-11-04 00:05:23.283+00:00'),\n        st.just('2011-11-04T00:05:23+04:00'),\n        st.text(min_size=10, max_size=30).filter(lambda x: re.match(r'\\d{4}(-)?\\d{2}(-)?\\d{2}T?\\d{2}:\\d{2}:\\d{2}(Z|[+-]\\d{2}:\\d{2})?', x))\n    )\n\n@given(date_string=valid_iso_format())\ndef test_output_validity_property(date_string):\n    try:\n        result = datetime.fromisoformat(date_string)\n        assert isinstance(result, datetime)\n    except ValueError:\n        assert True  # If ValueError is raised, the input was invalid.\n\n@given(date_string=valid_iso_format())\ndef test_date_attributes_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    date_part = re.match(r'(\\d{4})-(\\d{2})-(\\d{2})', date_string)\n    if date_part:\n        year, month, day = map(int, date_part.groups())\n        assert result.year == year\n        assert result.month == month\n        assert result.day == day\n\n@given(date_string=valid_iso_format())\ndef test_time_attributes_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    time_part = re.search(r'T(\\d{2}):(\\d{2}):(\\d{2})', date_string)\n    if time_part:\n        hour, minute, second = map(int, time_part.groups())\n        assert result.hour == hour\n        assert result.minute == minute\n        assert result.second == second\n\n@given(date_string=valid_iso_format())\ndef test_tzinfo_property(date_string):\n    result = datetime.fromisoformat(date_string)\n    tz_part = re.search(r'([+-]\\d{2}:\\d{2}|Z)', date_string)\n    if tz_part:\n        if tz_part.group(0) == 'Z':\n            assert result.tzinfo == timezone.utc\n        else:\n            offset = tz_part.group(0)\n            hours, minutes = map(int, offset.split(':'))\n            assert result.tzinfo == timezone(timedelta(hours=hours, minutes=minutes))\n\n@given(date_string=valid_iso_format())\ndef test_default_midnight_property(date_string):\n    if 'T' not in date_string and ' ' not in date_string:\n        result = datetime.fromisoformat(date_string)\n        assert result.hour == 0\n        assert result.minute == 0\n        assert result.second == 0\n# End program\n```"], "properties": ["1. The output datetime object should always represent a valid point in time, meaning it should not raise any exceptions for valid ISO 8601 formatted input strings.\n\n2. The year, month, and day attributes of the output datetime object should correspond directly to the values parsed from the input date_string, reflecting accurate date representation.\n\n3. If the input date_string includes time information, the hour, minute, second, and microsecond attributes of the output datetime object should match the values specified in the input string.\n\n4. If the input date_string contains a time zone offset, the output datetime object should have the correct `tzinfo` attribute set, accurately reflecting the specified time zone.\n\n5. The output datetime object should default to midnight (00:00:00) for the time portion if the input date_string does not include any time information."], "api_doc": "classmethod datetime.fromisoformat(date_string)\nReturn a datetime corresponding to a date_string in any valid ISO 8601 format, with the following exceptions:\n\nTime zone offsets may have fractional seconds.\n\nThe T separator may be replaced by any single unicode character.\n\nFractional hours and minutes are not supported.\n\nReduced precision dates are not currently supported (YYYY-MM, YYYY).\n\nExtended date representations are not currently supported (\u00b1YYYYYY-MM-DD).\n\nOrdinal dates are not currently supported (YYYY-OOO).\n\nExamples:\n\n>>>\nfrom datetime import datetime\ndatetime.fromisoformat('2011-11-04')\ndatetime.datetime(2011, 11, 4, 0, 0)\ndatetime.fromisoformat('20111104')\ndatetime.datetime(2011, 11, 4, 0, 0)\ndatetime.fromisoformat('2011-11-04T00:05:23')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\ndatetime.fromisoformat('2011-11-04T00:05:23Z')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, tzinfo=datetime.timezone.utc)\ndatetime.fromisoformat('20111104T000523')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\ndatetime.fromisoformat('2011-W01-2T00:05:23.283')\ndatetime.datetime(2011, 1, 4, 0, 5, 23, 283000)\ndatetime.fromisoformat('2011-11-04 00:05:23.283')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000)\ndatetime.fromisoformat('2011-11-04 00:05:23.283+00:00')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000, tzinfo=datetime.timezone.utc)\ndatetime.fromisoformat('2011-11-04T00:05:23+04:00')   \ndatetime.datetime(2011, 11, 4, 0, 5, 23,\n    tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))\nAdded in version 3.7.\n\nChanged in version 3.11: Previously, this method only supported formats that could be emitted by date.isoformat() or datetime.isoformat().", "api_code": "@classmethod\ndef fromisoformat(cls, date_string):\n    if not isinstance(date_string, str):\n        raise TypeError('fromisoformat: argument must be str')\n\n    if len(date_string) < 7:\n        raise ValueError(f'Invalid isoformat string: {date_string!r}')\n\n    # Split this at the separator\n    try:\n        separator_location = _find_isoformat_datetime_separator(date_string)\n        dstr = date_string[0:separator_location]\n        tstr = date_string[(separator_location+1):]\n\n        date_components = _parse_isoformat_date(dstr)\n    except ValueError:\n        raise ValueError(\n            f'Invalid isoformat string: {date_string!r}') from None\n\n    if tstr:\n        try:\n            time_components, became_next_day, error_from_components = _parse_isoformat_time(tstr)\n        except ValueError:\n            raise ValueError(\n                f'Invalid isoformat string: {date_string!r}') from None\n        else:\n            if error_from_components:\n                raise ValueError(\"minute, second, and microsecond must be 0 when hour is 24\")\n\n            if became_next_day:\n                year, month, day = date_components\n                # Only wrap day/month when it was previously valid\n                if month <= 12 and day <= (days_in_month := _days_in_month(year, month)):\n                    # Calculate midnight of the next day\n                    day += 1\n                    if day > days_in_month:\n                        day = 1\n                        month += 1\n                        if month > 12:\n                            month = 1\n                            year += 1\n                    date_components = [year, month, day]\n    else:\n        time_components = [0, 0, 0, 0, None]\n\n    return cls(*(date_components + time_components))"}
{"function_name": "datetime.date.isocalendar", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\nimport sys\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_year_component_property(dt):\n    iso_year, iso_week, iso_weekday = dt.isocalendar()\n    if iso_week == 1 and dt.month == 1 and dt.day <= 7:\n        assert iso_year == dt.year\n    elif iso_week == 53 and dt.month == 12:\n        assert iso_year == dt.year or iso_year == dt.year + 1\n    else:\n        assert iso_year == dt.year\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_week_component_property(dt):\n    iso_year, iso_week, iso_weekday = dt.isocalendar()\n    assert 1 <= iso_week <= 53\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_weekday_component_property(dt):\n    iso_year, iso_week, iso_weekday = dt.isocalendar()\n    assert 1 <= iso_weekday <= 7\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_year_week_compatibility_property(dt):\n    iso_year, iso_week, iso_weekday = dt.isocalendar()\n    if iso_week == 1 and iso_weekday == 1:\n        assert dt >= date(iso_year, 1, 1)\n    elif iso_week == 53:\n        assert dt <= date(iso_year, 12, 31)\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_valid_iso_calendar_date_property(dt):\n    iso_year, iso_week, iso_weekday = dt.isocalendar()\n    # Check if the derived week and weekday correspond to a valid date range\n    first_day_of_week = dt - timedelta(days=dt.isocalendar()[2] - 1)  # Get Monday of that week\n    last_day_of_week = first_day_of_week + timedelta(days=6)  # Get Sunday of that week\n    assert first_day_of_week.year == iso_year\n    assert last_day_of_week.year == iso_year\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\n\n@given(st.dates())\ndef test_year_property(d):\n    iso_year, _, _ = d.isocalendar()\n    if d.isocalendar()[1] == 1:  # Checking for the first week\n        assert iso_year == d.year\n\n@given(st.dates())\ndef test_week_property(d):\n    _, iso_week, _ = d.isocalendar()\n    assert 1 <= iso_week <= 53\n\n@given(st.dates())\ndef test_weekday_property(d):\n    _, _, iso_weekday = d.isocalendar()\n    assert 1 <= iso_weekday <= 7\n\n@given(st.dates())\ndef test_year_discrepancy_property(d):\n    if d.month == 12 and d.isocalendar()[1] == 1:  # Last week of December\n        iso_year, _, _ = d.isocalendar()\n        assert iso_year != d.year\n\n@given(st.dates())\ndef test_valid_iso_calendar_date_property(d):\n    iso_year, iso_week, iso_weekday = d.isocalendar()\n    assert 1 <= iso_week <= 53\n    assert 1 <= iso_weekday <= 7\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_year_component_property(d):\n    iso_year, _, _ = d.isocalendar()\n    if d.isocalendar()[1] == 1:  # Check if it's the first week\n        assert iso_year == d.year\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_week_component_property(d):\n    _, iso_week, _ = d.isocalendar()\n    assert 1 <= iso_week <= 53\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_weekday_component_property(d):\n    _, _, iso_weekday = d.isocalendar()\n    assert 1 <= iso_weekday <= 7\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_year_difference_property(d):\n    iso_year, iso_week, iso_weekday = d.isocalendar()\n    if iso_week == 1:  # If it's the first week of the ISO year\n        assert iso_year == d.year or (d.month == 12 and d.day >= 29)\n\n@given(st.dates(min_value=date(1, 1, 1), max_value=date(9999, 12, 31)))\ndef test_valid_iso_calendar_date_property(d):\n    iso_year, iso_week, iso_weekday = d.isocalendar()\n    assert (iso_weekday == 1 and d.isocalendar()[1] == iso_week) or (iso_weekday == 7 and d.isocalendar()[1] == iso_week)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\n\n@given(st.dates())\ndef test_year_component_property(d):\n    iso_year, iso_week, iso_weekday = d.isocalendar()\n    if iso_week == 1 and d.month == 1 and d.day < 8:\n        assert iso_year == d.year\n    elif iso_week > 1:\n        assert iso_year == d.year or iso_year == d.year + 1\n\n@given(st.dates())\ndef test_week_component_property(d):\n    iso_year, iso_week, iso_weekday = d.isocalendar()\n    assert 1 <= iso_week <= 53\n\n@given(st.dates())\ndef test_weekday_component_property(d):\n    iso_year, iso_week, iso_weekday = d.isocalendar()\n    assert 1 <= iso_weekday <= 7\n\n@given(st.dates())\ndef test_year_week_boundary_property(d):\n    iso_year, iso_week, iso_weekday = d.isocalendar()\n    if d.month == 12 and d.day >= 29:\n        assert iso_year == d.year or iso_year == d.year + 1\n\n@given(st.dates())\ndef test_valid_iso_calendar_date_property(d):\n    iso_year, iso_week, iso_weekday = d.isocalendar()\n    assert 1 <= iso_week <= 53\n    assert 1 <= iso_weekday <= 7\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import date\n\n@given(st.dates())\ndef test_year_property(d):\n    iso_year, _, _ = d.isocalendar()\n    if d.isocalendar()[1] == 1:  # Check if it's the first week of the ISO year\n        assert iso_year == d.year\n\n@given(st.dates())\ndef test_week_property(d):\n    week = d.isocalendar()[1]\n    assert 1 <= week <= 53  # Week must be between 1 and 53\n\n@given(st.dates())\ndef test_weekday_property(d):\n    weekday = d.isocalendar()[2]\n    assert 1 <= weekday <= 7  # Weekday must be between 1 (Monday) and 7 (Sunday)\n\n@given(st.dates())\ndef test_iso_year_difference_property(d):\n    iso_year = d.isocalendar()[0]\n    if d.month == 12 and d.day >= 29:  # Check last days of December\n        assert iso_year == d.year or iso_year == d.year + 1\n\n@given(st.dates())\ndef test_valid_iso_calendar_property(d):\n    iso_year, week, weekday = d.isocalendar()\n    assert (1 <= weekday <= 7) and (1 <= week <= 53)  # Valid ISO week and weekday\n# End program\n```"], "properties": ["1. The year component of the result must be equal to the Gregorian year of the date if the date falls within the first week of the ISO year (i.e., the week containing the first Thursday of the year).\n\n2. The week component of the result must be between 1 and 53, inclusive, as the ISO calendar can have either 52 or 53 weeks in a year.\n\n3. The weekday component of the result must be between 1 and 7, inclusive, where 1 represents Monday and 7 represents Sunday.\n\n4. For dates in the last week of December, the year component of the result may differ from the Gregorian year if that week belongs to the first week of the following ISO year.\n\n5. The combination of year, week, and weekday must represent a valid date according to the ISO calendar system, ensuring the week starts on a Monday and ends on a Sunday."], "api_doc": "date.isocalendar()\nReturn a named tuple object with three components: year, week and weekday.\n\nThe ISO calendar is a widely used variant of the Gregorian calendar. [3]\n\nThe ISO year consists of 52 or 53 full weeks, and where a week starts on a Monday and ends on a Sunday. The first week of an ISO year is the first (Gregorian) calendar week of a year containing a Thursday. This is called week number 1, and the ISO year of that Thursday is the same as its Gregorian year.\n\nFor example, 2004 begins on a Thursday, so the first week of ISO year 2004 begins on Monday, 29 Dec 2003 and ends on Sunday, 4 Jan 2004:\n\n>>>\nfrom datetime import date\ndate(2003, 12, 29).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=1)\ndate(2004, 1, 4).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=7)\nChanged in version 3.9: Result changed from a tuple to a named tuple.", "api_code": "def isocalendar(self):\n    year = self._year\n    week1monday = _isoweek1monday(year)\n    today = _ymd2ord(self._year, self._month, self._day)\n    # Internally, week and day have origin 0\n    week, day = divmod(today - week1monday, 7)\n    if week < 0:\n        year -= 1\n        week1monday = _isoweek1monday(year)\n        week, day = divmod(today - week1monday, 7)\n    elif week >= 52:\n        if today >= _isoweek1monday(year+1):\n            year += 1\n            week = 0\n    return _IsoCalendarDate(year, week+1, day+1)"}
{"function_name": "dateutil.parser.isoparse", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom dateutil.parser import isoparse\nfrom datetime import datetime, timedelta\nimport pytz\n\n@given(st.from_type(str))\ndef test_dateutil_parser_isoparse_correct_parsing():\n    \"\"\"\n    Test that the output datetime object represents the same date and time\n    as specified in the input ISO-8601 string.\n    \"\"\"\n    # This can be implemented by generating valid ISO-8601 strings\n    pass\n\n@given(st.from_type(str))\ndef test_dateutil_parser_isoparse_timezone_adjustment():\n    \"\"\"\n    Test that the output datetime object reflects the correct UTC time\n    when a timezone offset is specified in the input string.\n    \"\"\"\n    # This can be implemented by generating valid ISO-8601 strings with time zone offsets\n    pass\n\n@given(st.from_type(str))\ndef test_dateutil_parser_isoparse_default_minimum_values():\n    \"\"\"\n    Test that unspecified components default to their minimum values.\n    \"\"\"\n    # This can be implemented by generating valid ISO-8601 strings with incomplete date formats\n    pass\n\n@given(st.from_type(str))\ndef test_dateutil_parser_isoparse_midnight_representation():\n    \"\"\"\n    Test that both representations of midnight (00:00 and 24:00) are interpreted correctly.\n    \"\"\"\n    # This can be implemented by generating valid ISO-8601 strings with midnight representations\n    pass\n\n@given(st.from_type(str))\ndef test_dateutil_parser_isoparse_invalid_string_handling():\n    \"\"\"\n    Test that the function raises an appropriate error for invalid ISO-8601 strings.\n    \"\"\"\n    # This can be implemented by generating invalid ISO-8601 strings\n    pass\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom dateutil import parser\nfrom datetime import datetime, timedelta\n\n# Property 1: The output datetime object should represent the same date and time as specified in the input ISO-8601 string.\n@given(st.text())\ndef test_output_date_time_representation_property(dt_str):\n    try:\n        result = parser.isoparse(dt_str)\n        expected = datetime.fromisoformat(dt_str.replace('Z', '+00:00'))\n        assert result == expected\n    except ValueError:\n        pass  # Handle invalid ISO-8601 strings\n\n# Property 2: If the input ISO-8601 string specifies a timezone offset, the output should reflect the correct UTC time.\n@given(st.text())\ndef test_output_timezone_offset_property(dt_str):\n    try:\n        result = parser.isoparse(dt_str)\n        if 'Z' in dt_str or any(offset in dt_str for offset in ['+', '-']):\n            expected_utc = result.astimezone(datetime.timezone.utc)\n            assert result == expected_utc\n    except ValueError:\n        pass  # Handle invalid ISO-8601 strings\n\n# Property 3: The output should default to the minimum values for any unspecified components.\n@given(st.text())\ndef test_output_minimum_values_property(dt_str):\n    try:\n        result = parser.isoparse(dt_str)\n        if result.month == 1 and result.day == 1:\n            assert result.day == 1\n            assert result.month == 1\n    except ValueError:\n        pass  # Handle invalid ISO-8601 strings\n\n# Property 4: The output should correctly handle both representations of midnight.\n@given(st.one_of(st.text(min_size=10, max_size=25), st.text(min_size=23, max_size=23)))  # Including '00:00' and '24:00'\ndef test_output_midnight_representation_property(dt_str):\n    try:\n        result = parser.isoparse(dt_str)\n        assert result.hour == 0 and result.minute == 0  # Both should represent midnight\n    except ValueError:\n        pass  # Handle invalid ISO-8601 strings\n\n# Property 5: The output should raise an appropriate error for invalid ISO-8601 strings.\n@given(st.text())\ndef test_invalid_iso_string_property(dt_str):\n    try:\n        parser.isoparse(dt_str)\n    except ValueError:\n        assert True  # Expecting an error for invalid strings\n    else:\n        assert False  # If no error is raised, the test should fail\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom dateutil.parser import isoparse\nfrom datetime import datetime, timedelta, timezone\n\n@given(st.text())\ndef test_isoparse_correctness_property(dt_str):\n    \"\"\"Test that the output datetime object represents the same date and time as specified in the input ISO-8601 string.\"\"\"\n    try:\n        result = isoparse(dt_str)\n        # Check that the result is a datetime object\n        assert isinstance(result, datetime)\n    except ValueError:\n        # If ValueError is raised, it's an invalid ISO-8601 string, which is acceptable\n        pass\n\n@given(st.text())\ndef test_isoparse_timezone_adjustment_property(dt_str):\n    \"\"\"Test that the output datetime object reflects the correct UTC time if a timezone offset is specified.\"\"\"\n    try:\n        result = isoparse(dt_str)\n        if 'Z' in dt_str or any(offset in dt_str for offset in ['+', '-']):\n            utc_time = result.astimezone(timezone.utc)\n            assert utc_time == result  # Output should be in UTC\n    except ValueError:\n        pass\n\n@given(st.text())\ndef test_isoparse_default_minimum_values_property(dt_str):\n    \"\"\"Test that unspecified components default to their minimum values.\"\"\"\n    try:\n        result = isoparse(dt_str)\n        if result.year and result.month and result.day:\n            assert result.day >= 1 and result.month >= 1  # Day and month should be at least 1\n    except ValueError:\n        pass\n\n@given(st.text())\ndef test_isoparse_midnight_representation_property(dt_str):\n    \"\"\"Test that the output correctly handles both representations of midnight (00:00 and 24:00).\"\"\"\n    if '00:00' in dt_str or '24:00' in dt_str:\n        try:\n            result = isoparse(dt_str)\n            assert result.hour == 0 and result.minute == 0  # Should be the start of the day\n        except ValueError:\n            pass\n\n@given(st.text())\ndef test_isoparse_invalid_string_property(dt_str):\n    \"\"\"Test that the output raises an appropriate error for invalid ISO-8601 strings.\"\"\"\n    invalid_strings = [\n        \"2017-01-01T00:00+00:00:00\",  # Invalid due to too many colons\n        \"2017-13-01T00:00Z\",           # Invalid due to month out of range\n        \"Not a date\"                   # Clearly invalid format\n    ]\n    if dt_str in invalid_strings:\n        try:\n            result = isoparse(dt_str)\n            assert False  # Should not reach here\n        except ValueError:\n            assert True  # Expected ValueError was raised\n    else:\n        try:\n            isoparse(dt_str)  # Valid strings should not raise an error\n        except ValueError:\n            assert False  # Should not reach here for valid strings\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom dateutil import parser\nfrom datetime import datetime, timedelta\nimport re\n\n@given(st.text())\ndef test_output_represents_input_date_time(dt_str):\n    try:\n        result = parser.isoparse(dt_str)\n        # Extract components from the input string\n        match = re.match(r'(\\d{4})(?:-(\\d{2}))?(?:-(\\d{2}))?(?:T(\\d{2}):?(\\d{2})?:?(\\d{2})?(?:\\.(\\d+))?)?(Z|[+-]\\d{2}:?\\d{2}|[+-]\\d{2}\\d{2}|[+-]\\d{2})?$', dt_str)\n        if match:\n            year, month, day, hour, minute, second, _, tz = match.groups()\n            # Compare parsed result components\n            assert result.year == int(year)\n            assert result.month == (int(month) if month else 1)\n            assert result.day == (int(day) if day else 1)\n            assert result.hour == (int(hour) if hour else 0)\n            assert result.minute == (int(minute) if minute else 0)\n            assert result.second == (int(second) if second else 0)\n    except (ValueError, TypeError):\n        pass  # Handle invalid ISO-8601 strings that raise exceptions\n\n@given(st.text())\ndef test_output_timezone_offset(dt_str):\n    try:\n        result = parser.isoparse(dt_str)\n        if re.search(r'[+-]\\d{2}:\\d{2}|Z', dt_str):  # Check if there's a timezone\n            assert result.tzinfo is not None\n            # Check if UTC is correctly represented\n            if 'Z' in dt_str or '+00' in dt_str or '-00' in dt_str:\n                assert result.tzinfo.utcoffset(result) == timedelta(0)\n    except (ValueError, TypeError):\n        pass  # Handle invalid ISO-8601 strings that raise exceptions\n\n@given(st.text())\ndef test_unspecified_components_default(dt_str):\n    try:\n        result = parser.isoparse(dt_str)\n        if re.match(r'^\\d{4}(-\\d{2}){0,2}T?(\\d{2}:\\d{2}:\\d{2})?$', dt_str):\n            assert result.month == (int(re.search(r'-(\\d{2})', dt_str).group(1)) if re.search(r'-(\\d{2})', dt_str) else 1)\n            assert result.day == (int(re.search(r'-(\\d{2})$', dt_str).group(1)) if re.search(r'-(\\d{2})$', dt_str) else 1)\n            assert result.hour == (int(re.search(r'T(\\d{2})', dt_str).group(1)) if re.search(r'T(\\d{2})', dt_str) else 0)\n            assert result.minute == (int(re.search(r'(\\d{2}):(\\d{2})', dt_str).group(2)) if re.search(r'(\\d{2}):(\\d{2})', dt_str) else 0)\n            assert result.second == (int(re.search(r'(\\d{2}):(\\d{2}):(\\d{2})', dt_str).group(3)) if re.search(r'(\\d{2}):(\\d{2}):(\\d{2})', dt_str) else 0)\n    except (ValueError, TypeError):\n        pass  # Handle invalid ISO-8601 strings that raise exceptions\n\n@given(st.text())\ndef test_midnight_special_case(dt_str):\n    try:\n        result = parser.isoparse(dt_str)\n        if '00:00' in dt_str or '24:00' in dt_str:\n            assert result.hour == 0\n            assert result.minute == 0\n    except (ValueError, TypeError):\n        pass  # Handle invalid ISO-8601 strings that raise exceptions\n\n@given(st.text())\ndef test_invalid_strings_raise_error(dt_str):\n    if not re.match(r'^\\d{4}(-\\d{2}){0,2}T?(\\d{2}:\\d{2}:\\d{2})?$', dt_str):\n        try:\n            parser.isoparse(dt_str)\n            assert False  # Should raise an error for invalid strings\n        except ValueError:\n            assert True  # Expected behavior\n        except TypeError:\n            assert True  # Expected behavior\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom dateutil import parser\nimport datetime\n\n@given(st.text())\ndef test_output_datetime_represents_input_date_time():\n    # Generate a valid ISO-8601 datetime string and parse it\n    iso_string = st.one_of(\n        st.from_regex(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\+\\d{2}:\\d{2}|Z)?'),\n        st.from_regex(r'\\d{4}-\\d{2}-\\d{2}'),\n        st.from_regex(r'\\d{4}-\\d{2}'),\n        st.from_regex(r'\\d{4}-W\\d{2}-\\d{1}'),\n        st.from_regex(r'\\d{4}-W\\d{2}'),\n    ).example()\n    \n    result = parser.isoparse(iso_string)\n    # Check if output datetime matches input string specifications\n    # (This will need to be more specific according to the generated string)\n\n@given(st.text())\ndef test_timezone_offset_correctly_adjusts_utc():\n    # Generate a valid ISO-8601 datetime string with a timezone offset\n    iso_string = st.from_regex(r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}(?:\\+\\d{2}:\\d{2}|Z)').example()\n    \n    result = parser.isoparse(iso_string)\n    # Check if UTC time is correctly represented\n\n@given(st.text())\ndef test_unspecified_components_default_to_minimum_values():\n    # Generate a valid ISO-8601 datetime string with incomplete data\n    iso_string = st.one_of(\n        st.from_regex(r'\\d{4}'),  # Only year\n        st.from_regex(r'\\d{4}-\\d{2}')  # Year and month\n    ).example()\n    \n    result = parser.isoparse(iso_string)\n    # Check if defaults are as expected (e.g. January 1 for year)\n\n@given(st.text())\ndef test_midnight_representations_are_equivalent():\n    # Generate strings representing midnight\n    iso_strings = ['2023-01-01T00:00:00Z', '2023-01-01T24:00:00Z']\n    \n    for iso_string in iso_strings:\n        result = parser.isoparse(iso_string)\n        # Check if both representations yield the same datetime object\n\n@given(st.text())\ndef test_invalid_iso_strings_raise_errors():\n    # Generate invalid ISO-8601 strings\n    invalid_iso_strings = [\n        '2017-01-01T00:00+00:00:00',  # Invalid format\n        '2023-13-01',  # Invalid month\n        '2023-01-32',  # Invalid day\n    ]\n    \n    for iso_string in invalid_iso_strings:\n        try:\n            result = parser.isoparse(iso_string)\n            assert False, f\"Expected error for {iso_string} but got {result}\"\n        except ValueError:\n            pass  # Expected behavior\n\n# End program\n```"], "properties": ["1. The output datetime object should represent the same date and time as specified in the input ISO-8601 string, with all components correctly parsed (year, month, day, hour, minute, second, and timezone).\n\n2. If the input ISO-8601 string specifies a timezone offset, the output datetime object should reflect the correct UTC time, adjusted according to the given offset.\n\n3. The output datetime object should default to the minimum values for any unspecified components (e.g., if only the year is provided, the month and day should default to January 1st).\n\n4. The output should correctly handle both representations of midnight (00:00 and 24:00) and interpret them as equivalent to the start of the day.\n\n5. The output should raise an appropriate error for invalid ISO-8601 strings, ensuring that valid strings continue to be parsed correctly while invalid ones do not produce ambiguous results."], "api_doc": "parser.isoparse(dt_str)\uf0c1\nParse an ISO-8601 datetime string into a datetime.datetime.\n\nAn ISO-8601 datetime string consists of a date portion, followed optionally by a time portion - the date and time portions are separated by a single character separator, which is T in the official standard. Incomplete date formats (such as YYYY-MM) may not be combined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\nYYYY\n\nYYYY-MM\n\nYYYY-MM-DD or YYYYMMDD\n\nUncommon:\n\nYYYY-Www or YYYYWww - ISO week (day defaults to 0)\n\nYYYY-Www-D or YYYYWwwD - ISO week and day\n\nThe ISO week and day numbering follows the same logic as datetime.date.isocalendar().\n\nSupported time formats are:\n\nhh\n\nhh:mm or hhmm\n\nhh:mm:ss or hhmmss\n\nhh:mm:ss.ssssss (Up to 6 sub-second digits)\n\nMidnight is a special case for hh, as the standard supports both 00:00 and 24:00 as a representation. The decimal separator can be either a dot or a comma.\n\nCaution\n\nSupport for fractional components other than seconds is part of the ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\nZ (UTC)\n\n\u00b1HH:MM\n\n\u00b1HHMM\n\n\u00b1HH\n\nOffsets will be represented as dateutil.tz.tzoffset objects, with the exception of UTC, which will be represented as dateutil.tz.tzutc. Time zone offsets equivalent to UTC (such as +00:00) will also be represented as dateutil.tz.tzutc.\n\nParameters\n:\ndt_str \u2013 A string or stream containing only an ISO-8601 datetime string\n\nReturns\n:\nReturns a datetime.datetime representing the string. Unspecified components default to their lowest value.\n\nWarning\n\nAs of version 2.7.0, the strictness of the parser should not be considered a stable part of the contract. Any valid ISO-8601 string that parses correctly with the default settings will continue to parse correctly in future versions, but invalid strings that currently fail (e.g. 2017-01-01T00:00+00:00:00) are not guaranteed to continue failing in future versions if they encode a valid date.\n\nNew in version 2.7.0.", "api_code": "@_takes_ascii\ndef isoparse(self, dt_str):\n    components, pos = self._parse_isodate(dt_str)\n\n    if len(dt_str) > pos:\n        if self._sep is None or dt_str[pos:pos + 1] == self._sep:\n            components += self._parse_isotime(dt_str[pos + 1:])\n        else:\n            raise ValueError('String contains unknown ISO components')\n\n    if len(components) > 3 and components[3] == 24:\n        components[3] = 0\n        return datetime(*components) + timedelta(days=1)\n\n    return datetime(*components)"}
{"function_name": "statistics.mean", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom fractions import Fraction\nfrom decimal import Decimal\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_output_is_number_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, int, Fraction, Decimal))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_of_identical_values_property(data):\n    if len(set(data)) == 1:  # Check if all values are identical\n        result = statistics.mean(data)\n        assert result == data[0]\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_with_negatives_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, int, Fraction, Decimal))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_mean_empty_data_raises_error_property(data):\n    if not data:\n        try:\n            statistics.mean(data)\n            assert False, \"Expected StatisticsError for empty data\"\n        except statistics.StatisticsError:\n            pass  # Expected behavior\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_within_range_property(data):\n    result = statistics.mean(data)\n    assert result >= min(data) and result <= max(data)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom fractions import Fraction\nfrom decimal import Decimal\nimport pytest\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_return_value_is_number_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, int, Fraction, Decimal))\n    \n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_of_identical_values_property(data):\n    if len(data) > 0:\n        identical_value = data[0]\n        identical_data = [identical_value] * len(data)\n        result = statistics.mean(identical_data)\n        assert result == identical_value\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_of_dataset_with_negatives_property(data):\n    result = statistics.mean(data)\n    # The result could be negative, positive, or zero, so we just check it's a number\n    assert isinstance(result, (float, int, Fraction, Decimal))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_mean_of_empty_dataset_raises_statistics_error_property(data):\n    if len(data) == 0:\n        with pytest.raises(statistics.StatisticsError):\n            statistics.mean(data)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_within_range_of_min_max_property(data):\n    result = statistics.mean(data)\n    assert result >= min(data) and result <= max(data)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nimport numpy as np\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_is_number_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, int))  # Check if the result is a number\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_identical_values_property(data):\n    if len(data) > 0:\n        identical_value = data[0]\n        assert statistics.mean([identical_value] * len(data)) == identical_value\n\n@given(st.lists(st.floats(), min_size=1))\ndef test_mean_negative_numbers_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, int))  # Check if the result is a number\n    # Check if the mean doesn't necessarily need to be negative or positive\n\n@given()\ndef test_mean_empty_dataset_property():\n    try:\n        statistics.mean([])\n        assert False  # Should not reach this line\n    except statistics.StatisticsError:\n        pass  # Expected behavior\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_within_range_property(data):\n    result = statistics.mean(data)\n    assert result >= min(data) and result <= max(data)  # Mean should be within the range\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport statistics\nfrom fractions import Fraction\nfrom decimal import Decimal\nimport numpy as np\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_output_is_number_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, int, Fraction, Decimal))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_identical_values_property(data):\n    if len(set(data)) == 1:  # Only test if all elements are identical\n        result = statistics.mean(data)\n        assert result == data[0]\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_includes_negatives_property(data):\n    result = statistics.mean(data)\n    assert isinstance(result, (float, int, Fraction, Decimal))\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_mean_empty_dataset_raises_property(data):\n    if len(data) == 0:\n        try:\n            statistics.mean(data)\n            assert False  # Should not reach this line\n        except statistics.StatisticsError:\n            pass  # This is expected\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_within_range_property(data):\n    result = statistics.mean(data)\n    assert result >= min(data) and result <= max(data)\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom statistics import mean, StatisticsError\nimport math\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_output_is_number_property(data):\n    result = mean(data)\n    assert isinstance(result, (float, int)), \"The output should be a number.\"\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_identical_values_property(data):\n    if len(set(data)) == 1:  # All values are identical\n        result = mean(data)\n        assert result == data[0], \"The mean should equal the identical value.\"\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_negative_numbers_property(data):\n    result = mean(data)\n    assert isinstance(result, (float, int)), \"The output should be a number.\"\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=0))\ndef test_mean_empty_data_raises_property(data):\n    if len(data) == 0:\n        with pytest.raises(StatisticsError):\n            mean(data)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_mean_within_min_max_property(data):\n    result = mean(data)\n    assert min(data) <= result <= max(data), \"The mean should be within the range of min and max values.\"\n\n# End program\n```"], "properties": ["1. The mean of a non-empty dataset must always be a number (integer, float, Fraction, or Decimal) and cannot be NaN or infinite.\n\n2. The mean of a dataset containing identical values must equal that value (e.g., mean([5, 5, 5]) should return 5).\n\n3. The mean of a dataset that includes negative numbers must still yield a valid number, which could be negative or positive, depending on the values in the dataset.\n\n4. The mean of an empty dataset must raise a StatisticsError, ensuring that the function properly handles edge cases.\n\n5. The mean of a dataset must be within the range of the minimum and maximum values of that dataset (i.e., mean(data) should be \u2265 min(data) and \u2264 max(data))."], "api_doc": "statistics.mean(data)\nReturn the sample arithmetic mean of data which can be a sequence or iterable.\n\nThe arithmetic mean is the sum of the data divided by the number of data points. It is commonly called \u201cthe average\u201d, although it is only one of many different mathematical averages. It is a measure of the central location of the data.\n\nIf data is empty, StatisticsError will be raised.\n\nSome examples of use:\n\n>>>\nmean([1, 2, 3, 4, 4])\n2.8\nmean([-1.0, 2.5, 3.25, 5.75])\n2.625\n\nfrom fractions import Fraction as F\nmean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\nfrom decimal import Decimal as D\nmean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\nNote The mean is strongly affected by outliers and is not necessarily a typical example of the data points. For a more robust, although less efficient, measure of central tendency, see median().\nThe sample mean gives an unbiased estimate of the true population mean, so that when taken on average over all the possible samples, mean(sample) converges on the true mean of the entire population. If data represents the entire population rather than a sample, then mean(data) is equivalent to calculating the true population mean \u03bc.", "api_code": "def mean(data):\n    T, total, n = _sum(data)\n    if n < 1:\n        raise StatisticsError('mean requires at least one data point')\n    return _convert(total / n, T)"}
{"function_name": "networkx.algorithms.cycles.find_cycle", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), \n               st.sampled_from(['original', 'reverse', 'ignore', None]))\ndef test_find_cycle_non_empty_cycle_property(edges, orientation):\n    G = nx.Graph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation=orientation)\n        assert len(cycle) > 0  # There should be at least one edge if a cycle is found.\n    except NetworkXNoCycle:\n        pass  # No cycle found is valid.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), \n               st.sampled_from(['original', 'reverse', 'ignore', None]))\ndef test_find_cycle_valid_cycle_property(edges, orientation):\n    G = nx.Graph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation=orientation)\n        # Check if the last node of the last edge connects back to the first node of the first edge.\n        assert cycle[0][0] == cycle[-1][1]\n    except NetworkXNoCycle:\n        pass  # No cycle found is valid.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_find_cycle_original_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='original')\n        for u, v, direction in cycle:\n            assert (u, v) in G.edges()  # Edges must respect the original direction.\n    except NetworkXNoCycle:\n        pass  # No cycle found is valid.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_find_cycle_ignore_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='ignore')\n        for u, v, direction in cycle:\n            assert (u, v) in G.edges() or (v, u) in G.edges()  # Edges must be treated as undirected.\n    except NetworkXNoCycle:\n        pass  # No cycle found is valid.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100), \n               st.sampled_from(['original', 'reverse', 'ignore']))\ndef test_find_cycle_directional_information_property(edges, orientation):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation=orientation)\n        for u, v, direction in cycle:\n            assert direction in ['forward', 'reverse']  # Direction must be recorded if orientation is specified.\n    except NetworkXNoCycle:\n        pass  # No cycle found is valid.\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_cycle_found_property(edges):\n    G = nx.Graph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        assert len(cycle) > 0  # At least one edge should be present if a cycle is found.\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_cycle_validity_property(edges):\n    G = nx.Graph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        # Check that the last node of the last edge connects back to the first node of the first edge\n        assert cycle[0][0] == cycle[-1][1]  # Last node must connect back to the first node.\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_cycle_original_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='original')\n        for u, v, direction in cycle:\n            # Ensure the edges respect the original direction\n            assert (u, v) in G.edges()  # Edge must be in the original graph.\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_cycle_ignore_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='ignore')\n        # Ignore original direction, but edges should still connect\n        for i in range(len(cycle)):\n            u, v, direction = cycle[i]\n            # Check that the edge is treated as undirected\n            assert (u, v) in G.edges() or (v, u) in G.edges()\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable.\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True, min_size=1))\ndef test_cycle_directional_information_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='ignore')\n        for u, v, direction in cycle:\n            assert direction in ['forward', 'reverse']  # Directional information should be present.\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable.\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_cycle_not_empty_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        assert len(cycle) > 0  # Cycle must have at least one edge\n    except NetworkXNoCycle:\n        pass  # If no cycle, we do not assert anything\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_cycle_valid_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        # The last node of the last edge must connect back to the first node of the first edge\n        assert cycle[0][0] == cycle[-1][1]\n    except NetworkXNoCycle:\n        pass  # If no cycle, we do not assert anything\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_cycle_original_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='original')\n        for u, v in cycle:\n            assert (u, v) in G.edges()  # Edges must respect original direction\n    except NetworkXNoCycle:\n        pass  # If no cycle, we do not assert anything\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_cycle_ignore_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='ignore')\n        # The edges are treated as undirected, so we check both directions\n        for u, v in cycle:\n            assert (u, v) in G.edges() or (v, u) in G.edges()  # Edges must not respect direction\n    except NetworkXNoCycle:\n        pass  # If no cycle, we do not assert anything\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_cycle_directional_info_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='original')\n        for edge in cycle:\n            assert len(edge) == 3  # Each edge must include direction info when orientation is specified\n    except NetworkXNoCycle:\n        pass  # If no cycle, we do not assert anything\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_find_cycle_edges_property(edges):\n    G = nx.Graph(edges)\n    if nx.is_empty_graph(G):\n        return  # Skip empty graph\n    try:\n        cycle = nx.find_cycle(G)\n        assert len(cycle) >= 1  # There must be at least one edge in the cycle\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_find_cycle_valid_cycle_property(edges):\n    G = nx.Graph(edges)\n    if nx.is_empty_graph(G):\n        return  # Skip empty graph\n    try:\n        cycle = nx.find_cycle(G)\n        # Check if the last node of the last edge connects back to the first node of the first edge\n        assert cycle[0][0] == cycle[-1][1]  # The cycle must be valid\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_find_cycle_original_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_empty_graph(G):\n        return  # Skip empty graph\n    try:\n        cycle = nx.find_cycle(G, orientation='original')\n        # Check that all edges respect the original direction\n        for u, v in cycle:\n            assert (u, v) in G.edges()  # Edge must exist in the original orientation\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_find_cycle_ignore_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_empty_graph(G):\n        return  # Skip empty graph\n    try:\n        cycle = nx.find_cycle(G, orientation='ignore')\n        # Check that edges in the cycle can be traversed in any direction\n        for u, v in cycle:\n            assert (u, v) in G.edges() or (v, u) in G.edges()  # Either direction must exist\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=50).distinct())\ndef test_find_cycle_directional_info_property(edges):\n    G = nx.DiGraph(edges)\n    if nx.is_empty_graph(G):\n        return  # Skip empty graph\n    try:\n        cycle = nx.find_cycle(G, orientation='original')\n        # Check that each edge includes directional information\n        for edge in cycle:\n            assert len(edge) == 3  # Each edge should be of the form (u, v, direction)\n            assert edge[2] in ('forward', 'reverse')  # Direction must be either 'forward' or 'reverse'\n    except NetworkXNoCycle:\n        pass  # No cycle found, which is acceptable\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.exception import NetworkXNoCycle\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True))\ndef test_cycle_found_property(edges):\n    G = nx.Graph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        assert len(cycle) > 0  # There must be at least one edge in the cycle\n    except NetworkXNoCycle:\n        pass  # If no cycle is found, we do nothing\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True))\ndef test_cycle_validity_property(edges):\n    G = nx.Graph(edges)\n    try:\n        cycle = nx.find_cycle(G)\n        # Extract the nodes in the cycle\n        cycle_nodes = [u for u, v in cycle] + [cycle[-1][1]]  # Include last node to check cycle\n        assert cycle_nodes[0] == cycle_nodes[-1]  # The first and last nodes must be the same\n    except NetworkXNoCycle:\n        pass  # If no cycle is found, we do nothing\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True))\ndef test_original_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='original')\n        for (u, v) in cycle:\n            assert (u, v) in G.edges()  # Edges must respect the original orientation\n    except NetworkXNoCycle:\n        pass  # If no cycle is found, we do nothing\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True))\ndef test_ignore_orientation_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='ignore')\n        for (u, v) in cycle:\n            assert (u, v) in G.edges() or (v, u) in G.edges()  # Edges must be treated as undirected\n    except NetworkXNoCycle:\n        pass  # If no cycle is found, we do nothing\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), unique=True))\ndef test_directional_info_property(edges):\n    G = nx.DiGraph(edges)\n    try:\n        cycle = nx.find_cycle(G, orientation='original')\n        for (u, v, direction) in cycle:\n            assert direction in ['forward', 'reverse']  # Must include directional information\n    except NetworkXNoCycle:\n        pass  # If no cycle is found, we do nothing\n# End program\n```"], "properties": ["1. If a cycle is found, the output list of edges must contain at least one edge, indicating the presence of a cyclic path.\n\n2. The edges in the output must form a valid cycle, meaning the last node of the last edge must connect back to the first node of the first edge.\n\n3. For directed graphs, if the orientation is set to 'original', the output edges must respect the original direction of the edges in the graph.\n\n4. If the orientation is set to 'ignore', the output edges must not respect the direction of the edges, and the traversal should treat all edges as undirected.\n\n5. The output must include directional information (either 'forward' or 'reverse') for each edge if the orientation is specified as 'original', 'reverse', or 'ignore'."], "api_doc": "find_cycle\nfind_cycle(G, source=None, orientation=None)[source]\nReturns a cycle found via depth-first traversal.\n\nThe cycle is a list of edges indicating the cyclic path. Orientation of directed edges is controlled by orientation.\n\nParameters\n:\nG\ngraph\nA directed/undirected graph/multigraph.\n\nsource\nnode, list of nodes\nThe node from which the traversal begins. If None, then a source is chosen arbitrarily and repeatedly until all edges from each node in the graph are searched.\n\norientation\nNone | \u2018original\u2019 | \u2018reverse\u2019 | \u2018ignore\u2019 (default: None)\nFor directed graphs and directed multigraphs, edge traversals need not respect the original orientation of the edges. When set to \u2018reverse\u2019 every edge is traversed in the reverse direction. When set to \u2018ignore\u2019, every edge is treated as undirected. When set to \u2018original\u2019, every edge is treated as directed. In all three cases, the yielded edge tuples add a last entry to indicate the direction in which that edge was traversed. If orientation is None, the yielded edge has no direction indicated. The direction is respected, but not reported.\n\nReturns\n:\nedges\ndirected edges\nA list of directed edges indicating the path taken for the loop. If no cycle is found, then an exception is raised. For graphs, an edge is of the form (u, v) where u and v are the tail and head of the edge as determined by the traversal. For multigraphs, an edge is of the form (u, v, key), where key is the key of the edge. When the graph is directed, then u and v are always in the order of the actual directed edge. If orientation is not None then the edge tuple is extended to include the direction of traversal (\u2018forward\u2019 or \u2018reverse\u2019) on that edge.\n\nRaises\n:\nNetworkXNoCycle\nIf no cycle was found.\n\nSee also\n\nsimple_cycles\nExamples\n\nIn this example, we construct a DAG and find, in the first call, that there are no directed cycles, and so an exception is raised. In the second call, we ignore edge orientations and find that there is an undirected cycle. Note that the second call finds a directed cycle while effectively traversing an undirected graph, and so, we found an \u201cundirected cycle\u201d. This means that this DAG structure does not form a directed tree (which is also known as a polytree).\n\nG = nx.DiGraph([(0, 1), (0, 2), (1, 2)])\nnx.find_cycle(G, orientation=\"original\")\nTraceback (most recent call last):\n    ...\nnetworkx.exception.NetworkXNoCycle: No cycle found.\nlist(nx.find_cycle(G, orientation=\"ignore\"))\n[(0, 1, 'forward'), (1, 2, 'forward'), (0, 2, 'reverse')]", "api_code": "@nx._dispatchable\ndef find_cycle(G, source=None, orientation=None):\n    if not G.is_directed() or orientation in (None, \"original\"):\n\n        def tailhead(edge):\n            return edge[:2]\n\n    elif orientation == \"reverse\":\n\n        def tailhead(edge):\n            return edge[1], edge[0]\n\n    elif orientation == \"ignore\":\n\n        def tailhead(edge):\n            if edge[-1] == \"reverse\":\n                return edge[1], edge[0]\n            return edge[:2]\n\n    explored = set()\n    cycle = []\n    final_node = None\n    for start_node in G.nbunch_iter(source):\n        if start_node in explored:\n            # No loop is possible.\n            continue\n\n        edges = []\n        # All nodes seen in this iteration of edge_dfs\n        seen = {start_node}\n        # Nodes in active path.\n        active_nodes = {start_node}\n        previous_head = None\n\n        for edge in nx.edge_dfs(G, start_node, orientation):\n            # Determine if this edge is a continuation of the active path.\n            tail, head = tailhead(edge)\n            if head in explored:\n                # Then we've already explored it. No loop is possible.\n                continue\n            if previous_head is not None and tail != previous_head:\n                # This edge results from backtracking.\n                # Pop until we get a node whose head equals the current tail.\n                # So for example, we might have:\n                #  (0, 1), (1, 2), (2, 3), (1, 4)\n                # which must become:\n                #  (0, 1), (1, 4)\n                while True:\n                    try:\n                        popped_edge = edges.pop()\n                    except IndexError:\n                        edges = []\n                        active_nodes = {tail}\n                        break\n                    else:\n                        popped_head = tailhead(popped_edge)[1]\n                        active_nodes.remove(popped_head)\n\n                    if edges:\n                        last_head = tailhead(edges[-1])[1]\n                        if tail == last_head:\n                            break\n            edges.append(edge)\n\n            if head in active_nodes:\n                # We have a loop!\n                cycle.extend(edges)\n                final_node = head\n                break\n            else:\n                seen.add(head)\n                active_nodes.add(head)\n                previous_head = head\n\n        if cycle:\n            break\n        else:\n            explored.update(seen)\n\n    else:\n        assert len(cycle) == 0\n        raise nx.exception.NetworkXNoCycle(\"No cycle found.\")\n\n    # We now have a list of edges which ends on a cycle.\n    # So we need to remove from the beginning edges that are not relevant.\n\n    for i, edge in enumerate(cycle):\n        tail, head = tailhead(edge)\n        if tail == final_node:\n            break\n\n    return cycle[i:]"}
{"function_name": "networkx.algorithms.approximation.dominating_set.min_weighted_dominating_set", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_is_subset_of_nodes_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert result.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_is_dominating_set_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    for node in G.nodes:\n        if node not in result:\n            assert any(neighbor in result for neighbor in G.neighbors(node))\n\n@given(st.data())\ndef test_weight_constraint_with_weights_property(data):\n    weight_values = st.integers(min_value=1, max_value=100)\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weights = {node: data.draw(weight_values) for node in G.nodes}\n    nx.set_node_attributes(G, weights, 'weight')\n    \n    result = min_weighted_dominating_set(G, weight='weight')\n    \n    total_weight = sum(weights[node] for node in G.nodes)\n    min_weight_dominating_set_weight = sum(weights[node] for node in result)\n    \n    log_factor = (total_weight ** 0.5)  # Using square root to avoid overflow\n    assert min_weight_dominating_set_weight <= log_factor * min_weight_dominating_set_weight\n\n@given(st.data())\ndef test_output_weight_with_no_weights_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert sum(1 for node in result) == len(result)  # Each node has a weight of 1\n\n@given(st.data())\ndef test_networkx_not_implemented_exception_for_directed_graph_property(data):\n    G = nx.DiGraph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    try:\n        min_weighted_dominating_set(G)\n        assert False, \"Expected NetworkXNotImplemented exception\"\n    except nx.NetworkXNotImplemented:\n        pass  # Test passes if the exception is raised\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_is_subset_of_nodes_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert result.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_is_dominating_set_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    for node in G.nodes:\n        assert node in result or any(neighbor in result for neighbor in G.neighbors(node))\n\n@given(st.data())\ndef test_weight_bounds_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = data.draw(st.lists(st.tuples(st.integers(), st.floats(0, 100)), min_size=len(G.nodes)))\n    nx.set_node_attributes(G, dict(weight_attr), 'weight')\n    \n    result = min_weighted_dominating_set(G, weight='weight')\n    \n    total_weight = sum(G.nodes[n]['weight'] for n in G.nodes)\n    min_weight_dominating_set_weight = sum(G.nodes[n]['weight'] for n in result)\n    \n    log_weight = 0 if total_weight == 0 else log(total_weight)\n    assert min_weight_dominating_set_weight <= log_weight * total_weight\n\n@given(st.data())\ndef test_no_weight_assumed_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    # If no weights are given, each node is assumed to have weight 1\n    assert sum(1 for _ in result) == len(result)\n\n@given(st.data())\ndef test_directed_graph_exception_property(data):\n    G = nx.DiGraph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    try:\n        min_weighted_dominating_set(G)\n    except nx.NetworkXNotImplemented:\n        assert True\n    else:\n        assert False\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_is_subset_of_input_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert result.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_is_a_dominating_set_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    for node in G.nodes:\n        assert node in result or any(neighbor in result for neighbor in G.neighbors(node))\n\n@given(st.data())\ndef test_weight_property_with_weights(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weight_attr = {node: data.draw(st.integers(min_value=1, max_value=100)) for node in G.nodes}\n    nx.set_node_attributes(G, weight_attr, 'weight')\n    result = min_weighted_dominating_set(G, weight='weight')\n    total_weight = sum(weight_attr[node] for node in result)\n    total_graph_weight = sum(weight_attr.values())\n    \n    # Assuming w(V*) can be approximated as the weight of the result\n    w_V_star = total_weight\n    log_w_V = 1 if total_graph_weight == 0 else (total_graph_weight).bit_length()  # log base 2 approximation\n    assert total_weight <= log_w_V * w_V_star\n\n@given(st.data())\ndef test_weight_property_without_weights(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert sum(1 for _ in result) == len(result)  # Each node has weight of 1\n\n@given(st.data())\ndef test_raises_exception_for_directed_graph_property(data):\n    G = nx.DiGraph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    try:\n        min_weighted_dominating_set(G)\n        assert False, \"Expected NetworkXNotImplemented exception\"\n    except NotImplementedError:\n        pass  # Expected behavior\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_is_subset_of_nodes_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert result.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_is_dominating_set_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    for node in G.nodes:\n        if node not in result:\n            assert any(neighbor in result for neighbor in G.neighbors(node))\n\n@given(st.data())\ndef test_weight_constraint_with_weights_property(data):\n    weights = data.draw(st.dictionaries(st.integers(), st.floats(min_value=0.1, max_value=10.0)))\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    nx.set_node_attributes(G, weights, 'weight')\n    result = min_weighted_dominating_set(G, weight='weight')\n\n    total_weight_G = sum(weights.get(n, 1) for n in G.nodes)\n    # Calculate w(V^*) using a placeholder since we don't have the actual minimum weight dominating set weight\n    # This is to illustrate the property check; in practice, we would need to compute or know w(V^*)\n    w_V_star = 1  # placeholder\n    assert sum(weights.get(n, 1) for n in result) <= (np.log(total_weight_G) * w_V_star)\n\n@given(st.data())\ndef test_weight_constraint_without_weights_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert sum(1 for _ in result) == len(result)  # Each node assumed to have weight of 1\n\n@given(st.data())\ndef test_directed_graph_raises_exception_property(data):\n    G = nx.DiGraph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    try:\n        min_weighted_dominating_set(G)\n        assert False, \"Expected NetworkXNotImplemented exception\"\n    except NotImplementedError:\n        pass  # Expected behavior\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms.approximation import min_weighted_dominating_set\n\n@given(st.data())\ndef test_output_is_subset_of_nodes_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert result.issubset(G.nodes)\n\n@given(st.data())\ndef test_output_is_dominating_set_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    for node in G.nodes:\n        assert node in result or any(neighbor in result for neighbor in G.neighbors(node))\n\n@given(st.data())\ndef test_weight_bound_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    weights = data.draw(st.dictionaries(st.integers(), st.floats(min_value=0, max_value=100)))\n    nx.set_node_attributes(G, weights, 'weight')\n    result = min_weighted_dominating_set(G)\n    \n    total_weight = sum(weights[node] for node in G.nodes)\n    min_weight_dominating_set_weight = sum(weights[node] for node in result)\n    \n    log_w_v = 0 if total_weight == 0 else np.log(total_weight)\n    assert min_weight_dominating_set_weight <= log_w_v * min_weight_dominating_set_weight\n\n@given(st.data())\ndef test_no_weight_assumed_weight_property(data):\n    G = nx.Graph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    result = min_weighted_dominating_set(G)\n    assert sum(1 for _ in result) == len(result)  # Each node has weight of 1\n\n@given(st.data())\ndef test_raises_not_implemented_for_directed_graph_property(data):\n    G = nx.DiGraph(data.draw(st.lists(st.tuples(st.integers(), st.integers()), min_size=1)))\n    try:\n        min_weighted_dominating_set(G)\n        assert False, \"Expected NetworkXNotImplemented exception\"\n    except nx.NetworkXNotImplemented:\n        pass\n# End program\n```"], "properties": ["1. The output is a subset of the nodes in the input graph \\( G \\).\n\n2. The output set is a dominating set, meaning that every node in the graph \\( G \\) is either included in the output set or is adjacent to at least one node in the output set.\n\n3. If the weight attribute is provided, the sum of the weights of the nodes in the output set does not exceed \\( \\log(w(V)) \\times w(V^*) \\), where \\( w(V) \\) is the total weight of all nodes in \\( G \\) and \\( w(V^*) \\) is the weight of the minimum weight dominating set.\n\n4. If no weight attribute is specified, the output set still qualifies as a dominating set, and the sum of the weights is equal to the number of nodes in the output set, as each node is assumed to have a weight of one.\n\n5. The function raises a `NetworkXNotImplemented` exception if the input graph \\( G \\) is directed, indicating that the output is only valid for undirected graphs."], "api_doc": "min_weighted_dominating_set\nmin_weighted_dominating_set(G, weight=None)[source]\nReturns a dominating set that approximates the minimum weight node dominating set.\n\nParameters\n:\nG\nNetworkX graph\nUndirected graph.\n\nweight\nstring\nThe node attribute storing the weight of an node. If provided, the node attribute with this key must be a number for each node. If not provided, each node is assumed to have weight one.\n\nReturns\n:\nmin_weight_dominating_set\nset\nA set of nodes, the sum of whose weights is no more than (log w(V)) w(V^*), where w(V) denotes the sum of the weights of each node in the graph and w(V^*) denotes the sum of the weights of each node in the minimum weight dominating set.\n\nRaises\n:\nNetworkXNotImplemented\nIf G is directed.\n\nNotes\n\nThis algorithm computes an approximate minimum weighted dominating set for the graph G. The returned solution has weight (log w(V)) w(V^*), where w(V) denotes the sum of the weights of each node in the graph and w(V^*) denotes the sum of the weights of each node in the minimum weight dominating set for the graph.\n\nThis implementation of the algorithm runs in \n time, where \n is the number of edges in the graph.\n\nReferences\n\n[1]\nVazirani, Vijay V. Approximation Algorithms. Springer Science & Business Media, 2001.\n\nExamples\n\nG = nx.Graph([(0, 1), (0, 4), (1, 4), (1, 2), (2, 3), (3, 4), (2, 5)])\nnx.approximation.min_weighted_dominating_set(G)\n{1, 2, 4}", "api_code": "@not_implemented_for(\"directed\")\n@nx._dispatchable(node_attrs=\"weight\")\ndef min_weighted_dominating_set(G, weight=None):\n    # The unique dominating set for the null graph is the empty set.\n    if len(G) == 0:\n        return set()\n\n    # This is the dominating set that will eventually be returned.\n    dom_set = set()\n\n    def _cost(node_and_neighborhood):\n        \"\"\"Returns the cost-effectiveness of greedily choosing the given\n        node.\n\n        `node_and_neighborhood` is a two-tuple comprising a node and its\n        closed neighborhood.\n\n        \"\"\"\n        v, neighborhood = node_and_neighborhood\n        return G.nodes[v].get(weight, 1) / len(neighborhood - dom_set)\n\n    # This is a set of all vertices not already covered by the\n    # dominating set.\n    vertices = set(G)\n    # This is a dictionary mapping each node to the closed neighborhood\n    # of that node.\n    neighborhoods = {v: {v} | set(G[v]) for v in G}\n\n    # Continue until all vertices are adjacent to some node in the\n    # dominating set.\n    while vertices:\n        # Find the most cost-effective node to add, along with its\n        # closed neighborhood.\n        dom_node, min_set = min(neighborhoods.items(), key=_cost)\n        # Add the node to the dominating set and reduce the remaining\n        # set of nodes to cover.\n        dom_set.add(dom_node)\n        del neighborhoods[dom_node]\n        vertices -= min_set\n\n    return dom_set"}
{"function_name": "datetime.timedelta.total_seconds", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\nimport math\n\n@given(st.timedeltas(min_value=timedelta(0), max_value=timedelta(days=365*270)))\ndef test_total_seconds_non_negative_property(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedeltas())\ndef test_total_seconds_sum_of_components_property(td):\n    total_seconds = (td.days * 86400) + (td.seconds) + (td.microseconds / 1_000_000)\n    assert td.total_seconds() == total_seconds\n\n@given(st.timedeltas())\ndef test_total_seconds_consistency_property(td):\n    alternative_td = timedelta(days=td.days, seconds=td.seconds, microseconds=td.microseconds)\n    assert td.total_seconds() == alternative_td.total_seconds()\n\n@given(st.timedeltas(min_value=timedelta(days=365*270)))\ndef test_total_seconds_accuracy_loss_property(td):\n    if td.total_seconds() > 86400 * 365 * 270:  # More than 270 years\n        result = td.total_seconds()\n        assert isinstance(result, float)  # It may lose precision, but should be a float\n\n@given(st.timedeltas())\ndef test_total_seconds_zero_duration_property(td):\n    if td == timedelta(0):\n        assert td.total_seconds() == 0.0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedeltas())\ndef test_non_negative_seconds_property(td):\n    assert td.total_seconds() >= 0\n# End program\n\n@given(st.timedeltas())\ndef test_sum_of_components_property(td):\n    total_seconds_from_components = (td.days * 86400 + \n                                      td.seconds + \n                                      td.microseconds / 1_000_000)\n    assert td.total_seconds() == total_seconds_from_components\n# End program\n\n@given(st.timedeltas())\ndef test_consistent_representation_property(td):\n    # Create a timedelta using different components that should yield the same total duration\n    normalized_td = timedelta(days=td.days, seconds=td.seconds, microseconds=td.microseconds)\n    assert td.total_seconds() == normalized_td.total_seconds()\n# End program\n\n@given(st.timedeltas(max_value=timedelta(days=365 * 270)))  # Avoid very large inputs\ndef test_accuracy_loss_property(td):\n    result = td.total_seconds()\n    assert isinstance(result, float)  # Check if result is a float\n    # Note: Testing for exact accuracy loss is complex, so we just ensure it's a float here.\n# End program\n\n@given(st.timedeltas())\ndef test_zero_duration_property(td):\n    assert timedelta() == td and td.total_seconds() == 0 if (td.days == 0 and td.seconds == 0 and td.microseconds == 0) else True\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedeltas())\ndef test_non_negative_output_property(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedeltas())\ndef test_sum_of_components_property(td):\n    total_seconds = (td.days * 86400) + (td.seconds) + (td.microseconds / 1_000_000)\n    assert td.total_seconds() == total_seconds\n\n@given(st.timedeltas())\ndef test_consistency_of_representation_property(td):\n    assert td.total_seconds() == (timedelta(days=td.days, weeks=td.weeks, hours=td.hours,\n                                              minutes=td.minutes, seconds=td.seconds, microseconds=td.microseconds)).total_seconds()\n\n@given(st.timedeltas(max_value=timedelta(days=365 * 270)))  # Limiting to avoid overflow\ndef test_large_intervals_accuracy_property(td):\n    if td.days > 365 * 270:\n        # Skip the test for very large intervals\n        return\n    assert isinstance(td.total_seconds(), float)\n\n@given(st.timedeltas())\ndef test_zero_duration_property(td):\n    if td == timedelta(0):\n        assert td.total_seconds() == 0.0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedeltas())\ndef test_total_seconds_non_negative_property(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedeltas())\ndef test_total_seconds_equivalence_property(td):\n    total_seconds = td.total_seconds()\n    assert total_seconds == (td.days * 86400 + td.seconds + td.microseconds / 1_000_000)\n\n@given(st.timedeltas())\ndef test_total_seconds_consistency_property(td):\n    # Generate another timedelta with the same total duration\n    equivalent_td = timedelta(days=td.days, seconds=td.seconds, microseconds=td.microseconds)\n    assert td.total_seconds() == equivalent_td.total_seconds()\n\n@given(st.one_of(st.timedeltas(max_value=timedelta(days=365 * 270)),\n                 st.timedeltas(min_value=timedelta(days=0))))\ndef test_total_seconds_accuracy_property(td):\n    if td.total_seconds() > 0:\n        assert isinstance(td.total_seconds(), float)\n\n@given(st.timedeltas())\ndef test_total_seconds_zero_duration_property(td):\n    if td == timedelta(0):\n        assert td.total_seconds() == 0\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom datetime import timedelta\n\n@given(st.timedeltas(min_value=timedelta(seconds=0), max_value=timedelta(days=365*270)))\ndef test_total_seconds_non_negative(td):\n    assert td.total_seconds() >= 0\n\n@given(st.timedeltas())\ndef test_total_seconds_equivalent_to_components(td):\n    total_seconds = td.total_seconds()\n    assert total_seconds == (td.days * 86400 + td.seconds + td.microseconds / 1_000_000)\n\n@given(st.timedeltas())\ndef test_total_seconds_consistency_with_input_representation(td):\n    seconds_from_days = timedelta(days=td.days).total_seconds()\n    seconds_from_seconds = timedelta(seconds=td.seconds).total_seconds()\n    seconds_from_microseconds = timedelta(microseconds=td.microseconds).total_seconds()\n    assert td.total_seconds() == seconds_from_days + seconds_from_seconds + seconds_from_microseconds\n\n@given(st.timedeltas(max_value=timedelta(days=365*270)))\ndef test_total_seconds_loss_of_accuracy(td):\n    if td.total_seconds() > 2**63:\n        # Test should pass even if there's potential for loss of accuracy\n        pass\n    else:\n        assert td.total_seconds() == (td.days * 86400 + td.seconds + td.microseconds / 1_000_000)\n\n@given(st.timedeltas(min_value=timedelta(seconds=0)))\ndef test_total_seconds_zero_for_zero_duration(td):\n    if td == timedelta(0):\n        assert td.total_seconds() == 0\n# End program\n```"], "properties": ["1. The total number of seconds returned by `total_seconds()` should always be a non-negative float, as it represents a duration.\n2. The total number of seconds should be equal to the sum of the individual components of the `timedelta` object, converted into seconds.\n3. The total number of seconds should remain consistent regardless of the input representation (e.g., using days, weeks, hours, etc.) as long as the total duration is the same.\n4. For very large time intervals (greater than 270 years), the total number of seconds may lose microsecond accuracy, meaning the output may not accurately reflect the true duration.\n5. The total number of seconds should be zero if the `timedelta` object represents a duration of zero (i.e., all components are zero)."], "api_doc": "timedelta.total_seconds()\nReturn the total number of seconds contained in the duration. Equivalent to td / timedelta(seconds=1). For interval units other than seconds, use the division form directly (e.g. td / timedelta(microseconds=1)).\n\nNote that for very large time intervals (greater than 270 years on most platforms) this method will lose microsecond accuracy.\n\nAdded in version 3.2.\n\nExamples of usage: timedelta\nAn additional example of normalization:\n\n>>>\n# Components of another_year add up to exactly 365 days\nfrom datetime import timedelta\nyear = timedelta(days=365)\nanother_year = timedelta(weeks=40, days=84, hours=23,\n                         minutes=50, seconds=600)\nyear == another_year\nTrue\nyear.total_seconds()\n31536000.0", "api_code": "def total_seconds(self):\n        return ((self.days * 86400 + self.seconds) * 10**6 +\n                self.microseconds) / 10**6"}
{"function_name": "numpy.linalg.norm", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_vector_non_negativity_property(data):\n    vector = np.array(data)\n    norm_value = LA.norm(vector)\n    assert norm_value >= 0\n    if np.all(vector == 0):\n        assert norm_value == 0\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, min_size=1))\ndef test_matrix_non_negativity_property(data):\n    matrix = np.array(data)\n    norm_value = LA.norm(matrix)\n    assert norm_value >= 0\n    if np.all(matrix == 0):\n        assert norm_value == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_frobenius_and_flattened_2norm_property(data):\n    matrix = np.array(data).reshape(-1, len(data)//2) if len(data) % 2 == 0 else np.array(data).reshape(-1, len(data)//3)\n    frobenius_norm = LA.norm(matrix, 'fro')\n    flattened_2norm = LA.norm(matrix.ravel())\n    assert np.isclose(frobenius_norm, flattened_2norm)\n\n@given(st.floats(min_value=-1e6, max_value=1e6), st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_scalar_multiplication_property(scalar, data):\n    vector = np.array(data)\n    norm_original = LA.norm(vector)\n    norm_scaled = LA.norm(scalar * vector)\n    assert np.isclose(norm_scaled, abs(scalar) * norm_original)\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, min_size=1))\ndef test_axis_norm_consistency_property(data):\n    matrix = np.array(data)\n    axis_0_norm = LA.norm(matrix, axis=0)\n    axis_1_norm = LA.norm(matrix, axis=1)\n    total_norm = LA.norm(matrix)\n    assert np.isclose(np.sum(axis_0_norm), total_norm) or np.isclose(np.sum(axis_1_norm), total_norm)\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))  # Generate non-empty lists of floats\ndef test_vector_norm_non_negative_property(data):\n    norm_value = LA.norm(data)\n    assert norm_value >= 0\n    if all(x == 0 for x in data):\n        assert norm_value == 0  # Only zero vector has zero norm\n\n@given(st.lists(st.lists(st.floats(), min_size=1, max_size=10), min_size=1, max_size=10))  # Generate non-empty 2D lists of floats\ndef test_matrix_norm_non_negative_property(data):\n    norm_value = LA.norm(data)\n    assert norm_value >= 0\n    if all(all(x == 0 for x in row) for row in data):\n        assert norm_value == 0  # Only zero matrix has zero norm\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))  # Generate non-empty lists of floats\ndef test_frobenius_and_flattened_2norm_property(data):\n    if len(data) > 1:  # Ensure we have at least 2 elements for a matrix\n        matrix = np.array(data).reshape(-1, 2)  # Reshape into a 2D array\n        frobenius_norm = LA.norm(matrix, 'fro')\n        flattened_2norm = LA.norm(matrix.ravel())\n        assert np.isclose(frobenius_norm, flattened_2norm)\n\n@given(st.floats(min_value=-1000, max_value=1000), st.lists(st.floats(), min_size=1, max_size=1000))  # Generate a scalar and a list\ndef test_norm_scaling_property(scalar, data):\n    scaled_data = [x * scalar for x in data]\n    norm_value = LA.norm(data)\n    scaled_norm_value = LA.norm(scaled_data)\n    assert np.isclose(scaled_norm_value, abs(scalar) * norm_value)\n\n@given(st.lists(st.lists(st.floats(), min_size=1, max_size=10), min_size=1, max_size=10))  # Generate non-empty 2D lists of floats\ndef test_axis_norms_consistency_property(data):\n    if len(data) > 1 and len(data[0]) > 1:  # Ensure we have at least a 2D matrix\n        matrix = np.array(data)\n        overall_norm = LA.norm(matrix)\n        row_norms = LA.norm(matrix, axis=1)\n        assert np.isclose(overall_norm, np.sum(row_norms))  # This may vary based on norm type, but illustrates the idea\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_non_negative_norm_property(vector):\n    norm_value = LA.norm(vector)\n    assert norm_value >= 0\n    if all(v == 0 for v in vector):\n        assert norm_value == 0\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, max_size=5))\ndef test_zero_matrix_norm_property(matrix):\n    matrix_np = np.array(matrix)\n    norm_value = LA.norm(matrix_np)\n    assert norm_value >= 0\n    if np.all(matrix_np == 0):\n        assert norm_value == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_frobenius_equals_2norm_property(vector):\n    matrix_np = np.array(vector).reshape(-1, 1)  # Reshape to 2D\n    frobenius_norm = LA.norm(matrix_np, 'fro')\n    vector_norm = LA.norm(matrix_np.ravel())\n    assert np.isclose(frobenius_norm, vector_norm)\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1), st.floats())\ndef test_scalar_multiplication_property(vector, scalar):\n    norm_value = LA.norm(vector)\n    scaled_vector = [v * scalar for v in vector]\n    scaled_norm_value = LA.norm(scaled_vector)\n    assert np.isclose(scaled_norm_value, abs(scalar) * norm_value)\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, max_size=5))\ndef test_axis_consistency_property(matrix):\n    matrix_np = np.array(matrix)\n    norm_over_all = LA.norm(matrix_np)\n    norm_along_axis0 = LA.norm(matrix_np, axis=0)\n    norm_along_axis1 = LA.norm(matrix_np, axis=1)\n    \n    # Check if the sum of norms along axis 0 gives the overall norm (1-D case)\n    assert np.isclose(norm_over_all, np.sum(norm_along_axis0))\n    # Check if the sum of norms along axis 1 gives the overall norm (1-D case)\n    assert np.isclose(norm_over_all, np.sum(norm_along_axis1))\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_vector_non_negativity_property(data):\n    norm_value = LA.norm(data)\n    assert norm_value >= 0\n    if all(x == 0 for x in data):\n        assert norm_value == 0\n\n@given(st.lists(st.lists(st.floats(), min_size=1), min_size=1, max_size=100))\ndef test_matrix_non_negativity_property(data):\n    norm_value = LA.norm(data)\n    assert norm_value >= 0\n    if all(all(x == 0 for x in row) for row in data):\n        assert norm_value == 0\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000))\ndef test_frobenius_equals_flattened_2norm_property(data):\n    matrix = np.array(data).reshape(-1, 1)  # Reshape to 2-D for consistency\n    frobenius_norm = LA.norm(matrix, 'fro')\n    flattened_2norm = LA.norm(matrix.ravel())\n    assert np.isclose(frobenius_norm, flattened_2norm)\n\n@given(st.lists(st.floats(), min_size=1, max_size=1000), st.floats(allow_nan=False, allow_infinity=False))\ndef test_scaling_property(data, scalar):\n    scaled_data = [scalar * x for x in data]\n    assert np.isclose(LA.norm(scaled_data), abs(scalar) * LA.norm(data))\n\n@given(st.lists(st.lists(st.floats(), min_size=1), min_size=1, max_size=100))\ndef test_axis_norms_property(data):\n    matrix = np.array(data)\n    row_norms = LA.norm(matrix, axis=1)\n    col_norm = LA.norm(matrix, axis=0)\n    total_norm = LA.norm(matrix)\n    assert np.isclose(total_norm, np.sqrt(np.sum(row_norms**2)))  # Norm consistency across axes\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport numpy as np\nfrom numpy import linalg as LA\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_non_negative_norm_of_vector_property(vec):\n    norm_value = LA.norm(vec)\n    assert norm_value >= 0\n    if all(v == 0 for v in vec):\n        assert norm_value == 0\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1, min_size=1))\ndef test_non_negative_norm_of_matrix_property(mat):\n    mat_array = np.array(mat)\n    norm_value = LA.norm(mat_array)\n    assert norm_value >= 0\n    if np.all(mat_array == 0):\n        assert norm_value == 0\n\n@given(st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_frobenius_norm_equals_2norm_of_flattened_matrix_property(vec):\n    mat = np.array(vec).reshape(-1, 1)  # Reshape to 2-D\n    frobenius_norm = LA.norm(mat, 'fro')\n    flattened_2norm = LA.norm(mat.ravel())\n    assert np.isclose(frobenius_norm, flattened_2norm)\n\n@given(st.floats(allow_nan=False, allow_infinity=False), st.lists(st.floats(allow_nan=False, allow_infinity=False), min_size=1))\ndef test_scalar_multiplication_property(scalar, vec):\n    scaled_vec = [scalar * v for v in vec]\n    norm_value = LA.norm(vec)\n    scaled_norm_value = LA.norm(scaled_vec)\n    assert np.isclose(scaled_norm_value, abs(scalar) * norm_value)\n\n@given(st.lists(st.lists(st.floats(allow_nan=False, allow_infinity=False)), min_size=1))\ndef test_consistent_norm_across_axes_property(mat):\n    mat_array = np.array(mat)\n    if mat_array.ndim != 2:\n        return  # Skip if not a 2-D array\n    overall_norm = LA.norm(mat_array)\n    row_norms = LA.norm(mat_array, axis=1)\n    col_norms = LA.norm(mat_array, axis=0)\n\n    assert np.isclose(overall_norm, np.sqrt(np.sum(row_norms**2)))\n    assert np.isclose(overall_norm, np.sqrt(np.sum(col_norms**2)))\n\n# End program\n```"], "properties": ["1. The norm of a vector is always non-negative, and it is zero if and only if the vector is the zero vector.\n\n2. The norm of a matrix is non-negative, and it is zero if and only if the matrix is the zero matrix.\n\n3. The Frobenius norm of a matrix should be equal to the 2-norm of the matrix when calculated on a flattened version of the matrix.\n\n4. The result of the norm function should be invariant under scalar multiplication of the input (i.e., scaling the input by a constant should scale the norm by the absolute value of that constant).\n\n5. The norm computed along a specific axis should be consistent with the properties of norms, such that the overall norm of the array should equal the computed norms when aggregated appropriately along the specified axes."], "api_doc": "numpy.linalg.norm\nlinalg.norm(x, ord=None, axis=None, keepdims=False)[source]\nMatrix or vector norm.\n\nThis function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n\nParameters:\nxarray_like\nInput array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of x.ravel will be returned.\n\nord{non-zero int, inf, -inf, \u2018fro\u2019, \u2018nuc\u2019}, optional\nOrder of the norm (see table under Notes). inf means numpy\u2019s inf object. The default is None.\n\naxis{None, int, 2-tuple of ints}, optional.\nIf axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.\n\nNew in version 1.8.0.\n\nkeepdimsbool, optional\nIf this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n\nNew in version 1.10.0.\n\nReturns:\nn\nfloat or ndarray\nNorm of the matrix or vector(s).\n\nSee also\n\nscipy.linalg.norm\nSimilar function in SciPy.\n\nNotes\n\nFor values of ord < 1, the result is, strictly speaking, not a mathematical \u2018norm\u2019, but it may still be useful for various numerical purposes.\n\nThe following norms can be calculated:\n\nord\n\nnorm for matrices\n\nnorm for vectors\n\nNone\n\nFrobenius norm\n\n2-norm\n\n\u2018fro\u2019\n\nFrobenius norm\n\n\u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013\n\ninf\n\nmax(sum(abs(x), axis=1))\n\nmax(abs(x))\n\n-inf\n\nmin(sum(abs(x), axis=1))\n\nmin(abs(x))\n\n0\n\n\u2013\n\nsum(x != 0)\n\n1\n\nmax(sum(abs(x), axis=0))\n\nas below\n\n-1\n\nmin(sum(abs(x), axis=0))\n\nas below\n\n2\n\n2-norm (largest sing. value)\n\nas below\n\n-2\n\nsmallest singular value\n\nas below\n\nother\n\n\u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nThe Frobenius norm is given by [1]:\n\n\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when x.ndim != 2.\n\nReferences\n\n[1]\nG. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n\nExamples\n\nimport numpy as np\nfrom numpy import linalg as LA\na = np.arange(9) - 4\na\narray([-4, -3, -2, ...,  2,  3,  4])\nb = a.reshape((3, 3))\nb\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\nLA.norm(a)\n7.745966692414834\nLA.norm(b)\n7.745966692414834\nLA.norm(b, 'fro')\n7.745966692414834\nLA.norm(a, np.inf)\n4.0\nLA.norm(b, np.inf)\n9.0\nLA.norm(a, -np.inf)\n0.0\nLA.norm(b, -np.inf)\n2.0\nLA.norm(a, 1)\n20.0\nLA.norm(b, 1)\n7.0\nLA.norm(a, -1)\n-4.6566128774142013e-010\nLA.norm(b, -1)\n6.0\nLA.norm(a, 2)\n7.745966692414834\nLA.norm(b, 2)\n7.3484692283495345\nLA.norm(a, -2)\n0.0\nLA.norm(b, -2)\n1.8570331885190563e-016 # may vary\nLA.norm(a, 3)\n5.8480354764257312 # may vary\nLA.norm(a, -3)\n0.0\nUsing the axis argument to compute vector norms:\n\nc = np.array([[ 1, 2, 3],\n              [-1, 1, 4]])\nLA.norm(c, axis=0)\narray([ 1.41421356,  2.23606798,  5.        ])\nLA.norm(c, axis=1)\narray([ 3.74165739,  4.24264069])\nLA.norm(c, ord=1, axis=1)\narray([ 6.,  6.])\nUsing the axis argument to compute matrix norms:\n\nm = np.arange(8).reshape(2,2,2)\nLA.norm(m, axis=(1,2))\narray([  3.74165739,  11.22497216])\nLA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(3.7416573867739413, 11.224972160321824)", "api_code": "@array_function_dispatch(_norm_dispatcher)\ndef norm(x, ord=None, axis=None, keepdims=False):\n    x = asarray(x)\n\n    if not issubclass(x.dtype.type, (inexact, object_)):\n        x = x.astype(float)\n\n    # Immediately handle some default, simple, fast, and common cases.\n    if axis is None:\n        ndim = x.ndim\n        if (\n            (ord is None) or\n            (ord in ('f', 'fro') and ndim == 2) or\n            (ord == 2 and ndim == 1)\n        ):\n            x = x.ravel(order='K')\n            if isComplexType(x.dtype.type):\n                x_real = x.real\n                x_imag = x.imag\n                sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n            else:\n                sqnorm = x.dot(x)\n            ret = sqrt(sqnorm)\n            if keepdims:\n                ret = ret.reshape(ndim*[1])\n            return ret\n\n    # Normalize the `axis` argument to a tuple.\n    nd = x.ndim\n    if axis is None:\n        axis = tuple(range(nd))\n    elif not isinstance(axis, tuple):\n        try:\n            axis = int(axis)\n        except Exception as e:\n            raise TypeError(\n                \"'axis' must be None, an integer or a tuple of integers\"\n            ) from e\n        axis = (axis,)\n\n    if len(axis) == 1:\n        if ord == inf:\n            return abs(x).max(axis=axis, keepdims=keepdims)\n        elif ord == -inf:\n            return abs(x).min(axis=axis, keepdims=keepdims)\n        elif ord == 0:\n            # Zero norm\n            return (\n                (x != 0)\n                .astype(x.real.dtype)\n                .sum(axis=axis, keepdims=keepdims)\n            )\n        elif ord == 1:\n            # special case for speedup\n            return add.reduce(abs(x), axis=axis, keepdims=keepdims)\n        elif ord is None or ord == 2:\n            # special case for speedup\n            s = (x.conj() * x).real\n            return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n        # None of the str-type keywords for ord ('fro', 'nuc')\n        # are valid for vectors\n        elif isinstance(ord, str):\n            raise ValueError(f\"Invalid norm order '{ord}' for vectors\")\n        else:\n            absx = abs(x)\n            absx **= ord\n            ret = add.reduce(absx, axis=axis, keepdims=keepdims)\n            ret **= reciprocal(ord, dtype=ret.dtype)\n            return ret\n    elif len(axis) == 2:\n        row_axis, col_axis = axis\n        row_axis = normalize_axis_index(row_axis, nd)\n        col_axis = normalize_axis_index(col_axis, nd)\n        if row_axis == col_axis:\n            raise ValueError('Duplicate axes given.')\n        if ord == 2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amax)\n        elif ord == -2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amin)\n        elif ord == 1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).max(axis=col_axis)\n        elif ord == inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).max(axis=row_axis)\n        elif ord == -1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).min(axis=col_axis)\n        elif ord == -inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).min(axis=row_axis)\n        elif ord in [None, 'fro', 'f']:\n            ret = sqrt(add.reduce((x.conj() * x).real, axis=axis))\n        elif ord == 'nuc':\n            ret = _multi_svd_norm(x, row_axis, col_axis, sum)\n        else:\n            raise ValueError(\"Invalid norm order for matrices.\")\n        if keepdims:\n            ret_shape = list(x.shape)\n            ret_shape[axis[0]] = 1\n            ret_shape[axis[1]] = 1\n            ret = ret.reshape(ret_shape)\n        return ret\n    else:\n        raise ValueError(\"Improper number of dimensions to norm.\")"}
{"function_name": "decimal.Decimal.adjusted", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport decimal\n\n@given(st.decimals(min_value=decimal.Decimal('-1e100'), max_value=decimal.Decimal('1e100')))\ndef test_adjusted_exponent_bounds_property(d):\n    adjusted_value = d.adjusted()\n    assert adjusted_value >= d.as_tuple().exponent\n\n@given(st.decimals(min_value=decimal.Decimal('-1e100'), max_value=decimal.Decimal('1e100')))\ndef test_adjusted_exponent_limit_property(d):\n    adjusted_value = d.adjusted()\n    assert adjusted_value <= d.as_tuple().exponent + len(d.as_tuple().digits) - 1\n\n@given(st.decimals(allow_nan=False, allow_infinite=False))\ndef test_adjusted_exponent_zero_property(d):\n    if d.is_zero():\n        assert d.adjusted() == decimal.Decimal('-Infinity')\n\n@given(st.decimals(min_value=decimal.Decimal('1e-100'), max_value=decimal.Decimal('1e100')))\ndef test_adjusted_exponent_digit_count_property(d):\n    if d != 0:\n        adjusted_value = d.adjusted()\n        assert adjusted_value == len(d.as_tuple().digits) - 1\n\n@given(st.decimals(min_value=decimal.Decimal('-1e100'), max_value=decimal.Decimal('1e100')))\ndef test_adjusted_exponent_invariance_property(d):\n    d_repr_1 = decimal.Decimal(str(d))\n    d_repr_2 = decimal.Decimal(d_repr_1)\n    assert d_repr_1.adjusted() == d_repr_2.adjusted()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport decimal\n\n@given(st.decimals(min_value=decimal.Decimal('-1e+1000'), max_value=decimal.Decimal('1e+1000')))\ndef test_adjusted_exponent_minimum_property(d):\n    assert d.adjusted() >= d.as_tuple().exponent\n\n@given(st.decimals(min_value=decimal.Decimal('-1e+1000'), max_value=decimal.Decimal('1e+1000')))\ndef test_adjusted_exponent_maximum_property(d):\n    assert d.adjusted() <= d.as_tuple().exponent + len(d.as_tuple().digits) - 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_adjusted_exponent_zero_property(d):\n    if d.is_zero():\n        assert d.adjusted() == decimal.Decimal('Infinity').adjusted()  # or a predefined value for zero\n\n@given(st.decimals(min_value=decimal.Decimal('-1e+1000'), max_value=decimal.Decimal('1e+1000')))\ndef test_adjusted_exponent_digit_count_property(d):\n    if d.is_zero():\n        return  # Skip as there's no significant digit\n    assert d.adjusted() == len(d.as_tuple().digits) - 1\n\n@given(st.decimals(min_value=decimal.Decimal('-1e+1000'), max_value=decimal.Decimal('1e+1000')))\ndef test_adjusted_exponent_consistency_property(d):\n    d1 = decimal.Decimal(d)\n    d2 = decimal.Decimal(str(d))\n    assert d1.adjusted() == d2.adjusted()\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport decimal\n\n@given(st.decimals(min_value=decimal.Decimal('-1e100'), max_value=decimal.Decimal('1e100')))\ndef test_adjusted_exponent_min_property(d):\n    adjusted_value = d.adjusted()\n    assert adjusted_value >= d.as_tuple().exponent\n\n@given(st.decimals(min_value=decimal.Decimal('-1e100'), max_value=decimal.Decimal('1e100')))\ndef test_adjusted_exponent_max_property(d):\n    adjusted_value = d.adjusted()\n    assert adjusted_value <= d.as_tuple().exponent\n\n@given(st.decimals(0))  # Only zero input\ndef test_adjusted_exponent_zero_property(d):\n    adjusted_value = d.adjusted()\n    assert adjusted_value == decimal.Decimal(-float('inf'))\n\n@given(st.decimals(min_value=decimal.Decimal('1e-100'), max_value=decimal.Decimal('1e100')))\ndef test_adjusted_exponent_digit_count_property(d):\n    if d != 0:  # Ensure we're not testing with zero\n        adjusted_value = d.adjusted()\n        digit_count = len(d.as_tuple().digits)\n        assert adjusted_value == digit_count - 1\n\n@given(st.decimals(min_value=decimal.Decimal('-1e100'), max_value=decimal.Decimal('1e100')))\ndef test_adjusted_exponent_invariance_property(d):\n    d1 = d\n    d2 = decimal.Decimal(str(d1))  # Ensure another representation\n    assert d1.adjusted() == d2.adjusted()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation\n\n@given(st.decimals())\ndef test_adjusted_exponent_minimum_property(decimal_value):\n    try:\n        adjusted_value = decimal_value.adjusted()\n        assert adjusted_value >= decimal_value.as_tuple().exponent\n    except InvalidOperation:\n        pass  # Handle the case for zero or invalid Decimals\n\n@given(st.decimals())\ndef test_adjusted_exponent_maximum_property(decimal_value):\n    try:\n        adjusted_value = decimal_value.adjusted()\n        assert adjusted_value <= decimal_value.as_tuple().exponent + len(decimal_value.as_tuple().digits) - 1\n    except InvalidOperation:\n        pass  # Handle the case for zero or invalid Decimals\n\n@given(st.decimals())\ndef test_adjusted_exponent_zero_property(decimal_value):\n    if decimal_value == 0:\n        assert decimal_value.adjusted() == float('-inf')\n\n@given(st.decimals())\ndef test_adjusted_exponent_digit_count_property(decimal_value):\n    try:\n        adjusted_value = decimal_value.adjusted()\n        digit_count = len(decimal_value.as_tuple().digits)\n        assert adjusted_value == digit_count - 1\n    except InvalidOperation:\n        pass  # Handle the case for zero or invalid Decimals\n\n@given(st.decimals())\ndef test_adjusted_exponent_invariance_property(decimal_value):\n    if decimal_value != 0:\n        representation_1 = Decimal(str(decimal_value))\n        representation_2 = Decimal(decimal_value)\n        assert representation_1.adjusted() == representation_2.adjusted()\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nfrom decimal import Decimal, InvalidOperation\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_adjusted_exponent_minimum_property(decimal_value):\n    if decimal_value != 0:\n        adjusted_value = decimal_value.adjusted()\n        assert adjusted_value >= decimal_value.as_tuple().exponent\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_adjusted_exponent_maximum_property(decimal_value):\n    if decimal_value != 0:\n        adjusted_value = decimal_value.adjusted()\n        assert adjusted_value <= decimal_value.as_tuple().exponent + len(decimal_value.as_tuple().digits) - 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_adjusted_exponent_zero_property(decimal_value):\n    if decimal_value == 0:\n        assert decimal_value.adjusted() == float('-inf')\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_adjusted_exponent_digit_count_property(decimal_value):\n    if decimal_value != 0:\n        adjusted_value = decimal_value.adjusted()\n        digit_count = len(decimal_value.as_tuple().digits)\n        assert adjusted_value == digit_count - 1\n\n@given(st.decimals(allow_nan=False, allow_infinity=False))\ndef test_adjusted_exponent_invariance_property(decimal_value):\n    if decimal_value != 0:\n        representation_1 = Decimal(str(decimal_value))\n        representation_2 = Decimal(representation_1.to_eng_string())\n        assert representation_1.adjusted() == representation_2.adjusted()\n# End program\n```"], "properties": ["1. The adjusted exponent is always greater than or equal to the minimum exponent of the Decimal object, ensuring that it reflects the position of the most significant digit correctly.\n\n2. The adjusted exponent is always less than or equal to the maximum exponent of the Decimal object, confirming that it does not exceed the limits of the coefficient's representation.\n\n3. For any Decimal object with a value of zero, the adjusted exponent should consistently return a specific value (typically negative infinity), indicating that there are no significant digits.\n\n4. The adjusted exponent should be equal to the number of digits in the coefficient minus one, ensuring that it accurately represents the position of the most significant digit in relation to the decimal point.\n\n5. The adjusted exponent should be invariant under different representations of the same numerical value (e.g., Decimal('0.001') and Decimal('1e-3') should yield the same adjusted value), demonstrating consistency across various forms of input."], "api_doc": "adjusted()\nReturn the adjusted exponent after shifting out the coefficient\u2019s rightmost digits until only the lead digit remains: Decimal('321e+5').adjusted() returns seven. Used for determining the position of the most significant digit with respect to the decimal point.", "api_code": "def adjusted(self):\n    try:\n        return self._exp + len(self._int) - 1\n    # If NaN or Infinity, self._exp is string\n    except TypeError:\n        return 0"}
{"function_name": "html.escape", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_contains_no_special_characters_property(input_string):\n    output_string = html.escape(input_string)\n    assert '&' not in output_string\n    assert '<' not in output_string\n    assert '>' not in output_string\n\n@given(st.text(), st.booleans())\ndef test_output_contains_no_quotes_when_quote_true_property(input_string, quote):\n    output_string = html.escape(input_string, quote=True)\n    if quote:\n        assert '\"' not in output_string\n        assert \"'\" not in output_string\n\n@given(st.text())\ndef test_output_length_property(input_string):\n    output_string = html.escape(input_string)\n    assert len(output_string) >= len(input_string) or (len(output_string) == len(input_string) and not any(c in input_string for c in ['&', '<', '>', '\"', \"'\"]))\n\n@given(st.text())\ndef test_output_is_valid_html_property(input_string):\n    output_string = html.escape(input_string)\n    # A simple check for valid HTML: it should not contain unescaped special characters\n    assert all(c not in output_string for c in ['&', '<', '>', '\"', \"'\"])\n\n@given(st.text())\ndef test_output_unchanged_when_no_special_characters_property(input_string):\n    if not any(c in input_string for c in ['&', '<', '>', '\"', \"'\"]):\n        output_string = html.escape(input_string)\n        assert output_string == input_string\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_should_not_contain_special_characters_property(input_string):\n    output_string = html.escape(input_string)\n    assert '&' not in output_string\n    assert '<' not in output_string\n    assert '>' not in output_string\n\n@given(st.text())\ndef test_output_should_not_contain_quotes_when_quote_is_true_property(input_string):\n    output_string = html.escape(input_string, quote=True)\n    assert '\"' not in output_string\n    assert \"'\" not in output_string\n\n@given(st.text())\ndef test_output_length_should_be_greater_than_or_equal_to_input_length_property(input_string):\n    output_string = html.escape(input_string)\n    assert len(output_string) >= len(input_string)\n\n@given(st.text())\ndef test_output_should_be_valid_html_property(input_string):\n    output_string = html.escape(input_string)\n    # This is a placeholder for a more thorough HTML validation.\n    # In practice, you would need an HTML parser to validate this properly.\n    assert isinstance(output_string, str)\n\n@given(st.text())\ndef test_output_should_remain_unchanged_for_no_special_characters_property(input_string):\n    if not any(c in input_string for c in ['&', '<', '>', '\"', \"'\"]):\n        output_string = html.escape(input_string)\n        assert output_string == input_string\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_should_not_contain_ampersand_property(s):\n    result = html.escape(s)\n    assert '&' not in result\n\n@given(st.text())\ndef test_output_should_not_contain_less_than_property(s):\n    result = html.escape(s)\n    assert '<' not in result\n\n@given(st.text())\ndef test_output_should_not_contain_greater_than_property(s):\n    result = html.escape(s)\n    assert '>' not in result\n\n@given(st.text())\ndef test_output_should_not_contain_double_quote_when_quote_true_property(s):\n    result = html.escape(s, quote=True)\n    assert '\"' not in result\n\n@given(st.text())\ndef test_output_should_not_contain_single_quote_when_quote_true_property(s):\n    result = html.escape(s, quote=True)\n    assert \"'\" not in result\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_should_not_contain_escaped_characters_property(s):\n    output = html.escape(s)\n    assert '&' not in output\n    assert '<' not in output\n    assert '>' not in output\n\n@given(st.text())\ndef test_output_should_not_contain_quote_characters_when_quote_is_true_property(s):\n    output = html.escape(s, quote=True)\n    assert '\"' not in output\n    assert \"'\" not in output\n\n@given(st.text())\ndef test_output_length_should_be_greater_than_or_equal_to_input_length_when_escaping_property(s):\n    output = html.escape(s)\n    if any(char in s for char in '&<>'):\n        assert len(output) >= len(s)\n    else:\n        assert len(output) == len(s)\n\n@given(st.text())\ndef test_output_should_be_valid_html_property(s):\n    output = html.escape(s)\n    # Checking for the presence of unescaped characters that can disrupt HTML rendering\n    assert not any(char in output for char in ['&', '<', '>', '\"', \"'\"])\n\n@given(st.text())\ndef test_output_should_remain_unchanged_when_no_special_characters_property(s):\n    output = html.escape(s)\n    if not any(char in s for char in '&<>'):\n        assert output == s\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport html\n\n@given(st.text())\ndef test_output_should_not_contain_ampersand_property(s):\n    escaped = html.escape(s)\n    assert '&' not in escaped\n\n@given(st.text())\ndef test_output_should_not_contain_less_than_property(s):\n    escaped = html.escape(s)\n    assert '<' not in escaped\n\n@given(st.text())\ndef test_output_should_not_contain_greater_than_property(s):\n    escaped = html.escape(s)\n    assert '>' not in escaped\n\n@given(st.text(), st.booleans())\ndef test_output_length_should_be_greater_than_or_equal_to_input_length_property(s, quote):\n    escaped = html.escape(s, quote)\n    assert len(escaped) >= len(s)\n\n@given(st.text())\ndef test_output_should_be_valid_html_property(s):\n    escaped = html.escape(s)\n    # This check is not exhaustive, but it ensures that the critical characters are escaped.\n    assert all(char not in escaped for char in ['<', '>', '&'])\n\n# End program\n```"], "properties": ["1. The output string should not contain any of the characters &, <, or > if they are present in the input string.\n2. If the `quote` parameter is set to `True`, the output string should also not contain the characters \" or ' if they are present in the input string.\n3. The length of the output string should be greater than or equal to the length of the input string when any characters are escaped.\n4. The output string should be a valid HTML string, meaning it should not contain any unescaped special characters that could disrupt HTML rendering.\n5. The output should remain unchanged if the input string does not contain any characters that require escaping."], "api_doc": "html.escape(s, quote=True)\nConvert the characters &, < and > in string s to HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the optional flag quote is true, the characters (\") and (') are also translated; this helps for inclusion in an HTML attribute value delimited by quotes, as in <a href=\"...\">.\n\nAdded in version 3.2.", "api_code": "def escape(s, quote=True):\n    s = s.replace(\"&\", \"&amp;\") # Must be done first!\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace(\">\", \"&gt;\")\n    if quote:\n        s = s.replace('\"', \"&quot;\")\n        s = s.replace('\\'', \"&#x27;\")\n    return s"}
{"function_name": "networkx.algorithms.bipartite.basic.color", "pbt": ["```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\nfrom hypothesis import HealthCheck, settings\n\n# Settings to suppress certain warnings about large tests\nsettings.register_profile(\"ci\", max_examples=50, suppress_health_checks=[HealthCheck.too_slow])\nsettings.set_profile(\"ci\")\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_output_dictionary_size_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    try:\n        c = bipartite.color(G)\n        assert len(c) == G.number_of_nodes()\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_output_color_values_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    try:\n        c = bipartite.color(G)\n        assert all(color in {0, 1} for color in c.values())\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_connected_nodes_different_colors_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    try:\n        c = bipartite.color(G)\n        for u, v in G.edges():\n            assert c[u] != c[v]\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_valid_bipartite_coloring_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    try:\n        c = bipartite.color(G)\n        for u, v in G.edges():\n            assert c[u] != c[v]\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1, max_size=100))\ndef test_networkxerror_on_nonbipartite_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    # Add odd cycle to ensure it's non-bipartite\n    if len(edges) > 1:\n        G.add_edge(edges[0][0], edges[0][1])  # Create a cycle\n    try:\n        bipartite.color(G)\n        assert G.is_bipartite() or G.number_of_edges() < 3  # If we have a cycle and edges, it's non-bipartite\n    except nx.NetworkXError:\n        pass  # This is expected for non-bipartite graphs\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\n\n@given(st.data())\ndef test_output_length_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    if bipartite.is_bipartite(G):\n        c = bipartite.color(G)\n        assert len(c) == len(G.nodes)\n\n@given(st.data())\ndef test_color_value_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    if bipartite.is_bipartite(G):\n        c = bipartite.color(G)\n        for color in c.values():\n            assert color in {0, 1}\n\n@given(st.data())\ndef test_adjacent_nodes_diff_color_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    if bipartite.is_bipartite(G):\n        c = bipartite.color(G)\n        for u, v in G.edges:\n            assert c[u] != c[v]\n\n@given(st.data())\ndef test_bipartite_coloring_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=1)))\n    if bipartite.is_bipartite(G):\n        c = bipartite.color(G)\n        for u, v in G.edges:\n            assert c[u] != c[v]\n\n@given(st.data())\ndef test_non_bipartite_graph_error_property(data):\n    G = data.draw(st.builds(nx.Graph, nodes=st.lists(st.integers(), min_size=3), edges=st.lists(st.tuples(st.integers(), st.integers()))))\n    if not bipartite.is_bipartite(G):\n        try:\n            bipartite.color(G)\n            assert False, \"Expected NetworkXError for non-bipartite graph\"\n        except nx.NetworkXError:\n            pass\n\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_output_length_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    try:\n        c = bipartite.color(G)\n        assert len(c) == G.number_of_nodes()\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_output_color_values_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    try:\n        c = bipartite.color(G)\n        for color in c.values():\n            assert color in {0, 1}\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_adjacent_nodes_different_colors_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    try:\n        c = bipartite.color(G)\n        for u, v in G.edges():\n            assert c[u] != c[v]\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_valid_bipartite_coloring_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    try:\n        c = bipartite.color(G)\n        for u, v in G.edges():\n            assert c[u] != c[v]\n    except nx.NetworkXError:\n        pass  # Expected for non-bipartite graphs\n\n@given(st.lists(st.tuples(st.integers(), st.integers()), min_size=1))\ndef test_exception_for_non_bipartite_graph_property(edges):\n    G = nx.Graph()\n    G.add_edges_from(edges)\n    if not nx.is_bipartite(G):\n        try:\n            bipartite.color(G)\n            assert False  # Should raise an exception\n        except nx.NetworkXError:\n            assert True  # Exception raised as expected\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\nfrom networkx.exception import NetworkXError\n\n@given(st.data())\ndef test_output_size_property(data):\n    # Generate a bipartite graph\n    G = nx.bipartite.random_graph(10, 10, p=0.5)\n    c = bipartite.color(G)\n    assert len(c) == len(G.nodes)\n\n@given(st.data())\ndef test_color_values_property(data):\n    # Generate a bipartite graph\n    G = nx.bipartite.random_graph(10, 10, p=0.5)\n    c = bipartite.color(G)\n    assert all(color in [0, 1] for color in c.values())\n\n@given(st.data())\ndef test_adjacent_nodes_different_colors_property(data):\n    # Generate a bipartite graph\n    G = nx.bipartite.random_graph(10, 10, p=0.5)\n    c = bipartite.color(G)\n    for u, v in G.edges:\n        assert c[u] != c[v]\n\n@given(st.data())\ndef test_valid_bipartite_coloring_property(data):\n    # Generate a bipartite graph\n    G = nx.bipartite.random_graph(10, 10, p=0.5)\n    c = bipartite.color(G)\n    for u, v in G.edges:\n        assert c[u] != c[v]\n\n@given(st.data())\ndef test_non_bipartite_graph_raises_exception_property(data):\n    # Generate a non-bipartite graph\n    G = nx.complete_graph(3)  # A triangle is not bipartite\n    try:\n        bipartite.color(G)\n        assert False  # Should not reach this line\n    except NetworkXError:\n        assert True  # Exception was raised as expected\n# End program\n```", "```python\nfrom hypothesis import given, strategies as st\nimport networkx as nx\nfrom networkx.algorithms import bipartite\nfrom networkx.exception import NetworkXError\n\n@given(st.data())\ndef test_output_length_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.path_graph, st.integers(min_value=1, max_value=1000)),\n        st.builds(nx.complete_bipartite_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100))\n    ))\n    c = bipartite.color(G)\n    assert len(c) == G.number_of_nodes()\n\n@given(st.data())\ndef test_color_values_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.path_graph, st.integers(min_value=1, max_value=1000)),\n        st.builds(nx.complete_bipartite_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100))\n    ))\n    c = bipartite.color(G)\n    assert all(color in {0, 1} for color in c.values())\n\n@given(st.data())\ndef test_edges_different_colors_property(data):\n    G = data.draw(st.builds(nx.complete_bipartite_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)))\n    c = bipartite.color(G)\n    for u, v in G.edges():\n        assert c[u] != c[v]\n\n@given(st.data())\ndef test_bipartite_coloring_property(data):\n    G = data.draw(st.one_of(\n        st.builds(nx.complete_bipartite_graph, st.integers(min_value=1, max_value=100), st.integers(min_value=1, max_value=100)),\n        st.builds(nx.path_graph, st.integers(min_value=1, max_value=1000))\n    ))\n    c = bipartite.color(G)\n    for u, v in G.edges():\n        assert c[u] != c[v]\n\n@given(st.data())\ndef test_non_bipartite_graph_raises_error_property(data):\n    G = data.draw(st.builds(nx.complete_graph, st.integers(min_value=3, max_value=100)))\n    try:\n        bipartite.color(G)\n        assert False, \"Expected NetworkXError for non-bipartite graph\"\n    except NetworkXError:\n        pass\n# End program\n```"], "properties": ["1. The output dictionary must have the same number of entries as the number of nodes in the graph \\( G \\).\n\n2. Each node in the output dictionary must be assigned a color value of either 0 or 1.\n\n3. Nodes that are directly connected by an edge in the graph \\( G \\) must have different color values in the output dictionary.\n\n4. If the input graph \\( G \\) is bipartite, the output dictionary must represent a valid bipartite coloring, meaning there should be no edges connecting nodes of the same color.\n\n5. If the input graph \\( G \\) is not bipartite, the function must raise a `NetworkXError` exception, indicating that the graph cannot be two-colored."], "api_doc": "color\ncolor(G)[source]\nReturns a two-coloring of the graph.\n\nRaises an exception if the graph is not bipartite.\n\nParameters\n:\nG\nNetworkX graph\nReturns\n:\ncolor\ndictionary\nA dictionary keyed by node with a 1 or 0 as data for each node color.\n\nRaises\n:\nNetworkXError\nIf the graph is not two-colorable.\n\nExamples\n\nfrom networkx.algorithms import bipartite\nG = nx.path_graph(4)\nc = bipartite.color(G)\nprint(c)\n{0: 1, 1: 0, 2: 1, 3: 0}\nYou can use this to set a node attribute indicating the bipartite set:\n\nnx.set_node_attributes(G, c, \"bipartite\")\nprint(G.nodes[0][\"bipartite\"])\n1\nprint(G.nodes[1][\"bipartite\"])\n0", "api_code": "@nx._dispatchable\ndef color(G):\n    if G.is_directed():\n        import itertools\n\n        def neighbors(v):\n            return itertools.chain.from_iterable([G.predecessors(v), G.successors(v)])\n\n    else:\n        neighbors = G.neighbors\n\n    color = {}\n    for n in G:  # handle disconnected graphs\n        if n in color or len(G[n]) == 0:  # skip isolates\n            continue\n        queue = [n]\n        color[n] = 1  # nodes seen with color (1 or 0)\n        while queue:\n            v = queue.pop()\n            c = 1 - color[v]  # opposite color of node v\n            for w in neighbors(v):\n                if w in color:\n                    if color[w] == color[v]:\n                        raise nx.NetworkXError(\"Graph is not bipartite.\")\n                else:\n                    color[w] = c\n                    queue.append(w)\n    # color isolates with 0\n    color.update(dict.fromkeys(nx.isolates(G), 0))\n    return color"}
