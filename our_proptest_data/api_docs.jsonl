{"function_name": "decimal_Decimal_compare", "api_doc": "compare(other, context=None)\nCompare the values of two Decimal instances. compare() returns a Decimal instance, and if either operand is a NaN then the result is a NaN:\n\na or b is a NaN  ==> Decimal('NaN')\na < b            ==> Decimal('-1')\na == b           ==> Decimal('0')\na > b            ==> Decimal('1')"}
{"function_name": "statistics_variance", "api_doc": "statistics.variance(data, xbar=None)\nReturn the sample variance of data, an iterable of at least two real-valued numbers. Variance, or second moment about the mean, is a measure of the variability (spread or dispersion) of data. A large variance indicates that the data is spread out; a small variance indicates it is clustered closely around the mean.\n\nIf the optional second argument xbar is given, it should be the mean of data. If it is missing or None (the default), the mean is automatically calculated.\n\nUse this function when your data is a sample from a population. To calculate the variance from the entire population, see pvariance().\n\nRaises StatisticsError if data has fewer than two values.\n\nExamples:\n\n>>> data = [2.75, 1.75, 1.25, 0.25, 0.5, 1.25, 3.5]\n>>> variance(data)\n1.3720238095238095\nIf you have already calculated the mean of your data, you can pass it as the optional second argument xbar to avoid recalculation:\n\n>>> m = mean(data)\n>>> variance(data, m)\n1.3720238095238095\nThis function does not attempt to verify that you have passed the actual mean as xbar. Using arbitrary values for xbar can lead to invalid or impossible results.\n\nDecimal and Fraction values are supported:\n\n>>> from decimal import Decimal as D\n>>> variance([D(\"27.5\"), D(\"30.25\"), D(\"30.25\"), D(\"34.5\"), D(\"41.75\")])\nDecimal('31.01875')\n\n>>> from fractions import Fraction as F\n>>> variance([F(1, 6), F(1, 2), F(5, 3)])\nFraction(67, 108)\nNote This is the sample variance s\u00b2 with Bessel\u2019s correction, also known as variance with N-1 degrees of freedom. Provided that the data points are representative (e.g. independent and identically distributed), the result should be an unbiased estimate of the true population variance.\nIf you somehow know the actual population mean \u03bc you should pass it to the pvariance() function as the mu parameter to get the variance of a sample."}
{"function_name": "html_escape", "api_doc": "html.escape(s, quote=True)\nConvert the characters &, < and > in string s to HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the optional flag quote is true, the characters (\") and (') are also translated; this helps for inclusion in an HTML attribute value delimited by quotes, as in <a href=\"...\">.\n\nNew in version 3.2."}
{"function_name": "cryptography_fernet_Fernet_encrypt", "api_doc": "class cryptography.fernet.Fernet(key)[source]\uf0c1\nThis class provides both encryption and decryption facilities.\n\n>>> from cryptography.fernet import Fernet\n>>> key = Fernet.generate_key()\n>>> f = Fernet(key)\n>>> token = f.encrypt(b\"my deep dark secret\")\n>>> token\nb'...'\n>>> f.decrypt(token)\nb'my deep dark secret'\nParameters:\nkey (bytes or str) \u2013 A URL-safe base64-encoded 32-byte key. This must be kept secret. Anyone with this key is able to create and read messages.\n\nclassmethod generate_key()[source]\uf0c1\nGenerates a fresh fernet key. Keep this some place safe! If you lose it you\u2019ll no longer be able to decrypt messages; if anyone else gains access to it, they\u2019ll be able to decrypt all of your messages, and they\u2019ll also be able forge arbitrary messages that will be authenticated and decrypted.\n\nencrypt(data)[source]\uf0c1\nEncrypts data passed. The result of this encryption is known as a \u201cFernet token\u201d and has strong privacy and authenticity guarantees.\n\nParameters:\ndata (bytes) \u2013 The message you would like to encrypt.\n\nReturns bytes:\nA secure message that cannot be read or altered without the key. It is URL-safe base64-encoded. This is referred to as a \u201cFernet token\u201d.\n\nRaises:\nTypeError \u2013 This exception is raised if data is not bytes.\n\nNote\n\nThe encrypted message contains the current time when it was generated in plaintext, the time a message was created will therefore be visible to a possible attacker."}
{"function_name": "datetime_date_isocalendar", "api_doc": "date.isocalendar()\nReturn a named tuple object with three components: year, week and weekday.\n\nThe ISO calendar is a widely used variant of the Gregorian calendar. 3\n\nThe ISO year consists of 52 or 53 full weeks, and where a week starts on a Monday and ends on a Sunday. The first week of an ISO year is the first (Gregorian) calendar week of a year containing a Thursday. This is called week number 1, and the ISO year of that Thursday is the same as its Gregorian year.\n\nFor example, 2004 begins on a Thursday, so the first week of ISO year 2004 begins on Monday, 29 Dec 2003 and ends on Sunday, 4 Jan 2004:\n\n>>> from datetime import date\n>>> date(2003, 12, 29).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=1)\n>>> date(2004, 1, 4).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=7)\nChanged in version 3.9: Result changed from a tuple to a named tuple."}
{"function_name": "html_unescape", "api_doc": "html.unescape(s)\nConvert all named and numeric character references (e.g. &gt;, &#62;, &#x3e;) in the string s to the corresponding Unicode characters. This function uses the rules defined by the HTML 5 standard for both valid and invalid character references, and the list of HTML 5 named character references.\n\nNew in version 3.4."}
{"function_name": "cryptography_fernet_Fernet_decrypt", "api_doc": "classcryptography.fernet.Fernet(key)[source]\uf0c1\nThis class provides both encryption and decryption facilities.\n\nfrom cryptography.fernet import Fernet\nkey = Fernet.generate_key()\nf = Fernet(key)\ntoken = f.encrypt(b\"my deep dark secret\")\ntoken\nb'...'\nf.decrypt(token)\nb'my deep dark secret'\nParameters\n:\nkey (bytes or str) \u2013 A URL-safe base64-encoded 32-byte key. This must be kept secret. Anyone with this key is able to create and read messages.\n\nclassmethodgenerate_key()[source]\uf0c1\nGenerates a fresh fernet key. Keep this some place safe! If you lose it you\u2019ll no longer be able to decrypt messages; if anyone else gains access to it, they\u2019ll be able to decrypt all of your messages, and they\u2019ll also be able forge arbitrary messages that will be authenticated and decrypted.\n\nencrypt(data)[source]\uf0c1\nEncrypts data passed. The result of this encryption is known as a \u201cFernet token\u201d and has strong privacy and authenticity guarantees.\n\nParameters\n:\ndata (bytes) \u2013 The message you would like to encrypt.\n\nReturns bytes\n:\nA secure message that cannot be read or altered without the key. It is URL-safe base64-encoded. This is referred to as a \u201cFernet token\u201d.\n\nRaises\n:\nTypeError \u2013 This exception is raised if data is not bytes.\n\nNote\n\nThe encrypted message contains the current time when it was generated in plaintext, the time a message was created will therefore be visible to a possible attacker.\n\nencrypt_at_time(data, current_time)[source]\uf0c1\nNew in version 3.0.\n\nEncrypts data passed using explicitly passed current time. See encrypt() for the documentation of the data parameter, the return type and the exceptions raised.\n\nThe motivation behind this method is for the client code to be able to test token expiration. Since this method can be used in an insecure manner one should make sure the correct time (int(time.time())) is passed as current_time outside testing.\n\nParameters\n:\ncurrent_time (int) \u2013 The current time.\n\nNote\n\nSimilarly to encrypt() the encrypted message contains the timestamp in plaintext, in this case the timestamp is the value of the current_time parameter.\n\ndecrypt(token, ttl=None)[source]\uf0c1\nDecrypts a Fernet token. If successfully decrypted you will receive the original plaintext as the result, otherwise an exception will be raised. It is safe to use this data immediately as Fernet verifies that the data has not been tampered with prior to returning it.\n\nParameters\n:\ntoken (bytes or str) \u2013 The Fernet token. This is the result of calling encrypt().\n\nttl (int) \u2013 Optionally, the number of seconds old a message may be for it to be valid. If the message is older than ttl seconds (from the time it was originally created) an exception will be raised. If ttl is not provided (or is None), the age of the message is not considered.\n\nReturns bytes\n:\nThe original plaintext.\n\nRaises\n:\ncryptography.fernet.InvalidToken \u2013 If the token is in any way invalid, this exception is raised. A token may be invalid for a number of reasons: it is older than the ttl, it is malformed, or it does not have a valid signature.\n\nTypeError \u2013 This exception is raised if token is not bytes or str.\n\ndecrypt_at_time(token, ttl, current_time)[source]\uf0c1\nNew in version 3.0.\n\nDecrypts a token using explicitly passed current time. See decrypt() for the documentation of the token and ttl parameters (ttl is required here), the return type and the exceptions raised.\n\nThe motivation behind this method is for the client code to be able to test token expiration. Since this method can be used in an insecure manner one should make sure the correct time (int(time.time())) is passed as current_time outside testing.\n\nParameters\n:\ncurrent_time (int) \u2013 The current time."}
{"function_name": "zlib_decompress", "api_doc": "zlib.decompress(data, /, wbits=MAX_WBITS, bufsize=DEF_BUF_SIZE)\nDecompresses the bytes in data, returning a bytes object containing the uncompressed data. The wbits parameter depends on the format of data, and is discussed further below. If bufsize is given, it is used as the initial size of the output buffer. Raises the error exception if any error occurs.\n\nThe wbits parameter controls the size of the history buffer (or \u201cwindow size\u201d), and what header and trailer format is expected. It is similar to the parameter for compressobj(), but accepts more ranges of values:\n\n+8 to +15: The base-two logarithm of the window size. The input must include a zlib header and trailer.\n\n0: Automatically determine the window size from the zlib header. Only supported since zlib 1.2.3.5.\n\n\u22128 to \u221215: Uses the absolute value of wbits as the window size logarithm. The input must be a raw stream with no header or trailer.\n\n+24 to +31 = 16 + (8 to 15): Uses the low 4 bits of the value as the window size logarithm. The input must include a gzip header and trailer.\n\n+40 to +47 = 32 + (8 to 15): Uses the low 4 bits of the value as the window size logarithm, and automatically accepts either the zlib or gzip format.\n\nWhen decompressing a stream, the window size must not be smaller than the size originally used to compress the stream; using a too-small value may result in an error exception. The default wbits value corresponds to the largest window size and requires a zlib header and trailer to be included.\n\nbufsize is the initial size of the buffer used to hold decompressed data. If more space is required, the buffer size will be increased as needed, so you don\u2019t have to get this value exactly right; tuning it will only save a few calls to malloc().\n\nChanged in version 3.6: wbits and bufsize can be used as keyword arguments."}
{"function_name": "decimal_Decimal_fma", "api_doc": "fma(other, third, context=None)\nFused multiply-add. Return self*other+third with no rounding of the intermediate product self*other.\n\n>>> Decimal(2).fma(3, 5)\nDecimal('11')"}
{"function_name": "pandas_merge", "api_doc": "pandas.merge\npandas.merge(left, right, how='inner', on=None, left_on=None, right_on=None, left_index=False, right_index=False, sort=False, suffixes=('_x', '_y'), copy=None, indicator=False, validate=None)[source]\nMerge DataFrame or named Series objects with a database-style join.\n\nA named Series object is treated as a DataFrame with a single named column.\n\nThe join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored. Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. When performing a cross merge, no column specifications to merge on are allowed.\n\nWarning\n\nIf both key columns contain rows where the key is a null value, those rows will be matched against each other. This is different from usual SQL join behaviour and can lead to unexpected results.\n\nParameters:\nleft DataFrame or named Series\nright DataFrame or named Series\nObject to merge with.\n\nhow{\u2018left\u2019, \u2018right\u2019, \u2018outer\u2019, \u2018inner\u2019, \u2018cross\u2019}, default \u2018inner\u2019\nType of merge to be performed.\n\nleft: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n\nright: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n\nouter: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n\ninner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\n\ncross: creates the cartesian product from both frames, preserves the order of the left keys.\n\nNew in version 1.2.0.\n\non label or list\nColumn or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames.\n\nleft_on label or list, or array-like\nColumn or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns.\n\nright_on label or list, or array-like\nColumn or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns.\n\nleft_index bool, default False\nUse the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels.\n\nright_index bool, default False\nUse the index from the right DataFrame as the join key. Same caveats as left_index.\n\nsort bool, default False\nSort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword).\n\nsuffixes list-like, default is (\u201c_x\u201d, \u201c_y\u201d)\nA length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None.\n\ncopy bool, default True\nIf False, avoid copy if possible.\n\nindicator bool or str, default False\nIf True, adds a column to the output DataFrame called \u201c_merge\u201d with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \u201cleft_only\u201d for observations whose merge key only appears in the left DataFrame, \u201cright_only\u201d for observations whose merge key only appears in the right DataFrame, and \u201cboth\u201d if the observation\u2019s merge key is found in both DataFrames.\n\nvalidate str, optional\nIf specified, checks if merge is of specified type.\n\n\u201cone_to_one\u201d or \u201c1:1\u201d: check if merge keys are unique in both left and right datasets.\n\n\u201cone_to_many\u201d or \u201c1:m\u201d: check if merge keys are unique in left dataset.\n\n\u201cmany_to_one\u201d or \u201cm:1\u201d: check if merge keys are unique in right dataset.\n\n\u201cmany_to_many\u201d or \u201cm:m\u201d: allowed, but does not result in checks.\n\nReturns:\nDataFrame\nA DataFrame of the two merged objects.\n\nSee also\n\nmerge_ordered\nMerge with optional filling/interpolation.\n\nmerge_asof\nMerge on nearest keys.\n\nDataFrame.join\nSimilar method using indices.\n\nExamples\n\n>>> df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': [1, 2, 3, 5]})\n>>> df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'],\n                    'value': [5, 6, 7, 8]})\n>>> df1\n    lkey value\n0   foo      1\n1   bar      2\n2   baz      3\n3   foo      5\n>>> df2\n    rkey value\n0   foo      5\n1   bar      6\n2   baz      7\n3   foo      8\nMerge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey')\n  lkey  value_x rkey  value_y\n0  foo        1  foo        5\n1  foo        1  foo        8\n2  foo        5  foo        5\n3  foo        5  foo        8\n4  bar        2  bar        6\n5  baz        3  baz        7\nMerge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey',\n          suffixes=('_left', '_right'))\n  lkey  value_left rkey  value_right\n0  foo           1  foo            5\n1  foo           1  foo            8\n2  foo           5  foo            5\n3  foo           5  foo            8\n4  bar           2  bar            6\n5  baz           3  baz            7\nMerge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns.\n\n>>> df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False))\nTraceback (most recent call last):\n...\nValueError: columns overlap but no suffix specified:\n    Index(['value'], dtype='object')\n>>> df1 = pd.DataFrame({'a': ['foo', 'bar'], 'b': [1, 2]})\n>>> df2 = pd.DataFrame({'a': ['foo', 'baz'], 'c': [3, 4]})\n>>> df1\n      a  b\n0   foo  1\n1   bar  2\n>>> df2\n      a  c\n0   foo  3\n1   baz  4\n>>> df1.merge(df2, how='inner', on='a')\n      a  b  c\n0   foo  1  3\n>>> df1.merge(df2, how='left', on='a')\n      a  b  c\n0   foo  1  3.0\n1   bar  2  NaN\n>>> df1 = pd.DataFrame({'left': ['foo', 'bar']})\n>>> df2 = pd.DataFrame({'right': [7, 8]})\n>>> df1\n    left\n0   foo\n1   bar\n>>> df2\n    right\n0   7\n1   8\n>>> df1.merge(df2, how='cross')\n   left  right\n0   foo      7\n1   foo      8\n2   bar      7\n3   bar      8"}
{"function_name": "networkx_find_cycle", "api_doc": "find_cycle\nfind_cycle(G, source=None, orientation=None)[source]\nReturns a cycle found via depth-first traversal.\n\nThe cycle is a list of edges indicating the cyclic path. Orientation of directed edges is controlled by orientation.\n\nParameters: \nG: graph\nA directed/undirected graph/multigraph.\n\nsource: node, list of nodes\nThe node from which the traversal begins. If None, then a source is chosen arbitrarily and repeatedly until all edges from each node in the graph are searched.\n\norientation: None | \u2018original\u2019 | \u2018reverse\u2019 | \u2018ignore\u2019 (default: None)\nFor directed graphs and directed multigraphs, edge traversals need not respect the original orientation of the edges. When set to \u2018reverse\u2019 every edge is traversed in the reverse direction. When set to \u2018ignore\u2019, every edge is treated as undirected. When set to \u2018original\u2019, every edge is treated as directed. In all three cases, the yielded edge tuples add a last entry to indicate the direction in which that edge was traversed. If orientation is None, the yielded edge has no direction indicated. The direction is respected, but not reported.\n\nReturns:\nedges: directed edges\nA list of directed edges indicating the path taken for the loop. If no cycle is found, then an exception is raised. For graphs, an edge is of the form (u, v) where u and v are the tail and head of the edge as determined by the traversal. For multigraphs, an edge is of the form (u, v, key), where key is the key of the edge. When the graph is directed, then u and v are always in the order of the actual directed edge. If orientation is not None then the edge tuple is extended to include the direction of traversal (\u2018forward\u2019 or \u2018reverse\u2019) on that edge.\n\nRaises:\nNetworkXNoCycle: If no cycle was found.\n\nSee also\n\nsimple_cycles\nExamples\n\nIn this example, we construct a DAG and find, in the first call, that there are no directed cycles, and so an exception is raised. In the second call, we ignore edge orientations and find that there is an undirected cycle. Note that the second call finds a directed cycle while effectively traversing an undirected graph, and so, we found an \u201cundirected cycle\u201d. This means that this DAG structure does not form a directed tree (which is also known as a polytree).\n\n>>> G = nx.DiGraph([(0, 1), (0, 2), (1, 2)])\n>>> nx.find_cycle(G, orientation=\"original\")\nTraceback (most recent call last):\n    ...\nnetworkx.exception.NetworkXNoCycle: No cycle found.\n>>> list(nx.find_cycle(G, orientation=\"ignore\"))\n[(0, 1, 'forward'), (1, 2, 'forward'), (0, 2, 'reverse')]"}
{"function_name": "numpy_add", "api_doc": "numpy.add\nnumpy.add(x1, x2, /, out=None, *, where=True, casting='same_kind', order='K', dtype=None, subok=True[, signature, extobj]) = <ufunc 'add'>\nAdd arguments element-wise.\n\nParameters:\nx1, x2 array_like\nThe arrays to be added. If x1.shape != x2.shape, they must be broadcastable to a common shape (which becomes the shape of the output).\n\nout ndarray, None, or tuple of ndarray and None, optional\nA location into which the result is stored. If provided, it must have a shape that the inputs broadcast to. If not provided or None, a freshly-allocated array is returned. A tuple (possible only as a keyword argument) must have length equal to the number of outputs.\n\nwhere array_like, optional\nThis condition is broadcast over the input. At locations where the condition is True, the out array will be set to the ufunc result. Elsewhere, the out array will retain its original value. Note that if an uninitialized out array is created via the default out=None, locations within it where the condition is False will remain uninitialized.\n\n**kwargs\nFor other keyword-only arguments, see the ufunc docs.\n\nReturns:\nadd ndarray or scalar\nThe sum of x1 and x2, element-wise. This is a scalar if both x1 and x2 are scalars.\n\nNotes\n\nEquivalent to x1 + x2 in terms of array broadcasting.\n\nExamples\n\n>>> np.add(1.0, 4.0)\n5.0\n>>> x1 = np.arange(9.0).reshape((3, 3))\n>>> x2 = np.arange(3.0)\n>>> np.add(x1, x2)\narray([[  0.,   2.,   4.],\n       [  3.,   5.,   7.],\n       [  6.,   8.,  10.]])\nThe + operator can be used as a shorthand for np.add on ndarrays.\n\n>>> x1 = np.arange(9.0).reshape((3, 3))\n>>> x2 = np.arange(3.0)\n>>> x1 + x2\narray([[ 0.,  2.,  4.],\n       [ 3.,  5.,  7.],\n       [ 6.,  8., 10.]])"}
{"function_name": "networkx_maximum_flow", "api_doc": "maximum_flow\nmaximum_flow(flowG, _s, _t, capacity='capacity', flow_func=None, **kwargs)[source]\nFind a maximum single-commodity flow.\n\nParameters:\nflowG: NetworkX graph\nEdges of the graph are expected to have an attribute called \u2018capacity\u2019. If this attribute is not present, the edge is considered to have infinite capacity.\n\n_s: node\nSource node for the flow.\n\n_t: node\nSink node for the flow.\n\ncapacity: string\nEdges of the graph G are expected to have an attribute capacity that indicates how much flow the edge can support. If this attribute is not present, the edge is considered to have infinite capacity. Default value: \u2018capacity\u2019.\n\nflow_func: function\nA function for computing the maximum flow among a pair of nodes in a capacitated graph. The function has to accept at least three parameters: a Graph or Digraph, a source node, and a target node. And return a residual network that follows NetworkX conventions (see Notes). If flow_func is None, the default maximum flow function (preflow_push()) is used. See below for alternative algorithms. The choice of the default function may change from version to version and should not be relied on. Default value: None.\n\nkwargs: Any other keyword parameter is passed to the function that\ncomputes the maximum flow.\n\nReturns:\nflow_value: integer, float\nValue of the maximum flow, i.e., net outflow from the source.\n\nflow_dict\ndict\nA dictionary containing the value of the flow that went through each edge.\n\nRaises:\nNetworkXError\nThe algorithm does not support MultiGraph and MultiDiGraph. If the input graph is an instance of one of these two classes, a NetworkXError is raised.\n\nNetworkXUnbounded\nIf the graph has a path of infinite capacity, the value of a feasible flow on the graph is unbounded above and the function raises a NetworkXUnbounded.\n\nSee also\n\nmaximum_flow_value()\nminimum_cut()\nminimum_cut_value()\nedmonds_karp()\npreflow_push()\nshortest_augmenting_path()\nNotes\n\nThe function used in the flow_func parameter has to return a residual network that follows NetworkX conventions:\n\nThe residual network R from an input graph G has the same nodes as G. R is a DiGraph that contains a pair of edges (u, v) and (v, u) iff (u, v) is not a self-loop, and at least one of (u, v) and (v, u) exists in G.\n\nFor each edge (u, v) in R, R[u][v]['capacity'] is equal to the capacity of (u, v) in G if it exists in G or zero otherwise. If the capacity is infinite, R[u][v]['capacity'] will have a high arbitrary finite value that does not affect the solution of the problem. This value is stored in R.graph['inf']. For each edge (u, v) in R, R[u][v]['flow'] represents the flow function of (u, v) and satisfies R[u][v]['flow'] == -R[v][u]['flow'].\n\nThe flow value, defined as the total flow into t, the sink, is stored in R.graph['flow_value']. Reachability to t using only edges (u, v) such that R[u][v]['flow'] < R[u][v]['capacity'] induces a minimum s-t cut.\n\nSpecific algorithms may store extra data in R.\n\nThe function should supports an optional boolean parameter value_only. When True, it can optionally terminate the algorithm as soon as the maximum flow value and the minimum cut can be determined.\n\nExamples\n\n>>> G = nx.DiGraph()\n>>> G.add_edge(\"x\", \"a\", capacity=3.0)\n>>> G.add_edge(\"x\", \"b\", capacity=1.0)\n>>> G.add_edge(\"a\", \"c\", capacity=3.0)\n>>> G.add_edge(\"b\", \"c\", capacity=5.0)\n>>> G.add_edge(\"b\", \"d\", capacity=4.0)\n>>> G.add_edge(\"d\", \"e\", capacity=2.0)\n>>> G.add_edge(\"c\", \"y\", capacity=2.0)\n>>> G.add_edge(\"e\", \"y\", capacity=3.0)\nmaximum_flow returns both the value of the maximum flow and a dictionary with all flows.\n\n>>> flow_value, flow_dict = nx.maximum_flow(G, \"x\", \"y\")\n>>> flow_value\n3.0\n>>> print(flow_dict[\"x\"][\"b\"])\n1.0\nYou can also use alternative algorithms for computing the maximum flow by using the flow_func parameter.\n\n>>> from networkx.algorithms.flow import shortest_augmenting_path\n>>> flow_value == nx.maximum_flow(G, \"x\", \"y\", flow_func=shortest_augmenting_path)[0]\nTrue"}
{"function_name": "datetime_datetime_fromtimestamp", "api_doc": "classmethod datetime.fromtimestamp(timestamp, tz=None)\nReturn the local date and time corresponding to the POSIX timestamp, such as is returned by time.time(). If optional argument tz is None or not specified, the timestamp is converted to the platform\u2019s local date and time, and the returned datetime object is naive.\n\nIf tz is not None, it must be an instance of a tzinfo subclass, and the timestamp is converted to tz\u2019s time zone.\n\nfromtimestamp() may raise OverflowError, if the timestamp is out of the range of values supported by the platform C localtime() or gmtime() functions, and OSError on localtime() or gmtime() failure. It\u2019s common for this to be restricted to years in 1970 through 2038. Note that on non-POSIX systems that include leap seconds in their notion of a timestamp, leap seconds are ignored by fromtimestamp(), and then it\u2019s possible to have two timestamps differing by a second that yield identical datetime objects. This method is preferred over utcfromtimestamp().\n\nChanged in version 3.3: Raise OverflowError instead of ValueError if the timestamp is out of the range of values supported by the platform C localtime() or gmtime() functions. Raise OSError instead of ValueError on localtime() or gmtime() failure.\n\nChanged in version 3.6: fromtimestamp() may return instances with fold set to 1."}
{"function_name": "numpy_dot", "api_doc": "numpy.dot\nnumpy.dot(a, b, out=None)\nDot product of two arrays. Specifically,\n\nIf both a and b are 1-D arrays, it is inner product of vectors (without complex conjugation).\n\nIf both a and b are 2-D arrays, it is matrix multiplication, but using matmul or a @ b is preferred.\n\nIf either a or b is 0-D (scalar), it is equivalent to multiply and using numpy.multiply(a, b) or a * b is preferred.\n\nIf a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.\n\nIf a is an N-D array and b is an M-D array (where M>=2), it is a sum product over the last axis of a and the second-to-last axis of b:\n\ndot(a, b)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])\nIt uses an optimized BLAS library when possible (see numpy.linalg).\n\nParameters: a array_like\nFirst argument.\n\nb array_like\nSecond argument.\n\nout ndarray, optional\nOutput argument. This must have the exact kind that would be returned if it was not used. In particular, it must have the right type, must be C-contiguous, and its dtype must be the dtype that would be returned for dot(a,b). This is a performance feature. Therefore, if these conditions are not met, an exception is raised, instead of attempting to be flexible.\n\nReturns:\noutput ndarray\nReturns the dot product of a and b. If a and b are both scalars or both 1-D arrays then a scalar is returned; otherwise an array is returned. If out is given, then it is returned.\n\nRaises:\nValueError\nIf the last dimension of a is not the same size as the second-to-last dimension of b.\n\nSee also\n\nvdot\nComplex-conjugating dot product.\n\ntensordot\nSum products over arbitrary axes.\n\neinsum\nEinstein summation convention.\n\nmatmul\n\u2018@\u2019 operator as method with out parameter.\n\nlinalg.multi_dot\nChained dot product.\n\nExamples\n\n>>> np.dot(3, 4)\n12\nNeither argument is complex-conjugated:\n\n>>> np.dot([2j, 3j], [2j, 3j])\n(-13+0j)\nFor 2-D arrays it is the matrix product:\n\n>>> a = [[1, 0], [0, 1]]\nb>>>  = [[4, 1], [2, 2]]\nn>>> p.dot(a, b)\narray([[4, 1],\n       [2, 2]])\n>>> a = np.arange(3*4*5*6).reshape((3,4,5,6))\n>>> b = np.arange(3*4*5*6)[::-1].reshape((5,4,6,3))\n>>> np.dot(a, b)[2,3,2,1,2,2]\n499128\n>>> sum(a[2,3,2,:] * b[1,2,:,2])\n499128"}
{"function_name": "statistics_mean", "api_doc": "statistics.mean(data)\nReturn the sample arithmetic mean of data which can be a sequence or iterable.\n\nThe arithmetic mean is the sum of the data divided by the number of data points. It is commonly called \u201cthe average\u201d, although it is only one of many different mathematical averages. It is a measure of the central location of the data.\n\nIf data is empty, StatisticsError will be raised.\n\nSome examples of use:\n\n>>> mean([1, 2, 3, 4, 4])\n2.8\n>>> mean([-1.0, 2.5, 3.25, 5.75])\n2.625\n\n>>> from fractions import Fraction as F\n>>> mean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\n>>> from decimal import Decimal as D\n>>> mean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\nNote The mean is strongly affected by outliers and is not necessarily a typical example of the data points. For a more robust, although less efficient, measure of central tendency, see median().\nThe sample mean gives an unbiased estimate of the true population mean, so that when taken on average over all the possible samples, mean(sample) converges on the true mean of the entire population. If data represents the entire population rather than a sample, then mean(data) is equivalent to calculating the true population mean \u03bc."}
{"function_name": "zlib_adler32", "api_doc": "zlib.adler32(data[, value])\nComputes an Adler-32 checksum of data. (An Adler-32 checksum is almost as reliable as a CRC32 but can be computed much more quickly.) The result is an unsigned 32-bit integer. If value is present, it is used as the starting value of the checksum; otherwise, a default value of 1 is used. Passing in value allows computing a running checksum over the concatenation of several inputs. The algorithm is not cryptographically strong, and should not be used for authentication or digital signatures. Since the algorithm is designed for use as a checksum algorithm, it is not suitable for use as a general hash algorithm.\n\nChanged in version 3.0: The result is always unsigned."}
{"function_name": "statistics_geometric_mean", "api_doc": "statistics.geometric_mean(data)\nConvert data to floats and compute the geometric mean.\n\nThe geometric mean indicates the central tendency or typical value of the data using the product of the values (as opposed to the arithmetic mean which uses their sum).\n\nRaises a StatisticsError if the input dataset is empty, if it contains a zero, or if it contains a negative value. The data may be a sequence or iterable.\n\nNo special efforts are made to achieve exact results. (However, this may change in the future.)\n\n>>> round(geometric_mean([54, 24, 36]), 1)\n36.0"}
{"function_name": "cryptography_fernet_MultiFernet_rotate", "api_doc": "class cryptography.fernet.MultiFernet(fernets)[source]\nNew in version 0.7.\n\nThis class implements key rotation for Fernet. It takes a list of Fernet instances and implements the same API with the exception of one additional method: MultiFernet.rotate():\n\n>>> from cryptography.fernet import Fernet, MultiFernet\n>>> key1 = Fernet(Fernet.generate_key())\n>>> key2 = Fernet(Fernet.generate_key())\n>>> f = MultiFernet([key1, key2])\n>>> token = f.encrypt(b\"Secret message!\")\n>>> token\nb'...'\n>>> f.decrypt(token)\nb'Secret message!'\n\nMultiFernet performs all encryption options using the first key in the list provided. MultiFernet attempts to decrypt tokens with each key in turn. A cryptography.fernet.InvalidToken exception is raised if the correct key is not found in the list provided. \nKey rotation makes it easy to replace old keys. You can add your new key at the front of the list to start encrypting new messages, and remove old keys as they are no longer needed.\n\nToken rotation as offered by MultiFernet.rotate() is a best practice and manner of cryptographic hygiene designed to limit damage in the event of an undetected event and to increase the difficulty of attacks. For example, if an employee who had access to your company\u2019s fernet keys leaves, you\u2019ll want to generate new fernet key, rotate all of the tokens currently deployed using that new key, and then retire the old fernet key(s) to which the employee had access.\n\nrotate(msg)[source]\nNew in version 2.2.\n\nRotates a token by re-encrypting it under the MultiFernet instance\u2019s primary key. This preserves the timestamp that was originally saved with the token. If a token has successfully been rotated then the rotated token will be returned. If rotation fails this will raise an exception.\n\n>>> from cryptography.fernet import Fernet, MultiFernet\n>>> key1 = Fernet(Fernet.generate_key())\n>>> key2 = Fernet(Fernet.generate_key())\n>>> f = MultiFernet([key1, key2])\n>>> token = f.encrypt(b\"Secret message!\")\n>>> token\nb'...'\n>>> f.decrypt(token)\nb'Secret message!'\n>>> key3 = Fernet(Fernet.generate_key())\n>>> f2 = MultiFernet([key3, key1, key2])\n>>> rotated = f2.rotate(token)\n>>> f2.decrypt(rotated)\nb'Secret message!'\nParameters:\nmsg (bytes or str) \u2013 The token to re-encrypt.\n\nReturns bytes:\nA secure message that cannot be read or altered without the key. This is URL-safe base64-encoded. This is referred to as a \u201cFernet token\u201d.\n\nRaises:\ncryptography.fernet.InvalidToken \u2013 If a token is in any way invalid this exception is raised.\n\nTypeError \u2013 This exception is raised if the msg is not bytes or str."}
{"function_name": "decimal_Decimal_quantize", "api_doc": "quantize(exp, rounding=None, context=None)\nReturn a value equal to the first operand after rounding and having the exponent of the second operand.\n\n>>> Decimal('1.41421356').quantize(Decimal('1.000'))\nDecimal('1.414')\nUnlike other operations, if the length of the coefficient after the quantize operation would be greater than precision, then an InvalidOperation is signaled. This guarantees that, unless there is an error condition, the quantized exponent is always equal to that of the right-hand operand.\n\nAlso unlike other operations, quantize never signals Underflow, even if the result is subnormal and inexact.\n\nIf the exponent of the second operand is larger than that of the first then rounding may be necessary. In this case, the rounding mode is determined by the rounding argument if given, else by the given context argument; if neither argument is given the rounding mode of the current thread\u2019s context is used.\n\nAn error is returned whenever the resulting exponent is greater than Emax or less than Etiny()."}
{"function_name": "statistics_median", "api_doc": "statistics.median(data)\nReturn the median (middle value) of numeric data, using the common \u201cmean of middle two\u201d method. If data is empty, StatisticsError is raised. data can be a sequence or iterable.\n\nThe median is a robust measure of central location and is less affected by the presence of outliers. When the number of data points is odd, the middle data point is returned:\n\n>>> median([1, 3, 5])\n3\nWhen the number of data points is even, the median is interpolated by taking the average of the two middle values:\n\n>>> median([1, 3, 5, 7])\n4.0\nThis is suited for when your data is discrete, and you don\u2019t mind that the median may not be an actual data point.\n\nIf the data is ordinal (supports order operations) but not numeric (doesn\u2019t support addition), consider using median_low() or median_high() instead."}
{"function_name": "pandas_DataFrame_sort_values", "api_doc": "DataFrame.sort_values(by, *, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]\nSort by the values along either axis.\n\nParameters:\nby\nstr or list of str\nName or list of names to sort by.\n\nif axis is 0 or \u2018index\u2019 then by may contain index levels and/or column labels.\n\nif axis is 1 or \u2018columns\u2019 then by may contain column levels and/or index labels.\n\naxis\n\u201c{0 or \u2018index\u2019, 1 or \u2018columns\u2019}\u201d, default 0\nAxis to be sorted.\n\nascending\nbool or list of bool, default True\nSort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.\n\ninplace\nbool, default False\nIf True, perform operation in-place.\n\nkind\n{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.\n\nna_position\n{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\nPuts NaNs at the beginning if first; last puts NaNs at the end.\n\nignore_index\nbool, default False\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nkey\ncallable, optional\nApply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.\n\nReturns\n:\nDataFrame or None\nDataFrame with sorted values or None if inplace=True.\n\nSee also\n\nDataFrame.sort_index\nSort a DataFrame by the index.\n\nSeries.sort_values\nSimilar method for a Series.\n\nExamples\n\n>>> df = pd.DataFrame({\n    'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n    'col2': [2, 1, 9, 8, 7, 4],\n    'col3': [0, 1, 9, 4, 2, 3],\n    'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n})\n>>> df\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nSort by col1\n\n>>> df.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort by multiple columns\n\n>>> df.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort Descending\n\n>>> df.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D\nPutting NAs first\n\n>>> df.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\nSorting with a key function\n\n>>> df.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nNatural sort with the key argument, using the natsort <https://github.com/SethMMorton/natsort> package.\n\n>>> df = pd.DataFrame({\n   \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n   \"value\": [10, 20, 30, 40, 50]\n})\ndf\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\n>>> from natsort import index_natsorted\n>>> df.sort_values(\n    by=\"time\",\n    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n)\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20"}
{"function_name": "pandas_isna", "api_doc": "pandas.isna\npandas.isna(obj)[source]\nDetect missing values for an array-like object.\n\nThis function takes a scalar or array-like object and indicates whether values are missing (NaN in numeric arrays, None or NaN in object arrays, NaT in datetimelike).\n\nParameters:\nobj\nscalar or array-like\nObject to check for null or missing values.\n\nReturns:\nbool or array-like of bool\nFor scalar input, returns a scalar boolean. For array input, returns an array of boolean indicating whether each corresponding element is missing.\n\nSee also\n\nnotna\nBoolean inverse of pandas.isna.\n\nSeries.isna\nDetect missing values in a Series.\n\nDataFrame.isna\nDetect missing values in a DataFrame.\n\nIndex.isna\nDetect missing values in an Index.\n\nExamples\n\nScalar arguments (including strings) result in a scalar boolean.\n\n>>> pd.isna('dog')\nFalse\n>>> pd.isna(pd.NA)\nTrue\n>>> pd.isna(np.nan)\nTrue\nndarrays result in an ndarray of booleans.\n\n>>> array = np.array([[1, np.nan, 3], [4, 5, np.nan]])\n>>> array\narray([[ 1., nan,  3.],\n       [ 4.,  5., nan]])\n>>> pd.isna(array)\narray([[False,  True, False],\n       [False, False,  True]])\nFor indexes, an ndarray of booleans is returned.\n\n>>> index = pd.DatetimeIndex([\"2017-07-05\", \"2017-07-06\", None,\n                          \"2017-07-08\"])\n>>> index\nDatetimeIndex(['2017-07-05', '2017-07-06', 'NaT', '2017-07-08'],\n              dtype='datetime64[ns]', freq=None)\n>>> pd.isna(index)\narray([False, False,  True, False])\nFor Series and DataFrame, the same type is returned, containing booleans.\n\n>>> df = pd.DataFrame([['ant', 'bee', 'cat'], ['dog', None, 'fly']])\ndf\n     0     1    2\n0  ant   bee  cat\n1  dog  None  fly\n>>> pd.isna(df)\n       0      1      2\n0  False  False  False\n1  False   True  False\npd.isna(df[1])\n0    False\n1     True\nName: 1, dtype: bool"}
{"function_name": "decimal_Decimal_as_integer_ratio", "api_doc": "as_integer_ratio()\nReturn a pair (n, d) of integers that represent the given Decimal instance as a fraction, in lowest terms and with a positive denominator:\n\n>>> Decimal('-3.14').as_integer_ratio()\n(-157, 50)\nThe conversion is exact. Raise OverflowError on infinities and ValueError on NaNs.\n\nNew in version 3.6."}
{"function_name": "numpy_sum", "api_doc": "numpy.sum\nnumpy.sum(a, axis=None, dtype=None, out=None, keepdims=<no value>, initial=<no value>, where=<no value>)[source]\nSum of array elements over a given axis.\n\nParameters:\na array_like\nElements to sum.\n\naxis None or int or tuple of ints, optional\nAxis or axes along which a sum is performed. The default, axis=None, will sum all of the elements of the input array. If axis is negative it counts from the last to the first axis.\n\nNew in version 1.7.0.\n\nIf axis is a tuple of ints, a sum is performed on all of the axes specified in the tuple instead of a single axis or all the axes as before.\n\ndtype dtype, optional\nThe type of the returned array and of the accumulator in which the elements are summed. The dtype of a is used by default unless a has an integer dtype of less precision than the default platform integer. In that case, if a is signed then the platform integer is used while if a is unsigned then an unsigned integer of the same precision as the platform integer is used.\n\nout ndarray, optional\nAlternative output array in which to place the result. It must have the same shape as the expected output, but the type of the output values will be cast if necessary.\n\nkeepdims bool, optional\nIf this is set to True, the axes which are reduced are left in the result as dimensions with size one. With this option, the result will broadcast correctly against the input array.\n\nIf the default value is passed, then keepdims will not be passed through to the sum method of sub-classes of ndarray, however any non-default value will be. If the sub-class\u2019 method does not implement keepdims any exceptions will be raised.\n\ninitial scalar, optional\nStarting value for the sum. See reduce for details.\n\nNew in version 1.15.0.\n\nwhere array_like of bool, optional\nElements to include in the sum. See reduce for details.\n\nNew in version 1.17.0.\n\nReturns:\nsum_along_axisndarray\nAn array with the same shape as a, with the specified axis removed. If a is a 0-d array, or if axis is None, a scalar is returned. If an output array is specified, a reference to out is returned.\n\nSee also\n\nndarray.sum\nEquivalent method.\n\nadd.reduce\nEquivalent functionality of add.\n\ncumsum\nCumulative sum of array elements.\n\ntrapz\nIntegration of array values using the composite trapezoidal rule.\n\nmean, average\nNotes\n\nArithmetic is modular when using integer types, and no error is raised on overflow.\n\nThe sum of an empty array is the neutral element 0:\n\n>>> np.sum([])\n0.0\nFor floating point numbers the numerical precision of sum (and np.add.reduce) is in general limited by directly adding each number individually to the result causing rounding errors in every step. However, often numpy will use a numerically better approach (partial pairwise summation) leading to improved precision in many use-cases. This improved precision is always provided when no axis is given. When axis is given, it will depend on which axis is summed. Technically, to provide the best speed possible, the improved precision is only used when the summation is along the fast axis in memory. Note that the exact precision may vary depending on other parameters. In contrast to NumPy, Python\u2019s math.fsum function uses a slower but more precise approach to summation. Especially when summing a large number of lower precision floating point numbers, such as float32, numerical errors can become significant. In such cases it can be advisable to use dtype=\u201dfloat64\u201d to use a higher precision for the output.\n\nExamples\n\n>>> np.sum([0.5, 1.5])\n2.0\n>>> np.sum([0.5, 0.7, 0.2, 1.5], dtype=np.int32)\n1\n>>> np.sum([[0, 1], [0, 5]])\n6\n>>> np.sum([[0, 1], [0, 5]], axis=0)\narray([0, 6])\n>>> np.sum([[0, 1], [0, 5]], axis=1)\narray([1, 5])\n>>> np.sum([[0, 1], [np.nan, 5]], where=[False, True], axis=1)\narray([1., 5.])\nIf the accumulator is too small, overflow occurs:\n\n>>> np.ones(128, dtype=np.int8).sum(dtype=np.int8)\n-128\nYou can also start the sum with a value other than zero:\n\n>>> np.sum([10], initial=5)\n15"}
{"function_name": "datetime_datetime_fromordinal", "api_doc": "classmethod datetime.fromordinal(ordinal)\nReturn the datetime corresponding to the proleptic Gregorian ordinal, where January 1 of year 1 has ordinal 1. \n\nValueError is raised unless 1 <= ordinal <= datetime.max.toordinal(). The hour, minute, second and microsecond of the result are all 0, and tzinfo is None."}
{"function_name": "zlib_compress", "api_doc": "zlib.compress(data, /, level=- 1, wbits=MAX_WBITS)\nCompresses the bytes in data, returning a bytes object containing compressed data. level is an integer from 0 to 9 or -1 controlling the level of compression; 1 (Z_BEST_SPEED) is fastest and produces the least compression, 9 (Z_BEST_COMPRESSION) is slowest and produces the most. 0 (Z_NO_COMPRESSION) is no compression. The default value is -1 (Z_DEFAULT_COMPRESSION). Z_DEFAULT_COMPRESSION represents a default compromise between speed and compression (currently equivalent to level 6).\n\nThe wbits argument controls the size of the history buffer (or the \u201cwindow size\u201d) used when compressing data, and whether a header and trailer is included in the output. It can take several ranges of values, defaulting to 15 (MAX_WBITS):\n\n+9 to +15: The base-two logarithm of the window size, which therefore ranges between 512 and 32768. Larger values produce better compression at the expense of greater memory usage. The resulting output will include a zlib-specific header and trailer.\n\n\u22129 to \u221215: Uses the absolute value of wbits as the window size logarithm, while producing a raw output stream with no header or trailing checksum.\n\n+25 to +31 = 16 + (9 to 15): Uses the low 4 bits of the value as the window size logarithm, while including a basic gzip header and trailing checksum in the output.\n\nRaises the error exception if any error occurs.\n\nChanged in version 3.6: level can now be used as a keyword parameter.\n\nChanged in version 3.11: The wbits parameter is now available to set window bits and compression type."}
{"function_name": "datetime_timedelta_total_seconds", "api_doc": "timedelta.total_seconds()\nReturn the total number of seconds contained in the duration. Equivalent to td / timedelta(seconds=1). For interval units other than seconds, use the division form directly (e.g. td / timedelta(microseconds=1)).\n\nNote that for very large time intervals (greater than 270 years on most platforms) this method will lose microsecond accuracy.\n\nNew in version 3.2.\n\nExamples of usage: timedelta\nAn additional example of normalization:\n\n>>> # Components of another_year add up to exactly 365 days\n>>> from datetime import timedelta\n>>> year = timedelta(days=365)\n>>> another_year = timedelta(weeks=40, days=84, hours=23,\n                         minutes=50, seconds=600)\n>>> year == another_year\nTrue\n>>> year.total_seconds()\n31536000.0\nExamples of timedelta arithmetic:\n\n>>> from datetime import timedelta\n>>> year = timedelta(days=365)\n>>> ten_years = 10 * year\n>>> ten_years\ndatetime.timedelta(days=3650)\n>>> ten_years.days // 365\n10\n>>> nine_years = ten_years - year\n>>> nine_years\ndatetime.timedelta(days=3285)\n>>> three_years = nine_years // 3\n>>> three_years, three_years.days // 365\n(datetime.timedelta(days=1095), 3)"}
{"function_name": "statistics_correlation", "api_doc": "statistics.correlation(x, y, /, *, method='linear')\nReturn the Pearson\u2019s correlation coefficient for two inputs. Pearson\u2019s correlation coefficient r takes values between -1 and +1. It measures the strength and direction of a linear relationship.\n\nIf method is \u201cranked\u201d, computes Spearman\u2019s rank correlation coefficient for two inputs. The data is replaced by ranks. Ties are averaged so that equal values receive the same rank. The resulting coefficient measures the strength of a monotonic relationship.\n\nSpearman\u2019s correlation coefficient is appropriate for ordinal data or for continuous data that doesn\u2019t meet the linear proportion requirement for Pearson\u2019s correlation coefficient.\n\nBoth inputs must be of the same length (no less than two), and need not to be constant, otherwise StatisticsError is raised.\n\nExample with Kepler\u2019s laws of planetary motion:\n\n>>> # Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and  Neptune\n>>> orbital_period = [88, 225, 365, 687, 4331, 10_756, 30_687, 60_190]    # days\n>>> dist_from_sun = [58, 108, 150, 228, 778, 1_400, 2_900, 4_500] # million km\n\n>>> # Show that a perfect monotonic relationship exists\n>>> correlation(orbital_period, dist_from_sun, method='ranked')\n1.0\n\n>>> # Observe that a linear relationship is imperfect\n>>> round(correlation(orbital_period, dist_from_sun), 4)\n0.9882\n\n>>> # Demonstrate Kepler's third law: There is a linear correlation\n>>> # between the square of the orbital period and the cube of the\n>>> # distance from the sun.\n>>> period_squared = [p * p for p in orbital_period]\n>>> dist_cubed = [d * d * d for d in dist_from_sun]\n>>> round(correlation(period_squared, dist_cubed), 4)\n1.0\nNew in version 3.10.\n\nChanged in version 3.12: Added support for Spearman\u2019s rank correlation coefficient."}
{"function_name": "pandas_cut", "api_doc": "pandas.cut\npandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates='raise', ordered=True)[source]\nBin values into discrete intervals.\n\nUse cut when you need to segment and sort data values into bins. This function is also useful for going from a continuous variable to a categorical variable. For example, cut could convert ages to groups of age ranges. Supports binning into an equal number of bins, or a pre-specified array of bins.\n\nParameters:\nx array-like\nThe input array to be binned. Must be 1-dimensional.\n\nbins int, sequence of scalars, or IntervalIndex\nThe criteria to bin by.\n\nint : Defines the number of equal-width bins in the range of x. The range of x is extended by .1% on each side to include the minimum and maximum values of x.\n\nsequence of scalars : Defines the bin edges allowing for non-uniform width. No extension of the range of x is done.\n\nIntervalIndex : Defines the exact bins to be used. Note that IntervalIndex for bins must be non-overlapping.\n\nright bool, default True\nIndicates whether bins includes the rightmost edge or not. If right == True (the default), then the bins [1, 2, 3, 4] indicate (1,2], (2,3], (3,4]. This argument is ignored when bins is an IntervalIndex.\n\nlabels array or False, default None\nSpecifies the labels for the returned bins. Must be the same length as the resulting bins. If False, returns only integer indicators of the bins. This affects the type of the output container (see below). This argument is ignored when bins is an IntervalIndex. If True, raises an error. When ordered=False, labels must be provided.\n\nretbins bool, default False\nWhether to return the bins or not. Useful when bins is provided as a scalar.\n\nprecision int, default 3\nThe precision at which to store and display the bins labels.\n\ninclude_lowest bool, default False\nWhether the first interval should be left-inclusive or not.\n\nduplicates {default \u2018raise\u2019, \u2018drop\u2019}, optional\nIf bin edges are not unique, raise ValueError or drop non-uniques.\n\nordered bool, default True\nWhether the labels are ordered or not. Applies to returned types Categorical and Series (with Categorical dtype). If True, the resulting categorical will be ordered. If False, the resulting categorical will be unordered (labels must be provided).\n\nReturns: out\nCategorical, Series, or ndarray\nAn array-like object representing the respective bin for each value of x. The type depends on the value of labels.\n\nNone (default) : returns a Series for Series x or a Categorical for all other inputs. The values stored within are Interval dtype.\n\nsequence of scalars : returns a Series for Series x or a Categorical for all other inputs. The values stored within are whatever the type in the sequence is.\n\nFalse : returns an ndarray of integers.\n\nbins\nnumpy.ndarray or IntervalIndex.\nThe computed or specified bins. Only returned when retbins=True. For scalar or sequence bins, this is an ndarray with the computed bins. If set duplicates=drop, bins will drop non-unique bin. For an IntervalIndex bins, this is equal to bins.\n\nSee also\n\nqcut\nDiscretize variable into equal-sized buckets based on rank or based on sample quantiles.\n\nCategorical\nArray type for storing data that come from a fixed set of values.\n\nSeries\nOne-dimensional array with axis labels (including time series).\n\nIntervalIndex\nImmutable Index implementing an ordered, sliceable set.\n\nNotes\n\nAny NA values will be NA in the result. Out of bounds values will be NA in the resulting Series or Categorical object.\n\nReference the user guide for more examples.\n\nExamples\n\nDiscretize into three equal-sized bins.\n\n>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)\n\n[(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\nCategories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...\n>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True)\n\n([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ...\nCategories (3, interval[float64, right]): [(0.994, 3.0] < (3.0, 5.0] ...\narray([0.994, 3.   , 5.   , 7.   ]))\nDiscovers the same bins, but assign them specific labels. Notice that the returned Categorical\u2019s categories are labels and is ordered.\n\n>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]),\n       3, labels=[\"bad\", \"medium\", \"good\"])\n['bad', 'good', 'medium', 'medium', 'good', 'bad']\nCategories (3, object): ['bad' < 'medium' < 'good']\nordered=False will result in unordered categories when labels are passed. This parameter can be used to allow non-unique labels:\n\n>>> pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3,\n       labels=[\"B\", \"A\", \"B\"], ordered=False)\n['B', 'B', 'A', 'A', 'B', 'B']\nCategories (2, object): ['A', 'B']\nlabels=False implies you just want the bins back.\n\n>>> pd.cut([0, 1, 1, 2], bins=4, labels=False)\narray([0, 1, 1, 3])\nPassing a Series as an input returns a Series with categorical dtype:\n\n>>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\n              index=['a', 'b', 'c', 'd', 'e'])\n>>> pd.cut(s, 3)\n\na    (1.992, 4.667]\nb    (1.992, 4.667]\nc    (4.667, 7.333]\nd     (7.333, 10.0]\ne     (7.333, 10.0]\ndtype: category\nCategories (3, interval[float64, right]): [(1.992, 4.667] < (4.667, ...\nPassing a Series as an input returns a Series with mapping value. It is used to map numerically to intervals based on bins.\n\n>>> s = pd.Series(np.array([2, 4, 6, 8, 10]),\n              index=['a', 'b', 'c', 'd', 'e'])\n>>> pd.cut(s, [0, 2, 4, 6, 8, 10], labels=False, retbins=True, right=False)\n\n(a    1.0\n b    2.0\n c    3.0\n d    4.0\n e    NaN\n dtype: float64,\n array([ 0,  2,  4,  6,  8, 10]))\nUse drop optional when bins is not unique\n\n>>> pd.cut(s, [0, 2, 4, 6, 10, 10], labels=False, retbins=True,\n       right=False, duplicates='drop')\n\n(a    1.0\n b    2.0\n c    3.0\n d    3.0\n e    NaN\n dtype: float64,\n array([ 0,  2,  4,  6, 10]))\nPassing an IntervalIndex for bins results in those categories exactly. Notice that values not covered by the IntervalIndex are set to NaN. 0 is to the left of the first bin (which is closed on the right), and 1.5 falls between two bins.\n\n>>> bins = pd.IntervalIndex.from_tuples([(0, 1), (2, 3), (4, 5)])\n>>> pd.cut([0, 0.5, 1.5, 2.5, 4.5], bins)\n[NaN, (0.0, 1.0], NaN, (2.0, 3.0], (4.0, 5.0]]\nCategories (3, interval[int64, right]): [(0, 1] < (2, 3] < (4, 5]]"}
{"function_name": "networkx_degree", "api_doc": "degree(G, nbunch=None, weight=None)[source]\nReturns a degree view of single node or of nbunch of nodes. If nbunch is omitted, then return degrees of all nodes."}
{"function_name": "decimal_Decimal_from_float", "api_doc": "classmethod from_float(f)\nAlternative constructor that only accepts instances of float or int.\n\nNote Decimal.from_float(0.1) is not the same as Decimal('0.1'). Since 0.1 is not exactly representable in binary floating point, the value is stored as the nearest representable value which is 0x1.999999999999ap-4. That equivalent value in decimal is 0.1000000000000000055511151231257827021181583404541015625.\n\nNote From Python 3.2 onwards, a Decimal instance can also be constructed directly from a float.\n>>> Decimal.from_float(0.1)\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\n>>> Decimal.from_float(float('nan'))\nDecimal('NaN')\n>>> Decimal.from_float(float('inf'))\nDecimal('Infinity')\n>>> Decimal.from_float(float('-inf'))\nDecimal('-Infinity')\nNew in version 3.1."}
{"function_name": "networkx_find_cliques", "api_doc": "find_cliques\nfind_cliques(G, nodes=None)[source]\nReturns all maximal cliques in an undirected graph.\n\nFor each node n, a maximal clique for n is a largest complete subgraph containing n. The largest maximal clique is sometimes called the maximum clique.\n\nThis function returns an iterator over cliques, each of which is a list of nodes. It is an iterative implementation, so should not suffer from recursion depth issues.\n\nThis function accepts a list of nodes and only the maximal cliques containing all of these nodes are returned. It can considerably speed up the running time if some specific cliques are desired.\n\nParameters:\nG: NetworkX graph\nAn undirected graph.\n\nnodes: list, optional (default=None)\nIf provided, only yield maximal cliques containing all nodes in nodes. If nodes isn\u2019t a clique itself, a ValueError is raised.\n\nReturns:\niterator: An iterator over maximal cliques, each of which is a list of nodes in G. If nodes is provided, only the maximal cliques containing all the nodes in nodes are returned. The order of cliques is arbitrary.\n\nRaises:\nValueError\nIf nodes is not a clique.\n\nSee also\n\nfind_cliques_recursive\nA recursive version of the same algorithm.\n\nNotes\n\nTo obtain a list of all maximal cliques, use list(find_cliques(G)). However, be aware that in the worst-case, the length of this list can be exponential in the number of nodes in the graph. This function avoids storing all cliques in memory by only keeping current candidate node lists in memory during its search.\n\nThis implementation is based on the algorithm published by Bron and Kerbosch (1973) [1], as adapted by Tomita, Tanaka and Takahashi (2006) [2] and discussed in Cazals and Karande (2008) [3]. It essentially unrolls the recursion used in the references to avoid issues of recursion stack depth (for a recursive implementation, see find_cliques_recursive()).\n\nThis algorithm ignores self-loops and parallel edges, since cliques are not conventionally defined with such edges.\n\nReferences\n\n[1]\nBron, C. and Kerbosch, J. \u201cAlgorithm 457: finding all cliques of an undirected graph\u201d. Communications of the ACM 16, 9 (Sep. 1973), 575\u2013577. <http://portal.acm.org/citation.cfm?doid=362342.362367>\n\n[2]\nEtsuji Tomita, Akira Tanaka, Haruhisa Takahashi, \u201cThe worst-case time complexity for generating all maximal cliques and computational experiments\u201d, Theoretical Computer Science, Volume 363, Issue 1, Computing and Combinatorics, 10th Annual International Conference on Computing and Combinatorics (COCOON 2004), 25 October 2006, Pages 28\u201342 <https://doi.org/10.1016/j.tcs.2006.06.015>\n\n[3]\nF. Cazals, C. Karande, \u201cA note on the problem of reporting maximal cliques\u201d, Theoretical Computer Science, Volume 407, Issues 1\u20133, 6 November 2008, Pages 564\u2013568, <https://doi.org/10.1016/j.tcs.2008.05.010>\n\nExamples\n\n>>> from pprint import pprint  # For nice dict formatting\n>>> G = nx.karate_club_graph()\n>>> sum(1 for c in nx.find_cliques(G))  # The number of maximal cliques in G\n36\n>>> max(nx.find_cliques(G), key=len)  # The largest maximal clique in G\n[0, 1, 2, 3, 13]\nThe size of the largest maximal clique is known as the clique number of the graph, which can be found directly with:\n\n>>> max(len(c) for c in nx.find_cliques(G))\n5\nOne can also compute the number of maximal cliques in G that contain a given node. The following produces a dictionary keyed by node whose values are the number of maximal cliques in G that contain the node:\n\n>>> pprint({n: sum(1 for c in nx.find_cliques(G) if n in c) for n in G})\n{0: 13,\n 1: 6,\n 2: 7,\n 3: 3,\n 4: 2,\n 5: 3,\n 6: 3,\n 7: 1,\n 8: 3,\n 9: 2,\n 10: 2,\n 11: 1,\n 12: 1,\n 13: 2,\n 14: 1,\n 15: 1,\n 16: 1,\n 17: 1,\n 18: 1,\n 19: 2,\n 20: 1,\n 21: 1,\n 22: 1,\n 23: 3,\n 24: 2,\n 25: 2,\n 26: 1,\n 27: 3,\n 28: 2,\n 29: 2,\n 30: 2,\n 31: 4,\n 32: 9,\n 33: 14}\nOr, similarly, the maximal cliques in G that contain a given node. For example, the 4 maximal cliques that contain node 31:\n\n>>> [c for c in nx.find_cliques(G) if 31 in c]\n[[0, 31], [33, 32, 31], [33, 28, 31], [24, 25, 31]]"}
{"function_name": "numpy_linalg_norm", "api_doc": "numpy.linalg.norm\nlinalg.norm(x, ord=None, axis=None, keepdims=False)[source]\nMatrix or vector norm.\n\nThis function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n\nParameters:\nx array_like\nInput array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of x.ravel will be returned.\n\nord{non-zero int, inf, -inf, \u2018fro\u2019, \u2018nuc\u2019}, optional\nOrder of the norm (see table under Notes). inf means numpy\u2019s inf object. The default is None.\n\naxis{None, int, 2-tuple of ints}, optional.\nIf axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.\n\nNew in version 1.8.0.\n\nkeepdims bool, optional\nIf this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n\nNew in version 1.10.0.\n\nReturns:\nn float or ndarray\nNorm of the matrix or vector(s).\n\nSee also\n\nscipy.linalg.norm\nSimilar function in SciPy.\n\nNotes\n\nFor values of ord < 1, the result is, strictly speaking, not a mathematical \u2018norm\u2019, but it may still be useful for various numerical purposes.\n\nThe following norms can be calculated:\n\nord norm for matrices norm for vectors\n\nNone Frobenius norm 2-norm\n\n\u2018fro\u2019 Frobenius norm \u2013\n\n\u2018nuc\u2019 nuclear norm \u2013\n\ninf max(sum(abs(x), axis=1)) max(abs(x))\n\n-inf min(sum(abs(x), axis=1)) min(abs(x))\n\n0 \u2013 sum(x != 0)\n\n1 max(sum(abs(x), axis=0)) as below\n\n-1 min(sum(abs(x), axis=0)) as below\n\n2 2-norm (largest sing. value) as below\n\n-2 smallest singular value as below\n\nother \u2013 sum(abs(x)**ord)**(1./ord)\n\nThe Frobenius norm is given by [1]:\n\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when x.ndim != 2.\n\nReferences\n\n[1]\nG. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n\nExamples\n\n>>> from numpy import linalg as LA\n>>> a = np.arange(9) - 4\n>>> a\narray([-4, -3, -2, ...,  2,  3,  4])\n>>> b = a.reshape((3, 3))\n>>> b\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\n>>> LA.norm(a)\n7.745966692414834\n>>> LA.norm(b)\n7.745966692414834\n>>> LA.norm(b, 'fro')\n7.745966692414834\n>>> LA.norm(a, np.inf)\n4.0\n>>> LA.norm(b, np.inf)\n9.0\n>>> LA.norm(a, -np.inf)\n0.0\n>>> LA.norm(b, -np.inf)\n2.0\n>>> LA.norm(a, 1)\n20.0\n>>> LA.norm(b, 1)\n7.0\n>>> LA.norm(a, -1)\n-4.6566128774142013e-010\n>>> LA.norm(b, -1)\n6.0\n>>> LA.norm(a, 2)\n7.745966692414834\n>>> LA.norm(b, 2)\n7.3484692283495345\n>>> LA.norm(a, -2)\n0.0\n>>> LA.norm(b, -2)\n1.8570331885190563e-016 # may vary\n>>> LA.norm(a, 3)\n5.8480354764257312 # may vary\n>>> LA.norm(a, -3)\n0.0\nUsing the axis argument to compute vector norms:\n\n>>> c = np.array([[ 1, 2, 3],\n              [-1, 1, 4]])\n>>> LA.norm(c, axis=0)\narray([ 1.41421356,  2.23606798,  5.        ])\n>>> LA.norm(c, axis=1)\narray([ 3.74165739,  4.24264069])\n>>> LA.norm(c, ord=1, axis=1)\narray([ 6.,  6.])\nUsing the axis argument to compute matrix norms:\n\n>>> m = np.arange(8).reshape(2,2,2)\n>>> LA.norm(m, axis=(1,2))\narray([  3.74165739,  11.22497216])\n>>> LA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(3.7416573867739413, 11.224972160321824)"}
{"function_name": "dateutil_parser_isoparse", "api_doc": "classmethodparser.isoparse(dt_str)\nParse an ISO-8601 datetime string into a datetime.datetime.\n\nAn ISO-8601 datetime string consists of a date portion, followed optionally by a time portion - the date and time portions are separated by a single character separator, which is T in the official standard. Incomplete date formats (such as YYYY-MM) may not be combined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\nYYYY\nYYYY-MM or YYYYMM\nYYYY-MM-DD or YYYYMMDD\nUncommon:\n\nYYYY-Www or YYYYWww - ISO week (day defaults to 0)\nYYYY-Www-D or YYYYWwwD - ISO week and day\nThe ISO week and day numbering follows the same logic as datetime.date.isocalendar().\n\nSupported time formats are:\n\nhh\nhh:mm or hhmm\nhh:mm:ss or hhmmss\nhh:mm:ss.ssssss (Up to 6 sub-second digits)\nMidnight is a special case for hh, as the standard supports both 00:00 and 24:00 as a representation. The decimal separator can be either a dot or a comma.\n\nCaution\n\nSupport for fractional components other than seconds is part of the ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\nZ (UTC)\n\u00b1HH:MM\n\u00b1HHMM\n\u00b1HH\nOffsets will be represented as dateutil.tz.tzoffset objects, with the exception of UTC, which will be represented as dateutil.tz.tzutc. Time zone offsets equivalent to UTC (such as +00:00) will also be represented as dateutil.tz.tzutc.\n\nParameters:\tdt_str \u2013 A string or stream containing only an ISO-8601 datetime string\nReturns:\tReturns a datetime.datetime representing the string. Unspecified components default to their lowest value.\nWarning\n\nAs of version 2.7.0, the strictness of the parser should not be considered a stable part of the contract. Any valid ISO-8601 string that parses correctly with the default settings will continue to parse correctly in future versions, but invalid strings that currently fail (e.g. 2017-01-01T00:00+00:00:00) are not guaranteed to continue failing in future versions if they encode a valid date.\n\nNew in version 2.7.0."}
{"function_name": "networkx_lowest_common_ancestor", "api_doc": "lowest_common_ancestor\nlowest_common_ancestor(G, node1, node2, default=None)[source]\nCompute the lowest common ancestor of the given pair of nodes.\n\nParameters:\nG: NetworkX directed graph\nnode1, node2: nodes in the graph.\ndefault: object\nReturned if no common ancestor between node1 and node2\n\nReturns:\nThe lowest common ancestor of node1 and node2,\nor default if they have no common ancestors.\nSee also\n\nall_pairs_lowest_common_ancestor\nExamples\n\n>>> G = nx.DiGraph()\n>>> nx.add_path(G, (0, 1, 2, 3))\n>>> nx.add_path(G, (0, 4, 3))\n>>> nx.lowest_common_ancestor(G, 2, 4)\n0"}
{"function_name": "numpy_cumsum", "api_doc": "numpy.cumsum\nnumpy.cumsum(a, axis=None, dtype=None, out=None)[source]\nReturn the cumulative sum of the elements along a given axis.\n\nParameters:\na: array_like\nInput array.\n\naxis: int, optional\nAxis along which the cumulative sum is computed. The default (None) is to compute the cumsum over the flattened array.\n\ndtype: dtype, optional\nType of the returned array and of the accumulator in which the elements are summed. If dtype is not specified, it defaults to the dtype of a, unless a has an integer dtype with a precision less than that of the default platform integer. In that case, the default platform integer is used.\n\nout: ndarray, optional\nAlternative output array in which to place the result. It must have the same shape and buffer length as the expected output but the type will be cast if necessary. See Output type determination for more details.\n\nReturns:\ncumsum_along_axis: ndarray.\nA new array holding the result is returned unless out is specified, in which case a reference to out is returned. The result has the same size as a, and the same shape as a if axis is not None or a is a 1-d array.\n\nSee also\n\nsum\nSum array elements.\n\ntrapz\nIntegration of array values using the composite trapezoidal rule.\n\ndiff\nCalculate the n-th discrete difference along given axis.\n\nNotes\n\nArithmetic is modular when using integer types, and no error is raised on overflow.\n\ncumsum(a)[-1] may not be equal to sum(a) for floating-point values since sum may use a pairwise summation routine, reducing the roundoff-error. See sum for more information.\n\nExamples\n\n>>> a = np.array([[1,2,3], [4,5,6]])\n>>> a\narray([[1, 2, 3],\n       [4, 5, 6]])\n>>> np.cumsum(a)\narray([ 1,  3,  6, 10, 15, 21])\n>>> np.cumsum(a, dtype=float)     # specifies type of output value(s)\narray([  1.,   3.,   6.,  10.,  15.,  21.])\n>>> np.cumsum(a,axis=0)      # sum over rows for each of the 3 columns\narray([[1, 2, 3],\n       [5, 7, 9]])\n>>> np.cumsum(a,axis=1)      # sum over columns for each of the 2 rows\narray([[ 1,  3,  6],\n       [ 4,  9, 15]])\ncumsum(b)[-1] may not be equal to sum(b)\n\n>>> b = np.array([1, 2e-9, 3e-9] * 1000000)\n>>> b.cumsum()[-1]\n1000000.0050045159\n>>> b.sum()\n1000000.0050000029"}
{"function_name": "dateutil_parser_parse", "api_doc": "parser.parse(parserinfo=None, **kwargs)[source]\nParse a string in one of the supported formats, using the parserinfo parameters.\n\nParameters:\t\ntimestr \u2013 A string containing a date/time stamp.\nparserinfo \u2013 A parserinfo object containing parameters for the parser. If None, the default arguments to the parserinfo constructor are used.\nThe **kwargs parameter takes the following keyword arguments:\n\nParameters:\t\ndefault \u2013 The default datetime object, if this is a datetime object and not None, elements specified in timestr replace elements in the default object.\nignoretz \u2013 If set True, time zones in parsed strings are ignored and a naive datetime object is returned.\ntzinfos \u2013\nAdditional time zone names / aliases which may be present in the string. This argument maps time zone names (and optionally offsets from those time zones) to time zones. This parameter can be a dictionary with timezone aliases mapping time zone names to time zones or a function taking two parameters (tzname and tzoffset) and returning a time zone.\n\nThe timezones to which the names are mapped can be an integer offset from UTC in seconds or a tzinfo object.\n\n >>> from dateutil.parser import parse\n >>> from dateutil.tz import gettz\n >>> tzinfos = {\"BRST\": -7200, \"CST\": gettz(\"America/Chicago\")}\n >>> parse(\"2012-01-19 17:21:00 BRST\", tzinfos=tzinfos)\n datetime.datetime(2012, 1, 19, 17, 21, tzinfo=tzoffset(u'BRST', -7200))\n >>> parse(\"2012-01-19 17:21:00 CST\", tzinfos=tzinfos)\n datetime.datetime(2012, 1, 19, 17, 21,\n                   tzinfo=tzfile('/usr/share/zoneinfo/America/Chicago'))\nThis parameter is ignored if ignoretz is set.\n\ndayfirst \u2013 Whether to interpret the first value in an ambiguous 3-integer date (e.g. 01/05/09) as the day (True) or month (False). If yearfirst is set to True, this distinguishes between YDM and YMD. If set to None, this value is retrieved from the current parserinfo object (which itself defaults to False).\nyearfirst \u2013 Whether to interpret the first value in an ambiguous 3-integer date (e.g. 01/05/09) as the year. If True, the first number is taken to be the year, otherwise the last number is taken to be the year. If this is set to None, the value is retrieved from the current parserinfo object (which itself defaults to False).\nfuzzy \u2013 Whether to allow fuzzy parsing, allowing for string like \u201cToday is January 1, 2047 at 8:21:00AM\u201d.\nfuzzy_with_tokens \u2013\nIf True, fuzzy is automatically set to True, and the parser will return a tuple where the first element is the parsed datetime.datetime datetimestamp and the second element is a tuple containing the portions of the string which were ignored:\n\n>>> from dateutil.parser import parse\n>>> parse(\"Today is January 1, 2047 at 8:21:00AM\", fuzzy_with_tokens=True)\n(datetime.datetime(2047, 1, 1, 8, 21), (u'Today is ', u' ', u'at '))\nReturns:\t\nReturns a datetime.datetime object or, if the fuzzy_with_tokens option is True, returns a tuple, the first element being a datetime.datetime object, the second a tuple containing the fuzzy tokens.\n\nRaises:\t\nParserError \u2013 Raised for invalid or unknown string formats, if the provided tzinfo is not in a valid format, or if an invalid date would be created.\nOverflowError \u2013 Raised if the parsed date exceeds the largest valid C integer on your system."}
{"function_name": "statistics_linear_regression", "api_doc": "statistics.linear_regression(x, y, /, *, proportional=False)\nReturn the slope and intercept of simple linear regression parameters estimated using ordinary least squares. Simple linear regression describes the relationship between an independent variable x and a dependent variable y in terms of this linear function:\n\ny = slope * x + intercept + noise\n\nwhere slope and intercept are the regression parameters that are estimated, and noise represents the variability of the data that was not explained by the linear regression (it is equal to the difference between predicted and actual values of the dependent variable).\n\nBoth inputs must be of the same length (no less than two), and the independent variable x cannot be constant; otherwise a StatisticsError is raised.\n\nFor example, we can use the release dates of the Monty Python films to predict the cumulative number of Monty Python films that would have been produced by 2019 assuming that they had kept the pace.\n\n>>> year = [1971, 1975, 1979, 1982, 1983]\n>>> films_total = [1, 2, 3, 4, 5]\n>>> slope, intercept = linear_regression(year, films_total)\n>>> round(slope * 2019 + intercept)\n16\nIf proportional is true, the independent variable x and the dependent variable y are assumed to be directly proportional. The data is fit to a line passing through the origin. Since the intercept will always be 0.0, the underlying linear function simplifies to:\n\ny = slope * x + noise\n\nContinuing the example from correlation(), we look to see how well a model based on major planets can predict the orbital distances for dwarf planets:\n\n>>> model = linear_regression(period_squared, dist_cubed, proportional=True)\n>>> slope = model.slope\n\n# Dwarf planets:   Pluto,  Eris,    Makemake, Haumea, Ceres\n>>> orbital_periods = [90_560, 204_199, 111_845, 103_410, 1_680]  # days\n>>> predicted_dist = [math.cbrt(slope * (p * p)) for p in orbital_periods]\n>>> list(map(round, predicted_dist))\n[5912, 10166, 6806, 6459, 414]\n\n>>> [5_906, 10_152, 6_796, 6_450, 414]  # actual distance in million km\n[5906, 10152, 6796, 6450, 414]\nNew in version 3.10.\n\nChanged in version 3.11: Added support for proportional."}
{"function_name": "pandas_DataFrame_drop_duplicates", "api_doc": "DataFrame.drop_duplicates(subset=None, *, keep='first', inplace=False, ignore_index=False)[source]\nReturn DataFrame with duplicate rows removed.\n\nConsidering certain columns is optional. Indexes, including time indexes are ignored.\n\nParameters:\nsubsetcolumn label or sequence of labels, optional\nOnly consider certain columns for identifying duplicates, by default use all of the columns.\n\nkeep{\u2018first\u2019, \u2018last\u2019, False}, default \u2018first\u2019\nDetermines which duplicates (if any) to keep.\n\n\u2018first\u2019 : Drop duplicates except for the first occurrence.\n\n\u2018last\u2019 : Drop duplicates except for the last occurrence.\n\nFalse : Drop all duplicates.\n\ninplacebool, default False\nWhether to modify the DataFrame rather than creating a new one.\n\nignore_indexbool, default False\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nReturns:\nDataFrame or None\nDataFrame with duplicates removed or None if inplace=True.\n\nSee also\n\nDataFrame.value_counts\nCount unique combinations of columns.\n\nExamples\n\nConsider dataset containing ramen rating.\n\n>>> df = pd.DataFrame({\n    'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'],\n    'style': ['cup', 'cup', 'cup', 'pack', 'pack'],\n    'rating': [4, 4, 3.5, 15, 5]\n})\n>>> df\n    brand style  rating\n0  Yum Yum   cup     4.0\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\nBy default, it removes duplicate rows based on all columns.\n\n>>> df.drop_duplicates()\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n3  Indomie  pack    15.0\n4  Indomie  pack     5.0\nTo remove duplicates on specific column(s), use subset.\n\n>>> df.drop_duplicates(subset=['brand'])\n    brand style  rating\n0  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\nTo remove duplicates and keep last occurrences, use keep.\n\n>>> df.drop_duplicates(subset=['brand', 'style'], keep='last')\n    brand style  rating\n1  Yum Yum   cup     4.0\n2  Indomie   cup     3.5\n4  Indomie  pack     5.0"}
