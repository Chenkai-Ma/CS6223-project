# CS6223-project

## Tentative Schedule

### Tasks

|Name|Code|Paper|Other|
|----|----|----|----|
|mck|Evaluation/post-processing|||
|zx|LLM prompting (in both methods and)|||

### Timeline


## Related Material

- [Paper: *Can Large Language Models Write Good Property-Based Tests?*]((Vikram%20et%20al.%20-%202024%20-%20Can%20Large%20Language%20Models%20Write%20Good%20Property-Base.pdf))
- [Project proposal](CS6223-Proposal-Property_Based_Testing_with_LLMs.pdf)
- [Project planning](https://docs.google.com/document/d/1ly9GCxzbsbM736tBZ5D-mHyUhjS1L6Jd63JEcierU1o/edit#heading=h.r5acg7cz2fk7)
- [Hypothesis](https://hypothesis.readthedocs.io/en/latest/)

## (Re)implementation

### Method

- The actual prompt for single stage prompting is in [`prompts.py`](proptest_ai_data/prompts.py): `SYSTEM_PROMPT + PBT_PROMPT_TEMPLATE (filled with API docs) + OUTPUT_FORMAT_TEMPLATE (example)`
- The actual prompt for two stage prompting is in [`prompts.py`](proptest_ai_data/prompts.py): `SYSTEM_PROMPT + PROPERTIES_PROMPT (filled with API docs) + PBT_PROPERTIES_PROMPT + (*)OUTPUT_FORMAT_TEMPLATE`
- What they have in common: they both generate one PTB for one invocation.
- **Their difference**: the PTB generated by single stage prompting ([example](proptest_ai_data/proptests/gpt-4-final/datetime.date.isocalendar/single_stage/pbt_1.py)) has only one test function which tests multiple properties at once; the PTB generated by two stage prompting ([example](proptest_ai_data/proptests/gpt-4-final/datetime.date.isocalendar/two_stage/pbt_1.py)) has multiple testing functions, and each one test one property.

### Evaluation: Property Coverage

- Actual prompt to extract properties p1 for a API method f: reuse prompts in [`prompts.py`](proptest_ai_data/prompts.py) for two stage prompting.
- The code to generate five property mutants for each property pi is in [`prompts.py`](proptest_ai_data/prompts.py): `(Maybe)SYSTEM_PROMPT + MUTANTS_PROMPT (filled with api docs and one property)`

## Ideas for the project

### Method

- A simple baseline: for [`prompts.py`](proptest_ai_data/prompts.py), replace API docs with API source code. (expect improvement from better models)
- Leverage [ghostwriting](https://hypothesis.readthedocs.io/en/latest/ghostwriter.html#) of Hypothesis, e.g., let it ghostwrite test first, then llm refine.
- Vary the number of properties, instead of keeping it no more than 5.
- [`prompts.py`](proptest_ai_data/prompts.py) has two prompts about output format, which seems messy.

### Evaluating PTBs

- Improve current metrics, especially property coverage.
- Add other metrics.

### Experiment Setup

- Baselines: [ghostwriting](https://hypothesis.readthedocs.io/en/latest/ghostwriter.html#) of Hypothesis, PTB from API docs (as mentioned in the paper)
- API to test: could use the ones from the paper or else.