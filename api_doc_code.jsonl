{"function_name": "numpy.resize", "api_doc": "numpy.resize\nnumpy.resize(a, new_shape)[source]\nReturn a new array with the specified shape.\n\nIf the new array is larger than the original array, then the new array is filled with repeated copies of a. Note that this behavior is different from a.resize(new_shape) which fills with zeros instead of repeated copies of a.\n\nParameters\n:\na\narray_like\nArray to be resized.\n\nnew_shape\nint or tuple of int\nShape of resized array.\n\nReturns\n:\nreshaped_array\nndarray\nThe new array is formed from the data in the old array, repeated if necessary to fill out the required number of elements. The data are repeated iterating over the array in C-order.\n\nSee also\n\nnumpy.reshape\nReshape an array without changing the total size.\n\nnumpy.pad\nEnlarge and pad an array.\n\nnumpy.repeat\nRepeat elements of an array.\n\nndarray.resize\nresize an array in-place.\n\nNotes\n\nWhen the total size of the array does not change reshape should be used. In most other cases either indexing (to reduce the size) or padding (to increase the size) may be a more appropriate solution.\n\nWarning: This functionality does not consider axes separately, i.e. it does not apply interpolation/extrapolation. It fills the return array with the required number of elements, iterating over a in C-order, disregarding axes (and cycling back from the start if the new shape is larger). This functionality is therefore not suitable to resize images, or data where each axis represents a separate and distinct entity.\n\nExamples\n\nimport numpy as np\na = np.array([[0,1],[2,3]])\nnp.resize(a,(2,3))\narray([[0, 1, 2],\n       [3, 0, 1]])\nnp.resize(a,(1,4))\narray([[0, 1, 2, 3]])\nnp.resize(a,(2,4))\narray([[0, 1, 2, 3],\n       [0, 1, 2, 3]])", "api_code": "@array_function_dispatch(_resize_dispatcher)\ndef resize(a, new_shape):\n    if isinstance(new_shape, (int, nt.integer)):\n        new_shape = (new_shape,)\n\n    a = ravel(a)\n\n    new_size = 1\n    for dim_length in new_shape:\n        new_size *= dim_length\n        if dim_length < 0:\n            raise ValueError(\n                'all elements of `new_shape` must be non-negative'\n            )\n\n    if a.size == 0 or new_size == 0:\n        # First case must zero fill. The second would have repeats == 0.\n        return np.zeros_like(a, shape=new_shape)\n\n    repeats = -(-new_size // a.size)  # ceil division\n    a = concatenate((a,) * repeats)[:new_size]\n\n    return reshape(a, new_shape)"}
{"function_name": "networkx.algorithms.clique.find_cliques", "api_doc": "find_cliques\nfind_cliques(G, nodes=None)[source]\nReturns all maximal cliques in an undirected graph.\n\nFor each node n, a maximal clique for n is a largest complete subgraph containing n. The largest maximal clique is sometimes called the maximum clique.\n\nThis function returns an iterator over cliques, each of which is a list of nodes. It is an iterative implementation, so should not suffer from recursion depth issues.\n\nThis function accepts a list of nodes and only the maximal cliques containing all of these nodes are returned. It can considerably speed up the running time if some specific cliques are desired.\n\nParameters\n:\nG\nNetworkX graph\nAn undirected graph.\n\nnodes\nlist, optional (default=None)\nIf provided, only yield maximal cliques containing all nodes in nodes. If nodes isn\u2019t a clique itself, a ValueError is raised.\n\nReturns\n:\niterator\nAn iterator over maximal cliques, each of which is a list of nodes in G. If nodes is provided, only the maximal cliques containing all the nodes in nodes are returned. The order of cliques is arbitrary.\n\nRaises\n:\nValueError\nIf nodes is not a clique.\n\nSee also\n\nfind_cliques_recursive\nA recursive version of the same algorithm.\n\nNotes\n\nTo obtain a list of all maximal cliques, use list(find_cliques(G)). However, be aware that in the worst-case, the length of this list can be exponential in the number of nodes in the graph. This function avoids storing all cliques in memory by only keeping current candidate node lists in memory during its search.\n\nThis implementation is based on the algorithm published by Bron and Kerbosch (1973) [1], as adapted by Tomita, Tanaka and Takahashi (2006) [2] and discussed in Cazals and Karande (2008) [3]. It essentially unrolls the recursion used in the references to avoid issues of recursion stack depth (for a recursive implementation, see find_cliques_recursive()).\n\nThis algorithm ignores self-loops and parallel edges, since cliques are not conventionally defined with such edges.\n\nReferences\n\n[1]\nBron, C. and Kerbosch, J. \u201cAlgorithm 457: finding all cliques of an undirected graph\u201d. Communications of the ACM 16, 9 (Sep. 1973), 575\u2013577. <http://portal.acm.org/citation.cfm?doid=362342.362367>\n\n[2]\nEtsuji Tomita, Akira Tanaka, Haruhisa Takahashi, \u201cThe worst-case time complexity for generating all maximal cliques and computational experiments\u201d, Theoretical Computer Science, Volume 363, Issue 1, Computing and Combinatorics, 10th Annual International Conference on Computing and Combinatorics (COCOON 2004), 25 October 2006, Pages 28\u201342 <https://doi.org/10.1016/j.tcs.2006.06.015>\n\n[3]\nF. Cazals, C. Karande, \u201cA note on the problem of reporting maximal cliques\u201d, Theoretical Computer Science, Volume 407, Issues 1\u20133, 6 November 2008, Pages 564\u2013568, <https://doi.org/10.1016/j.tcs.2008.05.010>\n\nExamples\n\nfrom pprint import pprint  # For nice dict formatting\nG = nx.karate_club_graph()\nsum(1 for c in nx.find_cliques(G))  # The number of maximal cliques in G\n36\nmax(nx.find_cliques(G), key=len)  # The largest maximal clique in G\n[0, 1, 2, 3, 13]\nThe size of the largest maximal clique is known as the clique number of the graph, which can be found directly with:\n\nmax(len(c) for c in nx.find_cliques(G))\n5\nOne can also compute the number of maximal cliques in G that contain a given node. The following produces a dictionary keyed by node whose values are the number of maximal cliques in G that contain the node:\n\npprint({n: sum(1 for c in nx.find_cliques(G) if n in c) for n in G})\n{0: 13,\n 1: 6,\n 2: 7,\n 3: 3,\n 4: 2,\n 5: 3,\n 6: 3,\n 7: 1,\n 8: 3,\n 9: 2,\n 10: 2,\n 11: 1,\n 12: 1,\n 13: 2,\n 14: 1,\n 15: 1,\n 16: 1,\n 17: 1,\n 18: 1,\n 19: 2,\n 20: 1,\n 21: 1,\n 22: 1,\n 23: 3,\n 24: 2,\n 25: 2,\n 26: 1,\n 27: 3,\n 28: 2,\n 29: 2,\n 30: 2,\n 31: 4,\n 32: 9,\n 33: 14}\nOr, similarly, the maximal cliques in G that contain a given node. For example, the 4 maximal cliques that contain node 31:\n\n[c for c in nx.find_cliques(G) if 31 in c]\n[[0, 31], [33, 32, 31], [33, 28, 31], [24, 25, 31]]", "api_code": "@not_implemented_for(\"directed\")\n@nx._dispatchable\ndef find_cliques(G, nodes=None):\n    if len(G) == 0:\n        return\n\n    adj = {u: {v for v in G[u] if v != u} for u in G}\n\n    # Initialize Q with the given nodes and subg, cand with their nbrs\n    Q = nodes[:] if nodes is not None else []\n    cand = set(G)\n    for node in Q:\n        if node not in cand:\n            raise ValueError(f\"The given `nodes` {nodes} do not form a clique\")\n        cand &= adj[node]\n\n    if not cand:\n        yield Q[:]\n        return\n\n    subg = cand.copy()\n    stack = []\n    Q.append(None)\n\n    u = max(subg, key=lambda u: len(cand & adj[u]))\n    ext_u = cand - adj[u]\n\n    try:\n        while True:\n            if ext_u:\n                q = ext_u.pop()\n                cand.remove(q)\n                Q[-1] = q\n                adj_q = adj[q]\n                subg_q = subg & adj_q\n                if not subg_q:\n                    yield Q[:]\n                else:\n                    cand_q = cand & adj_q\n                    if cand_q:\n                        stack.append((subg, cand, ext_u))\n                        Q.append(None)\n                        subg = subg_q\n                        cand = cand_q\n                        u = max(subg, key=lambda u: len(cand & adj[u]))\n                        ext_u = cand - adj[u]\n            else:\n                Q.pop()\n                subg, cand, ext_u = stack.pop()\n    except IndexError:\n        pass"}
{"function_name": "statistics.correlation", "api_doc": "statistics.correlation(x, y, /, *, method='linear')\nReturn the Pearson\u2019s correlation coefficient for two inputs. Pearson\u2019s correlation coefficient r takes values between -1 and +1. It measures the strength and direction of a linear relationship.\n\nIf method is \u201cranked\u201d, computes Spearman\u2019s rank correlation coefficient for two inputs. The data is replaced by ranks. Ties are averaged so that equal values receive the same rank. The resulting coefficient measures the strength of a monotonic relationship.\n\nSpearman\u2019s correlation coefficient is appropriate for ordinal data or for continuous data that doesn\u2019t meet the linear proportion requirement for Pearson\u2019s correlation coefficient.\n\nBoth inputs must be of the same length (no less than two), and need not to be constant, otherwise StatisticsError is raised.\n\nExample with Kepler\u2019s laws of planetary motion:\n\n>>>\n# Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and  Neptune\norbital_period = [88, 225, 365, 687, 4331, 10_756, 30_687, 60_190]    # days\ndist_from_sun = [58, 108, 150, 228, 778, 1_400, 2_900, 4_500] # million km\n\n# Show that a perfect monotonic relationship exists\ncorrelation(orbital_period, dist_from_sun, method='ranked')\n1.0\n\n# Observe that a linear relationship is imperfect\nround(correlation(orbital_period, dist_from_sun), 4)\n0.9882\n\n# Demonstrate Kepler's third law: There is a linear correlation\n# between the square of the orbital period and the cube of the\n# distance from the sun.\nperiod_squared = [p * p for p in orbital_period]\ndist_cubed = [d * d * d for d in dist_from_sun]\nround(correlation(period_squared, dist_cubed), 4)\n1.0\nAdded in version 3.10.\n\nChanged in version 3.12: Added support for Spearman\u2019s rank correlation coefficient.", "api_code": "def correlation(x, y, /, *, method='linear'):\n    n = len(x)\n    if len(y) != n:\n        raise StatisticsError('correlation requires that both inputs have same number of data points')\n    if n < 2:\n        raise StatisticsError('correlation requires at least two data points')\n    if method not in {'linear', 'ranked'}:\n        raise ValueError(f'Unknown method: {method!r}')\n    if method == 'ranked':\n        start = (n - 1) / -2            # Center rankings around zero\n        x = _rank(x, start=start)\n        y = _rank(y, start=start)\n    else:\n        xbar = fsum(x) / n\n        ybar = fsum(y) / n\n        x = [xi - xbar for xi in x]\n        y = [yi - ybar for yi in y]\n    sxy = sumprod(x, y)\n    sxx = sumprod(x, x)\n    syy = sumprod(y, y)\n    try:\n        return sxy / _sqrtprod(sxx, syy)\n    except ZeroDivisionError:\n        raise StatisticsError('at least one of the inputs is constant')"}
{"function_name": "networkx.algorithms.approximation.clustering_coefficient.average_clustering", "api_doc": "average_clustering\naverage_clustering(G, trials=1000, seed=None)[source]\nEstimates the average clustering coefficient of G.\n\nThe local clustering of each node in G is the fraction of triangles that actually exist over all possible triangles in its neighborhood. The average clustering coefficient of a graph G is the mean of local clusterings.\n\nThis function finds an approximate average clustering coefficient for G by repeating n times (defined in trials) the following experiment: choose a node at random, choose two of its neighbors at random, and check if they are connected. The approximate coefficient is the fraction of triangles found over the number of trials [1].\n\nParameters\n:\nG\nNetworkX graph\ntrials\ninteger\nNumber of trials to perform (default 1000).\n\nseed\ninteger, random_state, or None (default)\nIndicator of random number generation state. See Randomness.\n\nReturns\n:\nc\nfloat\nApproximated average clustering coefficient.\n\nRaises\n:\nNetworkXNotImplemented\nIf G is directed.\n\nReferences\n\n[1]\nSchank, Thomas, and Dorothea Wagner. Approximating clustering coefficient and transitivity. Universit\u00e4t Karlsruhe, Fakult\u00e4t f\u00fcr Informatik, 2004. https://doi.org/10.5445/IR/1000001239\n\nExamples\n\nfrom networkx.algorithms import approximation\nG = nx.erdos_renyi_graph(10, 0.2, seed=10)\napproximation.average_clustering(G, trials=1000, seed=10)\n0.214", "api_code": "@not_implemented_for(\"directed\")\n@py_random_state(2)\n@nx._dispatchable(name=\"approximate_average_clustering\")\ndef average_clustering(G, trials=1000, seed=None):\n    n = len(G)\n    triangles = 0\n    nodes = list(G)\n    for i in [int(seed.random() * n) for i in range(trials)]:\n        nbrs = list(G[nodes[i]])\n        if len(nbrs) < 2:\n            continue\n        u, v = seed.sample(nbrs, 2)\n        if u in G[v]:\n            triangles += 1\n    return triangles / trials"}
{"function_name": "statistics.pstdev", "api_doc": "statistics.pstdev(data, mu=None)\nReturn the population standard deviation (the square root of the population variance). See pvariance() for arguments and other details.\n\n>>>\npstdev([1.5, 2.5, 2.5, 2.75, 3.25, 4.75])\n0.986893273527251", "api_code": "def pstdev(data, mu=None):\n    T, ss, c, n = _ss(data, mu)\n    if n < 1:\n        raise StatisticsError('pstdev requires at least one data point')\n    mss = ss / n\n    if issubclass(T, Decimal):\n        return _decimal_sqrt_of_frac(mss.numerator, mss.denominator)\n    return _float_sqrt_of_frac(mss.numerator, mss.denominator)"}
{"function_name": "datetime.date.weekday", "api_doc": "date.weekday()\nReturn the day of the week as an integer, where Monday is 0 and Sunday is 6. For example, date(2002, 12, 4).weekday() == 2, a Wednesday. See also isoweekday().", "api_code": "def weekday(self):\n    return (self.toordinal() + 6) % 7"}
{"function_name": "decimal.Decimal.from_float", "api_doc": "classmethod from_float(f)\nAlternative constructor that only accepts instances of float or int.\n\nNote Decimal.from_float(0.1) is not the same as Decimal('0.1'). Since 0.1 is not exactly representable in binary floating point, the value is stored as the nearest representable value which is 0x1.999999999999ap-4. That equivalent value in decimal is 0.1000000000000000055511151231257827021181583404541015625.\n\nNote From Python 3.2 onwards, a Decimal instance can also be constructed directly from a float.\n>>>\nDecimal.from_float(0.1)\nDecimal('0.1000000000000000055511151231257827021181583404541015625')\nDecimal.from_float(float('nan'))\nDecimal('NaN')\nDecimal.from_float(float('inf'))\nDecimal('Infinity')\nDecimal.from_float(float('-inf'))\nDecimal('-Infinity')\nAdded in version 3.1.", "api_code": "@classmethod\ndef from_float(cls, f):\n    if isinstance(f, int):                # handle integer inputs\n        sign = 0 if f >= 0 else 1\n        k = 0\n        coeff = str(abs(f))\n    elif isinstance(f, float):\n        if _math.isinf(f) or _math.isnan(f):\n            return cls(repr(f))\n        if _math.copysign(1.0, f) == 1.0:\n            sign = 0\n        else:\n            sign = 1\n        n, d = abs(f).as_integer_ratio()\n        k = d.bit_length() - 1\n        coeff = str(n*5**k)\n    else:\n        raise TypeError(\"argument must be int or float.\")\n\n    result = _dec_from_triple(sign, coeff, -k)\n    if cls is Decimal:\n        return result\n    else:\n        return cls(result)"}
{"function_name": "decimal.Decimal.shift", "api_doc": "shift(other, context=None)\nReturn the result of shifting the digits of the first operand by an amount specified by the second operand. The second operand must be an integer in the range -precision through precision. The absolute value of the second operand gives the number of places to shift. If the second operand is positive then the shift is to the left; otherwise the shift is to the right. Digits shifted into the coefficient are zeros. The sign and exponent of the first operand are unchanged.", "api_code": "def shift(self, other, context=None):\n    if context is None:\n        context = getcontext()\n\n    other = _convert_other(other, raiseit=True)\n\n    ans = self._check_nans(other, context)\n    if ans:\n        return ans\n\n    if other._exp != 0:\n        return context._raise_error(InvalidOperation)\n    if not (-context.prec <= int(other) <= context.prec):\n        return context._raise_error(InvalidOperation)\n\n    if self._isinfinity():\n        return Decimal(self)\n\n    # get values, pad if necessary\n    torot = int(other)\n    rotdig = self._int\n    topad = context.prec - len(rotdig)\n    if topad > 0:\n        rotdig = '0'*topad + rotdig\n    elif topad < 0:\n        rotdig = rotdig[-topad:]\n\n    # let's shift!\n    if torot < 0:\n        shifted = rotdig[:torot]\n    else:\n        shifted = rotdig + '0'*torot\n        shifted = shifted[-context.prec:]\n\n    return _dec_from_triple(self._sign,\n                                shifted.lstrip('0') or '0', self._exp)"}
{"function_name": "decimal.Decimal.exp", "api_doc": "exp(context=None)\nReturn the value of the (natural) exponential function e**x at the given number. The result is correctly rounded using the ROUND_HALF_EVEN rounding mode.\n\n>>>\nDecimal(1).exp()\nDecimal('2.718281828459045235360287471')\nDecimal(321).exp()\nDecimal('2.561702493119680037517373933E+139')", "api_code": "def exp(self, context=None):\n\n    if context is None:\n        context = getcontext()\n\n    # exp(NaN) = NaN\n    ans = self._check_nans(context=context)\n    if ans:\n        return ans\n\n    # exp(-Infinity) = 0\n    if self._isinfinity() == -1:\n        return _Zero\n\n    # exp(0) = 1\n    if not self:\n        return _One\n\n    # exp(Infinity) = Infinity\n    if self._isinfinity() == 1:\n        return Decimal(self)"}
{"function_name": "decimal.Decimal.as_integer_ratio", "api_doc": "as_integer_ratio()\nReturn a pair (n, d) of integers that represent the given Decimal instance as a fraction, in lowest terms and with a positive denominator:\n\n>>>\nDecimal('-3.14').as_integer_ratio()\n(-157, 50)\nThe conversion is exact. Raise OverflowError on infinities and ValueError on NaNs.\n\nAdded in version 3.6.", "api_code": "def as_integer_ratio(self):\n    return (self._numerator, self._denominator)"}
{"function_name": "statistics.median", "api_doc": "statistics.median(data)\nReturn the median (middle value) of numeric data, using the common \u201cmean of middle two\u201d method. If data is empty, StatisticsError is raised. data can be a sequence or iterable.\n\nThe median is a robust measure of central location and is less affected by the presence of outliers. When the number of data points is odd, the middle data point is returned:\n\n>>>\nmedian([1, 3, 5])\n3\nWhen the number of data points is even, the median is interpolated by taking the average of the two middle values:\n\n>>>\nmedian([1, 3, 5, 7])\n4.0\nThis is suited for when your data is discrete, and you don\u2019t mind that the median may not be an actual data point.\n\nIf the data is ordinal (supports order operations) but not numeric (doesn\u2019t support addition), consider using median_low() or median_high() instead.", "api_code": "def median(data):\n    data = sorted(data)\n    n = len(data)\n    if n == 0:\n        raise StatisticsError(\"no median for empty data\")\n    if n % 2 == 1:\n        return data[n // 2]\n    else:\n        i = n // 2\n        return (data[i - 1] + data[i]) / 2"}
{"function_name": "statistics.linear_regression", "api_doc": "statistics.linear_regression(x, y, /, *, proportional=False)\nReturn the slope and intercept of simple linear regression parameters estimated using ordinary least squares. Simple linear regression describes the relationship between an independent variable x and a dependent variable y in terms of this linear function:\n\ny = slope * x + intercept + noise\n\nwhere slope and intercept are the regression parameters that are estimated, and noise represents the variability of the data that was not explained by the linear regression (it is equal to the difference between predicted and actual values of the dependent variable).\n\nBoth inputs must be of the same length (no less than two), and the independent variable x cannot be constant; otherwise a StatisticsError is raised.\n\nFor example, we can use the release dates of the Monty Python films to predict the cumulative number of Monty Python films that would have been produced by 2019 assuming that they had kept the pace.\n\n>>>\nyear = [1971, 1975, 1979, 1982, 1983]\nfilms_total = [1, 2, 3, 4, 5]\nslope, intercept = linear_regression(year, films_total)\nround(slope * 2019 + intercept)\n16\nIf proportional is true, the independent variable x and the dependent variable y are assumed to be directly proportional. The data is fit to a line passing through the origin. Since the intercept will always be 0.0, the underlying linear function simplifies to:\n\ny = slope * x + noise\n\nContinuing the example from correlation(), we look to see how well a model based on major planets can predict the orbital distances for dwarf planets:\n\n>>>\nmodel = linear_regression(period_squared, dist_cubed, proportional=True)\nslope = model.slope\n\n# Dwarf planets:   Pluto,  Eris,    Makemake, Haumea, Ceres\norbital_periods = [90_560, 204_199, 111_845, 103_410, 1_680]  # days\npredicted_dist = [math.cbrt(slope * (p * p)) for p in orbital_periods]\nlist(map(round, predicted_dist))\n[5912, 10166, 6806, 6459, 414]\n\n[5_906, 10_152, 6_796, 6_450, 414]  # actual distance in million km\n[5906, 10152, 6796, 6450, 414]\nAdded in version 3.10.\n\nChanged in version 3.11: Added support for proportional.", "api_code": "def linear_regression(x, y, /, *, proportional=False):\n    n = len(x)\n    if len(y) != n:\n        raise StatisticsError('linear regression requires that both inputs have same number of data points')\n    if n < 2:\n        raise StatisticsError('linear regression requires at least two data points')\n    if not proportional:\n        xbar = fsum(x) / n\n        ybar = fsum(y) / n\n        x = [xi - xbar for xi in x]  # List because used three times below\n        y = (yi - ybar for yi in y)  # Generator because only used once below\n    sxy = sumprod(x, y) + 0.0        # Add zero to coerce result to a float\n    sxx = sumprod(x, x)\n    try:\n        slope = sxy / sxx   # equivalent to:  covariance(x, y) / variance(x)\n    except ZeroDivisionError:\n        raise StatisticsError('x is constant')\n    intercept = 0.0 if proportional else ybar - slope * xbar\n    return LinearRegression(slope=slope, intercept=intercept)"}
{"function_name": "networkx.algorithms.approximation.clique.large_clique_size", "api_doc": "large_clique_size\nlarge_clique_size(G)[source]\nFind the size of a large clique in a graph.\n\nA clique is a subset of nodes in which each pair of nodes is adjacent. This function is a heuristic for finding the size of a large clique in the graph.\n\nParameters\n:\nG\nNetworkX graph\nReturns\n:\nk: integer\nThe size of a large clique in the graph.\n\nRaises\n:\nNetworkXNotImplemented\nIf the graph is directed or is a multigraph.\n\nSee also\n\nnetworkx.algorithms.approximation.clique.max_clique()\nA function that returns an approximate maximum clique with a guarantee on the approximation ratio.\n\nnetworkx.algorithms.clique\nFunctions for finding the exact maximum clique in a graph.\n\nNotes\n\nThis implementation is from [1]. Its worst case time complexity is \n, where n is the number of nodes in the graph and d is the maximum degree.\n\nThis function is a heuristic, which means it may work well in practice, but there is no rigorous mathematical guarantee on the ratio between the returned number and the actual largest clique size in the graph.\n\nReferences\n\n[1]\nPattabiraman, Bharath, et al. \u201cFast Algorithms for the Maximum Clique Problem on Massive Graphs with Applications to Overlapping Community Detection.\u201d Internet Mathematics 11.4-5 (2015): 421\u2013448. <https://doi.org/10.1080/15427951.2014.986778>\n\nExamples\n\nG = nx.path_graph(10)\nnx.approximation.large_clique_size(G)\n2", "api_code": "@not_implemented_for(\"directed\")\n@not_implemented_for(\"multigraph\")\n@nx._dispatchable\ndef large_clique_size(G):\n    degrees = G.degree\n\n    def _clique_heuristic(G, U, size, best_size):\n        if not U:\n            return max(best_size, size)\n        u = max(U, key=degrees)\n        U.remove(u)\n        N_prime = {v for v in G[u] if degrees[v] >= best_size}\n        return _clique_heuristic(G, U & N_prime, size + 1, best_size)\n\n    best_size = 0\n    nodes = (u for u in G if degrees[u] >= best_size)\n    for u in nodes:\n        neighbors = {v for v in G[u] if degrees[v] >= best_size}\n        best_size = _clique_heuristic(G, neighbors, 1, best_size)\n    return best_size"}
{"function_name": "datetime.datetime.combine", "api_doc": "classmethod datetime.combine(date, time, tzinfo=time.tzinfo)\nReturn a new datetime object whose date components are equal to the given date object\u2019s, and whose time components are equal to the given time object\u2019s. If the tzinfo argument is provided, its value is used to set the tzinfo attribute of the result, otherwise the tzinfo attribute of the time argument is used. If the date argument is a datetime object, its time components and tzinfo attributes are ignored.\n\nFor any datetime object d, d == datetime.combine(d.date(), d.time(), d.tzinfo).\n\nChanged in version 3.6: Added the tzinfo argument.", "api_code": "@classmethod\ndef combine(cls, date, time, tzinfo=True):\n    if not isinstance(date, _date_class):\n        raise TypeError(\"date argument must be a date instance\")\n    if not isinstance(time, _time_class):\n        raise TypeError(\"time argument must be a time instance\")\n    if tzinfo is True:\n        tzinfo = time.tzinfo\n    return cls(date.year, date.month, date.day,\n                time.hour, time.minute, time.second, time.microsecond,\n                tzinfo, fold=time.fold)"}
{"function_name": "decimal.Decimal.fma", "api_doc": "fma(other, third, context=None)\nFused multiply-add. Return self*other+third with no rounding of the intermediate product self*other.\n\n>>>\nDecimal(2).fma(3, 5)\nDecimal('11')", "api_code": "def fma(self, other, third, context=None):\n    if self._is_special or other._is_special:\n        if context is None:\n            context = getcontext()\n        if self._exp == 'N':\n            return context._raise_error(InvalidOperation, 'sNaN', self)\n        if other._exp == 'N':\n            return context._raise_error(InvalidOperation, 'sNaN', other)\n        if self._exp == 'n':\n            product = self\n        elif other._exp == 'n':\n            product = other\n        elif self._exp == 'F':\n            if not other:\n                return context._raise_error(InvalidOperation,\n                                            'INF * 0 in fma')\n            product = _SignedInfinity[self._sign ^ other._sign]\n        elif other._exp == 'F':\n            if not self:\n                return context._raise_error(InvalidOperation,\n                                            '0 * INF in fma')\n            product = _SignedInfinity[self._sign ^ other._sign]\n    else:\n        product = _dec_from_triple(self._sign ^ other._sign,\n                                    str(int(self._int) * int(other._int)),\n                                    self._exp + other._exp)\n\n    return product.__add__(third, context)"}
{"function_name": "pandas.DataFrame.sort_values", "api_doc": "pandas.DataFrame.sort_values\nDataFrame.sort_values(by, *, axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last', ignore_index=False, key=None)[source]\nSort by the values along either axis.\n\nParameters\n:\nby\nstr or list of str\nName or list of names to sort by.\n\nif axis is 0 or \u2018index\u2019 then by may contain index levels and/or column labels.\n\nif axis is 1 or \u2018columns\u2019 then by may contain column levels and/or index labels.\n\naxis\n\u201c{0 or \u2018index\u2019, 1 or \u2018columns\u2019}\u201d, default 0\nAxis to be sorted.\n\nascending\nbool or list of bool, default True\nSort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by.\n\ninplace\nbool, default False\nIf True, perform operation in-place.\n\nkind\n{\u2018quicksort\u2019, \u2018mergesort\u2019, \u2018heapsort\u2019, \u2018stable\u2019}, default \u2018quicksort\u2019\nChoice of sorting algorithm. See also numpy.sort() for more information. mergesort and stable are the only stable algorithms. For DataFrames, this option is only applied when sorting on a single column or label.\n\nna_position\n{\u2018first\u2019, \u2018last\u2019}, default \u2018last\u2019\nPuts NaNs at the beginning if first; last puts NaNs at the end.\n\nignore_index\nbool, default False\nIf True, the resulting axis will be labeled 0, 1, \u2026, n - 1.\n\nkey\ncallable, optional\nApply the key function to the values before sorting. This is similar to the key argument in the builtin sorted() function, with the notable difference that this key function should be vectorized. It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently.\n\nReturns\n:\nDataFrame or None\nDataFrame with sorted values or None if inplace=True.\n\nSee also\n\nDataFrame.sort_index\nSort a DataFrame by the index.\n\nSeries.sort_values\nSimilar method for a Series.\n\nExamples\n\ndf = pd.DataFrame({\n    'col1': ['A', 'A', 'B', np.nan, 'D', 'C'],\n    'col2': [2, 1, 9, 8, 7, 4],\n    'col3': [0, 1, 9, 4, 2, 3],\n    'col4': ['a', 'B', 'c', 'D', 'e', 'F']\n})\ndf\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nSort by col1\n\ndf.sort_values(by=['col1'])\n  col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort by multiple columns\n\ndf.sort_values(by=['col1', 'col2'])\n  col1  col2  col3 col4\n1    A     1     1    B\n0    A     2     0    a\n2    B     9     9    c\n5    C     4     3    F\n4    D     7     2    e\n3  NaN     8     4    D\nSort Descending\n\ndf.sort_values(by='col1', ascending=False)\n  col1  col2  col3 col4\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\n3  NaN     8     4    D\nPutting NAs first\n\ndf.sort_values(by='col1', ascending=False, na_position='first')\n  col1  col2  col3 col4\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\n2    B     9     9    c\n0    A     2     0    a\n1    A     1     1    B\nSorting with a key function\n\ndf.sort_values(by='col4', key=lambda col: col.str.lower())\n   col1  col2  col3 col4\n0    A     2     0    a\n1    A     1     1    B\n2    B     9     9    c\n3  NaN     8     4    D\n4    D     7     2    e\n5    C     4     3    F\nNatural sort with the key argument, using the natsort <https://github.com/SethMMorton/natsort> package.\n\ndf = pd.DataFrame({\n   \"time\": ['0hr', '128hr', '72hr', '48hr', '96hr'],\n   \"value\": [10, 20, 30, 40, 50]\n})\ndf\n    time  value\n0    0hr     10\n1  128hr     20\n2   72hr     30\n3   48hr     40\n4   96hr     50\nfrom natsort import index_natsorted\ndf.sort_values(\n    by=\"time\",\n    key=lambda x: np.argsort(index_natsorted(df[\"time\"]))\n)\n    time  value\n0    0hr     10\n3   48hr     40\n2   72hr     30\n4   96hr     50\n1  128hr     20", "api_code": "def sort_values(\n    self,\n    by: IndexLabel,\n    *,\n    axis: Axis = 0,\n    ascending: bool | list[bool] | tuple[bool, ...] = True,\n    inplace: bool = False,\n    kind: SortKind = \"quicksort\",\n    na_position: str = \"last\",\n    ignore_index: bool = False,\n    key: ValueKeyFunc | None = None,\n) -> DataFrame | None:\n    inplace = validate_bool_kwarg(inplace, \"inplace\")\n    axis = self._get_axis_number(axis)\n    ascending = validate_ascending(ascending)\n    if not isinstance(by, list):\n        by = [by]\n    # error: Argument 1 to \"len\" has incompatible type \"Union[bool, List[bool]]\";\n    # expected \"Sized\"\n    if is_sequence(ascending) and (\n        len(by) != len(ascending)  # type: ignore[arg-type]\n    ):\n        # error: Argument 1 to \"len\" has incompatible type \"Union[bool,\n        # List[bool]]\"; expected \"Sized\"\n        raise ValueError(\n            f\"Length of ascending ({len(ascending)})\"  # type: ignore[arg-type]\n            f\" != length of by ({len(by)})\"\n        )\n    if len(by) > 1:\n        keys = [self._get_label_or_level_values(x, axis=axis) for x in by]\n\n        # need to rewrap columns in Series to apply key function\n        if key is not None:\n            # error: List comprehension has incompatible type List[Series];\n            # expected List[ndarray]\n            keys = [\n                Series(k, name=name)  # type: ignore[misc]\n                for (k, name) in zip(keys, by)\n            ]\n\n        indexer = lexsort_indexer(\n            keys, orders=ascending, na_position=na_position, key=key\n        )\n    elif len(by):\n        # len(by) == 1\n\n        k = self._get_label_or_level_values(by[0], axis=axis)\n\n        # need to rewrap column in Series to apply key function\n        if key is not None:\n            # error: Incompatible types in assignment (expression has type\n            # \"Series\", variable has type \"ndarray\")\n            k = Series(k, name=by[0])  # type: ignore[assignment]\n\n        if isinstance(ascending, (tuple, list)):\n            ascending = ascending[0]\n\n        indexer = nargsort(\n            k, kind=kind, ascending=ascending, na_position=na_position, key=key\n        )\n    else:\n        if inplace:\n            return self._update_inplace(self)\n        else:\n            return self.copy(deep=None)\n\n    if is_range_indexer(indexer, len(indexer)):\n        result = self.copy(deep=(not inplace and not using_copy_on_write()))\n        if ignore_index:\n            result.index = default_index(len(result))\n\n        if inplace:\n            return self._update_inplace(result)\n        else:\n            return result\n\n    new_data = self._mgr.take(\n        indexer, axis=self._get_block_manager_axis(axis), verify=False\n    )\n\n    if ignore_index:\n        new_data.set_axis(\n            self._get_block_manager_axis(axis), default_index(len(indexer))\n        )\n\n    result = self._constructor_from_mgr(new_data, axes=new_data.axes)\n    if inplace:\n        return self._update_inplace(result)\n    else:\n        return result.__finalize__(self, method=\"sort_values\")"}
{"function_name": "networkx.algorithms.assortativity.average_neighbor_degree", "api_doc": "average_neighbor_degree\naverage_neighbor_degree(G, source='out', target='out', nodes=None, weight=None)[source]\nReturns the average degree of the neighborhood of each node.\n\nIn an undirected graph, the neighborhood N(i) of node i contains the nodes that are connected to i by an edge.\n\nFor directed graphs, N(i) is defined according to the parameter source:\n\nif source is \u2018in\u2019, then N(i) consists of predecessors of node i.\n\nif source is \u2018out\u2019, then N(i) consists of successors of node i.\n\nif source is \u2018in+out\u2019, then N(i) is both predecessors and successors.\n\nThe average neighborhood degree of a node i is\n\n \n \nwhere N(i) are the neighbors of node i and k_j is the degree of node j which belongs to N(i). For weighted graphs, an analogous measure can be defined [1],\n\n \n \nwhere s_i is the weighted degree of node i, w_{ij} is the weight of the edge that links i and j and N(i) are the neighbors of node i.\n\nParameters\n:\nG\nNetworkX graph\nsource\nstring (\u201cin\u201d|\u201dout\u201d|\u201din+out\u201d), optional (default=\u201dout\u201d)\nDirected graphs only. Use \u201cin\u201d- or \u201cout\u201d-neighbors of source node.\n\ntarget\nstring (\u201cin\u201d|\u201dout\u201d|\u201din+out\u201d), optional (default=\u201dout\u201d)\nDirected graphs only. Use \u201cin\u201d- or \u201cout\u201d-degree for target node.\n\nnodes\nlist or iterable, optional (default=G.nodes)\nCompute neighbor degree only for specified nodes.\n\nweight\nstring or None, optional (default=None)\nThe edge attribute that holds the numerical value used as a weight. If None, then each edge has weight 1.\n\nReturns\n:\nd: dict\nA dictionary keyed by node to the average degree of its neighbors.\n\nRaises\n:\nNetworkXError\nIf either source or target are not one of \u2018in\u2019, \u2018out\u2019, or \u2018in+out\u2019. If either source or target is passed for an undirected graph.\n\nSee also\n\naverage_degree_connectivity\nReferences\n\n[1]\nA. Barrat, M. Barth\u00e9lemy, R. Pastor-Satorras, and A. Vespignani, \u201cThe architecture of complex weighted networks\u201d. PNAS 101 (11): 3747\u20133752 (2004).\n\nExamples\n\nG = nx.path_graph(4)\nG.edges[0, 1][\"weight\"] = 5\nG.edges[2, 3][\"weight\"] = 3\nnx.average_neighbor_degree(G)\n{0: 2.0, 1: 1.5, 2: 1.5, 3: 2.0}\nnx.average_neighbor_degree(G, weight=\"weight\")\n{0: 2.0, 1: 1.1666666666666667, 2: 1.25, 3: 2.0}\nG = nx.DiGraph()\nnx.add_path(G, [0, 1, 2, 3])\nnx.average_neighbor_degree(G, source=\"in\", target=\"in\")\n{0: 0.0, 1: 0.0, 2: 1.0, 3: 1.0}\nnx.average_neighbor_degree(G, source=\"out\", target=\"out\")\n{0: 1.0, 1: 1.0, 2: 0.0, 3: 0.0}", "api_code": "@nx._dispatchable(edge_attrs=\"weight\")\ndef average_neighbor_degree(G, source=\"out\", target=\"out\", nodes=None, weight=None):\n    if G.is_directed():\n        if source == \"in\":\n            source_degree = G.in_degree\n        elif source == \"out\":\n            source_degree = G.out_degree\n        elif source == \"in+out\":\n            source_degree = G.degree\n        else:\n            raise nx.NetworkXError(\n                f\"source argument {source} must be 'in', 'out' or 'in+out'\"\n            )\n\n        if target == \"in\":\n            target_degree = G.in_degree\n        elif target == \"out\":\n            target_degree = G.out_degree\n        elif target == \"in+out\":\n            target_degree = G.degree\n        else:\n            raise nx.NetworkXError(\n                f\"target argument {target} must be 'in', 'out' or 'in+out'\"\n            )\n    else:\n        if source != \"out\" or target != \"out\":\n            raise nx.NetworkXError(\n                f\"source and target arguments are only supported for directed graphs\"\n            )\n        source_degree = target_degree = G.degree\n\n    # precompute target degrees -- should *not* be weighted degree\n    t_deg = dict(target_degree())\n\n    # Set up both predecessor and successor neighbor dicts leaving empty if not needed\n    G_P = G_S = {n: {} for n in G}\n    if G.is_directed():\n        # \"in\" or \"in+out\" cases: G_P contains predecessors\n        if \"in\" in source:\n            G_P = G.pred\n        # \"out\" or \"in+out\" cases: G_S contains successors\n        if \"out\" in source:\n            G_S = G.succ\n    else:\n        # undirected leave G_P empty but G_S is the adjacency\n        G_S = G.adj\n\n    # Main loop: Compute average degree of neighbors\n    avg = {}\n    for n, deg in source_degree(nodes, weight=weight):\n        # handle degree zero average\n        if deg == 0:\n            avg[n] = 0.0\n            continue\n\n        # we sum over both G_P and G_S, but one of the two is usually empty.\n        if weight is None:\n            avg[n] = (\n                sum(t_deg[nbr] for nbr in G_S[n]) + sum(t_deg[nbr] for nbr in G_P[n])\n            ) / deg\n        else:\n            avg[n] = (\n                sum(dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_S[n].items())\n                + sum(dd.get(weight, 1) * t_deg[nbr] for nbr, dd in G_P[n].items())\n            ) / deg\n    return avg"}
{"function_name": "networkx.algorithms.dag.is_aperiodic", "api_doc": "is_aperiodic\nis_aperiodic(G)[source]\nReturns True if G is aperiodic.\n\nA directed graph is aperiodic if there is no integer k > 1 that divides the length of every cycle in the graph.\n\nParameters\n:\nG\nNetworkX DiGraph\nA directed graph\n\nReturns\n:\nbool\nTrue if the graph is aperiodic False otherwise\n\nRaises\n:\nNetworkXError\nIf G is not directed\n\nNotes\n\nThis uses the method outlined in [1], which runs in \n time given \n edges in G. Note that a graph is not aperiodic if it is acyclic as every integer trivial divides length 0 cycles.\n\nReferences\n\n[1]\nJarvis, J. P.; Shier, D. R. (1996), \u201cGraph-theoretic analysis of finite Markov chains,\u201d in Shier, D. R.; Wallenius, K. T., Applied Mathematical Modeling: A Multidisciplinary Approach, CRC Press.\n\nExamples\n\nA graph consisting of one cycle, the length of which is 2. Therefore k = 2 divides the length of every cycle in the graph and thus the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 1)])\nnx.is_aperiodic(DG)\nFalse\nA graph consisting of two cycles: one of length 2 and the other of length 3. The cycle lengths are coprime, so there is no single value of k where k > 1 that divides each cycle length and therefore the graph is aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 3), (3, 1), (1, 4), (4, 1)])\nnx.is_aperiodic(DG)\nTrue\nA graph consisting of two cycles: one of length 2 and the other of length 4. The lengths of the cycles share a common factor k = 2, and therefore the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 1), (3, 4), (4, 5), (5, 6), (6, 3)])\nnx.is_aperiodic(DG)\nFalse\nAn acyclic graph, therefore the graph is not aperiodic:\n\nDG = nx.DiGraph([(1, 2), (2, 3)])\nnx.is_aperiodic(DG)\nFalse", "api_code": "@nx._dispatchable\ndef is_aperiodic(G):\n    if not G.is_directed():\n        raise nx.NetworkXError(\"is_aperiodic not defined for undirected graphs\")\n    if len(G) == 0:\n        raise nx.NetworkXPointlessConcept(\"Graph has no nodes.\")\n    s = arbitrary_element(G)\n    levels = {s: 0}\n    this_level = [s]\n    g = 0\n    lev = 1\n    while this_level:\n        next_level = []\n        for u in this_level:\n            for v in G[u]:\n                if v in levels:  # Non-Tree Edge\n                    g = gcd(g, levels[u] - levels[v] + 1)\n                else:  # Tree Edge\n                    next_level.append(v)\n                    levels[v] = lev\n        this_level = next_level\n        lev += 1\n    if len(levels) == len(G):  # All nodes in tree\n        return g == 1\n    else:\n        return g == 1 and nx.is_aperiodic(G.subgraph(set(G) - set(levels)))"}
{"function_name": "statistics.geometric_mean", "api_doc": "statistics.geometric_mean(data)\nConvert data to floats and compute the geometric mean.\n\nThe geometric mean indicates the central tendency or typical value of the data using the product of the values (as opposed to the arithmetic mean which uses their sum).\n\nRaises a StatisticsError if the input dataset is empty, if it contains a zero, or if it contains a negative value. The data may be a sequence or iterable.\n\nNo special efforts are made to achieve exact results. (However, this may change in the future.)\n\n>>>\nround(geometric_mean([54, 24, 36]), 1)\n36.0\nAdded in version 3.8.", "api_code": "def geometric_mean(data):\n    n = 0\n    found_zero = False\n    def count_positive(iterable):\n        nonlocal n, found_zero\n        for n, x in enumerate(iterable, start=1):\n            if x > 0.0 or math.isnan(x):\n                yield x\n            elif x == 0.0:\n                found_zero = True\n            else:\n                raise StatisticsError('No negative inputs allowed', x)\n    total = fsum(map(log, count_positive(data)))\n    if not n:\n        raise StatisticsError('Must have a non-empty dataset')\n    if math.isnan(total):\n        return math.nan\n    if found_zero:\n        return math.nan if total == math.inf else 0.0\n    return exp(total / n)"}
{"function_name": "datetime.datetime.fromisoformat", "api_doc": "classmethod datetime.fromisoformat(date_string)\nReturn a datetime corresponding to a date_string in any valid ISO 8601 format, with the following exceptions:\n\nTime zone offsets may have fractional seconds.\n\nThe T separator may be replaced by any single unicode character.\n\nFractional hours and minutes are not supported.\n\nReduced precision dates are not currently supported (YYYY-MM, YYYY).\n\nExtended date representations are not currently supported (\u00b1YYYYYY-MM-DD).\n\nOrdinal dates are not currently supported (YYYY-OOO).\n\nExamples:\n\n>>>\nfrom datetime import datetime\ndatetime.fromisoformat('2011-11-04')\ndatetime.datetime(2011, 11, 4, 0, 0)\ndatetime.fromisoformat('20111104')\ndatetime.datetime(2011, 11, 4, 0, 0)\ndatetime.fromisoformat('2011-11-04T00:05:23')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\ndatetime.fromisoformat('2011-11-04T00:05:23Z')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, tzinfo=datetime.timezone.utc)\ndatetime.fromisoformat('20111104T000523')\ndatetime.datetime(2011, 11, 4, 0, 5, 23)\ndatetime.fromisoformat('2011-W01-2T00:05:23.283')\ndatetime.datetime(2011, 1, 4, 0, 5, 23, 283000)\ndatetime.fromisoformat('2011-11-04 00:05:23.283')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000)\ndatetime.fromisoformat('2011-11-04 00:05:23.283+00:00')\ndatetime.datetime(2011, 11, 4, 0, 5, 23, 283000, tzinfo=datetime.timezone.utc)\ndatetime.fromisoformat('2011-11-04T00:05:23+04:00')   \ndatetime.datetime(2011, 11, 4, 0, 5, 23,\n    tzinfo=datetime.timezone(datetime.timedelta(seconds=14400)))\nAdded in version 3.7.\n\nChanged in version 3.11: Previously, this method only supported formats that could be emitted by date.isoformat() or datetime.isoformat().", "api_code": "@classmethod\ndef fromisoformat(cls, date_string):\n    if not isinstance(date_string, str):\n        raise TypeError('fromisoformat: argument must be str')\n\n    if len(date_string) < 7:\n        raise ValueError(f'Invalid isoformat string: {date_string!r}')\n\n    # Split this at the separator\n    try:\n        separator_location = _find_isoformat_datetime_separator(date_string)\n        dstr = date_string[0:separator_location]\n        tstr = date_string[(separator_location+1):]\n\n        date_components = _parse_isoformat_date(dstr)\n    except ValueError:\n        raise ValueError(\n            f'Invalid isoformat string: {date_string!r}') from None\n\n    if tstr:\n        try:\n            time_components, became_next_day, error_from_components = _parse_isoformat_time(tstr)\n        except ValueError:\n            raise ValueError(\n                f'Invalid isoformat string: {date_string!r}') from None\n        else:\n            if error_from_components:\n                raise ValueError(\"minute, second, and microsecond must be 0 when hour is 24\")\n\n            if became_next_day:\n                year, month, day = date_components\n                # Only wrap day/month when it was previously valid\n                if month <= 12 and day <= (days_in_month := _days_in_month(year, month)):\n                    # Calculate midnight of the next day\n                    day += 1\n                    if day > days_in_month:\n                        day = 1\n                        month += 1\n                        if month > 12:\n                            month = 1\n                            year += 1\n                    date_components = [year, month, day]\n    else:\n        time_components = [0, 0, 0, 0, None]\n\n    return cls(*(date_components + time_components))"}
{"function_name": "datetime.date.isocalendar", "api_doc": "date.isocalendar()\nReturn a named tuple object with three components: year, week and weekday.\n\nThe ISO calendar is a widely used variant of the Gregorian calendar. [3]\n\nThe ISO year consists of 52 or 53 full weeks, and where a week starts on a Monday and ends on a Sunday. The first week of an ISO year is the first (Gregorian) calendar week of a year containing a Thursday. This is called week number 1, and the ISO year of that Thursday is the same as its Gregorian year.\n\nFor example, 2004 begins on a Thursday, so the first week of ISO year 2004 begins on Monday, 29 Dec 2003 and ends on Sunday, 4 Jan 2004:\n\n>>>\nfrom datetime import date\ndate(2003, 12, 29).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=1)\ndate(2004, 1, 4).isocalendar()\ndatetime.IsoCalendarDate(year=2004, week=1, weekday=7)\nChanged in version 3.9: Result changed from a tuple to a named tuple.", "api_code": "def isocalendar(self):\n    year = self._year\n    week1monday = _isoweek1monday(year)\n    today = _ymd2ord(self._year, self._month, self._day)\n    # Internally, week and day have origin 0\n    week, day = divmod(today - week1monday, 7)\n    if week < 0:\n        year -= 1\n        week1monday = _isoweek1monday(year)\n        week, day = divmod(today - week1monday, 7)\n    elif week >= 52:\n        if today >= _isoweek1monday(year+1):\n            year += 1\n            week = 0\n    return _IsoCalendarDate(year, week+1, day+1)"}
{"function_name": "dateutil.parser.isoparse", "api_doc": "parser.isoparse(dt_str)\uf0c1\nParse an ISO-8601 datetime string into a datetime.datetime.\n\nAn ISO-8601 datetime string consists of a date portion, followed optionally by a time portion - the date and time portions are separated by a single character separator, which is T in the official standard. Incomplete date formats (such as YYYY-MM) may not be combined with a time portion.\n\nSupported date formats are:\n\nCommon:\n\nYYYY\n\nYYYY-MM\n\nYYYY-MM-DD or YYYYMMDD\n\nUncommon:\n\nYYYY-Www or YYYYWww - ISO week (day defaults to 0)\n\nYYYY-Www-D or YYYYWwwD - ISO week and day\n\nThe ISO week and day numbering follows the same logic as datetime.date.isocalendar().\n\nSupported time formats are:\n\nhh\n\nhh:mm or hhmm\n\nhh:mm:ss or hhmmss\n\nhh:mm:ss.ssssss (Up to 6 sub-second digits)\n\nMidnight is a special case for hh, as the standard supports both 00:00 and 24:00 as a representation. The decimal separator can be either a dot or a comma.\n\nCaution\n\nSupport for fractional components other than seconds is part of the ISO-8601 standard, but is not currently implemented in this parser.\n\nSupported time zone offset formats are:\n\nZ (UTC)\n\n\u00b1HH:MM\n\n\u00b1HHMM\n\n\u00b1HH\n\nOffsets will be represented as dateutil.tz.tzoffset objects, with the exception of UTC, which will be represented as dateutil.tz.tzutc. Time zone offsets equivalent to UTC (such as +00:00) will also be represented as dateutil.tz.tzutc.\n\nParameters\n:\ndt_str \u2013 A string or stream containing only an ISO-8601 datetime string\n\nReturns\n:\nReturns a datetime.datetime representing the string. Unspecified components default to their lowest value.\n\nWarning\n\nAs of version 2.7.0, the strictness of the parser should not be considered a stable part of the contract. Any valid ISO-8601 string that parses correctly with the default settings will continue to parse correctly in future versions, but invalid strings that currently fail (e.g. 2017-01-01T00:00+00:00:00) are not guaranteed to continue failing in future versions if they encode a valid date.\n\nNew in version 2.7.0.", "api_code": "@_takes_ascii\ndef isoparse(self, dt_str):\n    components, pos = self._parse_isodate(dt_str)\n\n    if len(dt_str) > pos:\n        if self._sep is None or dt_str[pos:pos + 1] == self._sep:\n            components += self._parse_isotime(dt_str[pos + 1:])\n        else:\n            raise ValueError('String contains unknown ISO components')\n\n    if len(components) > 3 and components[3] == 24:\n        components[3] = 0\n        return datetime(*components) + timedelta(days=1)\n\n    return datetime(*components)"}
{"function_name": "statistics.mean", "api_doc": "statistics.mean(data)\nReturn the sample arithmetic mean of data which can be a sequence or iterable.\n\nThe arithmetic mean is the sum of the data divided by the number of data points. It is commonly called \u201cthe average\u201d, although it is only one of many different mathematical averages. It is a measure of the central location of the data.\n\nIf data is empty, StatisticsError will be raised.\n\nSome examples of use:\n\n>>>\nmean([1, 2, 3, 4, 4])\n2.8\nmean([-1.0, 2.5, 3.25, 5.75])\n2.625\n\nfrom fractions import Fraction as F\nmean([F(3, 7), F(1, 21), F(5, 3), F(1, 3)])\nFraction(13, 21)\n\nfrom decimal import Decimal as D\nmean([D(\"0.5\"), D(\"0.75\"), D(\"0.625\"), D(\"0.375\")])\nDecimal('0.5625')\nNote The mean is strongly affected by outliers and is not necessarily a typical example of the data points. For a more robust, although less efficient, measure of central tendency, see median().\nThe sample mean gives an unbiased estimate of the true population mean, so that when taken on average over all the possible samples, mean(sample) converges on the true mean of the entire population. If data represents the entire population rather than a sample, then mean(data) is equivalent to calculating the true population mean \u03bc.", "api_code": "def mean(data):\n    T, total, n = _sum(data)\n    if n < 1:\n        raise StatisticsError('mean requires at least one data point')\n    return _convert(total / n, T)"}
{"function_name": "networkx.algorithms.cycles.find_cycle", "api_doc": "find_cycle\nfind_cycle(G, source=None, orientation=None)[source]\nReturns a cycle found via depth-first traversal.\n\nThe cycle is a list of edges indicating the cyclic path. Orientation of directed edges is controlled by orientation.\n\nParameters\n:\nG\ngraph\nA directed/undirected graph/multigraph.\n\nsource\nnode, list of nodes\nThe node from which the traversal begins. If None, then a source is chosen arbitrarily and repeatedly until all edges from each node in the graph are searched.\n\norientation\nNone | \u2018original\u2019 | \u2018reverse\u2019 | \u2018ignore\u2019 (default: None)\nFor directed graphs and directed multigraphs, edge traversals need not respect the original orientation of the edges. When set to \u2018reverse\u2019 every edge is traversed in the reverse direction. When set to \u2018ignore\u2019, every edge is treated as undirected. When set to \u2018original\u2019, every edge is treated as directed. In all three cases, the yielded edge tuples add a last entry to indicate the direction in which that edge was traversed. If orientation is None, the yielded edge has no direction indicated. The direction is respected, but not reported.\n\nReturns\n:\nedges\ndirected edges\nA list of directed edges indicating the path taken for the loop. If no cycle is found, then an exception is raised. For graphs, an edge is of the form (u, v) where u and v are the tail and head of the edge as determined by the traversal. For multigraphs, an edge is of the form (u, v, key), where key is the key of the edge. When the graph is directed, then u and v are always in the order of the actual directed edge. If orientation is not None then the edge tuple is extended to include the direction of traversal (\u2018forward\u2019 or \u2018reverse\u2019) on that edge.\n\nRaises\n:\nNetworkXNoCycle\nIf no cycle was found.\n\nSee also\n\nsimple_cycles\nExamples\n\nIn this example, we construct a DAG and find, in the first call, that there are no directed cycles, and so an exception is raised. In the second call, we ignore edge orientations and find that there is an undirected cycle. Note that the second call finds a directed cycle while effectively traversing an undirected graph, and so, we found an \u201cundirected cycle\u201d. This means that this DAG structure does not form a directed tree (which is also known as a polytree).\n\nG = nx.DiGraph([(0, 1), (0, 2), (1, 2)])\nnx.find_cycle(G, orientation=\"original\")\nTraceback (most recent call last):\n    ...\nnetworkx.exception.NetworkXNoCycle: No cycle found.\nlist(nx.find_cycle(G, orientation=\"ignore\"))\n[(0, 1, 'forward'), (1, 2, 'forward'), (0, 2, 'reverse')]", "api_code": "@nx._dispatchable\ndef find_cycle(G, source=None, orientation=None):\n    if not G.is_directed() or orientation in (None, \"original\"):\n\n        def tailhead(edge):\n            return edge[:2]\n\n    elif orientation == \"reverse\":\n\n        def tailhead(edge):\n            return edge[1], edge[0]\n\n    elif orientation == \"ignore\":\n\n        def tailhead(edge):\n            if edge[-1] == \"reverse\":\n                return edge[1], edge[0]\n            return edge[:2]\n\n    explored = set()\n    cycle = []\n    final_node = None\n    for start_node in G.nbunch_iter(source):\n        if start_node in explored:\n            # No loop is possible.\n            continue\n\n        edges = []\n        # All nodes seen in this iteration of edge_dfs\n        seen = {start_node}\n        # Nodes in active path.\n        active_nodes = {start_node}\n        previous_head = None\n\n        for edge in nx.edge_dfs(G, start_node, orientation):\n            # Determine if this edge is a continuation of the active path.\n            tail, head = tailhead(edge)\n            if head in explored:\n                # Then we've already explored it. No loop is possible.\n                continue\n            if previous_head is not None and tail != previous_head:\n                # This edge results from backtracking.\n                # Pop until we get a node whose head equals the current tail.\n                # So for example, we might have:\n                #  (0, 1), (1, 2), (2, 3), (1, 4)\n                # which must become:\n                #  (0, 1), (1, 4)\n                while True:\n                    try:\n                        popped_edge = edges.pop()\n                    except IndexError:\n                        edges = []\n                        active_nodes = {tail}\n                        break\n                    else:\n                        popped_head = tailhead(popped_edge)[1]\n                        active_nodes.remove(popped_head)\n\n                    if edges:\n                        last_head = tailhead(edges[-1])[1]\n                        if tail == last_head:\n                            break\n            edges.append(edge)\n\n            if head in active_nodes:\n                # We have a loop!\n                cycle.extend(edges)\n                final_node = head\n                break\n            else:\n                seen.add(head)\n                active_nodes.add(head)\n                previous_head = head\n\n        if cycle:\n            break\n        else:\n            explored.update(seen)\n\n    else:\n        assert len(cycle) == 0\n        raise nx.exception.NetworkXNoCycle(\"No cycle found.\")\n\n    # We now have a list of edges which ends on a cycle.\n    # So we need to remove from the beginning edges that are not relevant.\n\n    for i, edge in enumerate(cycle):\n        tail, head = tailhead(edge)\n        if tail == final_node:\n            break\n\n    return cycle[i:]"}
{"function_name": "networkx.algorithms.approximation.dominating_set.min_weighted_dominating_set", "api_doc": "min_weighted_dominating_set\nmin_weighted_dominating_set(G, weight=None)[source]\nReturns a dominating set that approximates the minimum weight node dominating set.\n\nParameters\n:\nG\nNetworkX graph\nUndirected graph.\n\nweight\nstring\nThe node attribute storing the weight of an node. If provided, the node attribute with this key must be a number for each node. If not provided, each node is assumed to have weight one.\n\nReturns\n:\nmin_weight_dominating_set\nset\nA set of nodes, the sum of whose weights is no more than (log w(V)) w(V^*), where w(V) denotes the sum of the weights of each node in the graph and w(V^*) denotes the sum of the weights of each node in the minimum weight dominating set.\n\nRaises\n:\nNetworkXNotImplemented\nIf G is directed.\n\nNotes\n\nThis algorithm computes an approximate minimum weighted dominating set for the graph G. The returned solution has weight (log w(V)) w(V^*), where w(V) denotes the sum of the weights of each node in the graph and w(V^*) denotes the sum of the weights of each node in the minimum weight dominating set for the graph.\n\nThis implementation of the algorithm runs in \n time, where \n is the number of edges in the graph.\n\nReferences\n\n[1]\nVazirani, Vijay V. Approximation Algorithms. Springer Science & Business Media, 2001.\n\nExamples\n\nG = nx.Graph([(0, 1), (0, 4), (1, 4), (1, 2), (2, 3), (3, 4), (2, 5)])\nnx.approximation.min_weighted_dominating_set(G)\n{1, 2, 4}", "api_code": "@not_implemented_for(\"directed\")\n@nx._dispatchable(node_attrs=\"weight\")\ndef min_weighted_dominating_set(G, weight=None):\n    # The unique dominating set for the null graph is the empty set.\n    if len(G) == 0:\n        return set()\n\n    # This is the dominating set that will eventually be returned.\n    dom_set = set()\n\n    def _cost(node_and_neighborhood):\n        \"\"\"Returns the cost-effectiveness of greedily choosing the given\n        node.\n\n        `node_and_neighborhood` is a two-tuple comprising a node and its\n        closed neighborhood.\n\n        \"\"\"\n        v, neighborhood = node_and_neighborhood\n        return G.nodes[v].get(weight, 1) / len(neighborhood - dom_set)\n\n    # This is a set of all vertices not already covered by the\n    # dominating set.\n    vertices = set(G)\n    # This is a dictionary mapping each node to the closed neighborhood\n    # of that node.\n    neighborhoods = {v: {v} | set(G[v]) for v in G}\n\n    # Continue until all vertices are adjacent to some node in the\n    # dominating set.\n    while vertices:\n        # Find the most cost-effective node to add, along with its\n        # closed neighborhood.\n        dom_node, min_set = min(neighborhoods.items(), key=_cost)\n        # Add the node to the dominating set and reduce the remaining\n        # set of nodes to cover.\n        dom_set.add(dom_node)\n        del neighborhoods[dom_node]\n        vertices -= min_set\n\n    return dom_set"}
{"function_name": "datetime.timedelta.total_seconds", "api_doc": "timedelta.total_seconds()\nReturn the total number of seconds contained in the duration. Equivalent to td / timedelta(seconds=1). For interval units other than seconds, use the division form directly (e.g. td / timedelta(microseconds=1)).\n\nNote that for very large time intervals (greater than 270 years on most platforms) this method will lose microsecond accuracy.\n\nAdded in version 3.2.\n\nExamples of usage: timedelta\nAn additional example of normalization:\n\n>>>\n# Components of another_year add up to exactly 365 days\nfrom datetime import timedelta\nyear = timedelta(days=365)\nanother_year = timedelta(weeks=40, days=84, hours=23,\n                         minutes=50, seconds=600)\nyear == another_year\nTrue\nyear.total_seconds()\n31536000.0", "api_code": "def total_seconds(self):\n        return ((self.days * 86400 + self.seconds) * 10**6 +\n                self.microseconds) / 10**6"}
{"function_name": "numpy.linalg.norm", "api_doc": "numpy.linalg.norm\nlinalg.norm(x, ord=None, axis=None, keepdims=False)[source]\nMatrix or vector norm.\n\nThis function is able to return one of eight different matrix norms, or one of an infinite number of vector norms (described below), depending on the value of the ord parameter.\n\nParameters:\nxarray_like\nInput array. If axis is None, x must be 1-D or 2-D, unless ord is None. If both axis and ord are None, the 2-norm of x.ravel will be returned.\n\nord{non-zero int, inf, -inf, \u2018fro\u2019, \u2018nuc\u2019}, optional\nOrder of the norm (see table under Notes). inf means numpy\u2019s inf object. The default is None.\n\naxis{None, int, 2-tuple of ints}, optional.\nIf axis is an integer, it specifies the axis of x along which to compute the vector norms. If axis is a 2-tuple, it specifies the axes that hold 2-D matrices, and the matrix norms of these matrices are computed. If axis is None then either a vector norm (when x is 1-D) or a matrix norm (when x is 2-D) is returned. The default is None.\n\nNew in version 1.8.0.\n\nkeepdimsbool, optional\nIf this is set to True, the axes which are normed over are left in the result as dimensions with size one. With this option the result will broadcast correctly against the original x.\n\nNew in version 1.10.0.\n\nReturns:\nn\nfloat or ndarray\nNorm of the matrix or vector(s).\n\nSee also\n\nscipy.linalg.norm\nSimilar function in SciPy.\n\nNotes\n\nFor values of ord < 1, the result is, strictly speaking, not a mathematical \u2018norm\u2019, but it may still be useful for various numerical purposes.\n\nThe following norms can be calculated:\n\nord\n\nnorm for matrices\n\nnorm for vectors\n\nNone\n\nFrobenius norm\n\n2-norm\n\n\u2018fro\u2019\n\nFrobenius norm\n\n\u2013\n\n\u2018nuc\u2019\n\nnuclear norm\n\n\u2013\n\ninf\n\nmax(sum(abs(x), axis=1))\n\nmax(abs(x))\n\n-inf\n\nmin(sum(abs(x), axis=1))\n\nmin(abs(x))\n\n0\n\n\u2013\n\nsum(x != 0)\n\n1\n\nmax(sum(abs(x), axis=0))\n\nas below\n\n-1\n\nmin(sum(abs(x), axis=0))\n\nas below\n\n2\n\n2-norm (largest sing. value)\n\nas below\n\n-2\n\nsmallest singular value\n\nas below\n\nother\n\n\u2013\n\nsum(abs(x)**ord)**(1./ord)\n\nThe Frobenius norm is given by [1]:\n\n\nThe nuclear norm is the sum of the singular values.\n\nBoth the Frobenius and nuclear norm orders are only defined for matrices and raise a ValueError when x.ndim != 2.\n\nReferences\n\n[1]\nG. H. Golub and C. F. Van Loan, Matrix Computations, Baltimore, MD, Johns Hopkins University Press, 1985, pg. 15\n\nExamples\n\nimport numpy as np\nfrom numpy import linalg as LA\na = np.arange(9) - 4\na\narray([-4, -3, -2, ...,  2,  3,  4])\nb = a.reshape((3, 3))\nb\narray([[-4, -3, -2],\n       [-1,  0,  1],\n       [ 2,  3,  4]])\nLA.norm(a)\n7.745966692414834\nLA.norm(b)\n7.745966692414834\nLA.norm(b, 'fro')\n7.745966692414834\nLA.norm(a, np.inf)\n4.0\nLA.norm(b, np.inf)\n9.0\nLA.norm(a, -np.inf)\n0.0\nLA.norm(b, -np.inf)\n2.0\nLA.norm(a, 1)\n20.0\nLA.norm(b, 1)\n7.0\nLA.norm(a, -1)\n-4.6566128774142013e-010\nLA.norm(b, -1)\n6.0\nLA.norm(a, 2)\n7.745966692414834\nLA.norm(b, 2)\n7.3484692283495345\nLA.norm(a, -2)\n0.0\nLA.norm(b, -2)\n1.8570331885190563e-016 # may vary\nLA.norm(a, 3)\n5.8480354764257312 # may vary\nLA.norm(a, -3)\n0.0\nUsing the axis argument to compute vector norms:\n\nc = np.array([[ 1, 2, 3],\n              [-1, 1, 4]])\nLA.norm(c, axis=0)\narray([ 1.41421356,  2.23606798,  5.        ])\nLA.norm(c, axis=1)\narray([ 3.74165739,  4.24264069])\nLA.norm(c, ord=1, axis=1)\narray([ 6.,  6.])\nUsing the axis argument to compute matrix norms:\n\nm = np.arange(8).reshape(2,2,2)\nLA.norm(m, axis=(1,2))\narray([  3.74165739,  11.22497216])\nLA.norm(m[0, :, :]), LA.norm(m[1, :, :])\n(3.7416573867739413, 11.224972160321824)", "api_code": "@array_function_dispatch(_norm_dispatcher)\ndef norm(x, ord=None, axis=None, keepdims=False):\n    x = asarray(x)\n\n    if not issubclass(x.dtype.type, (inexact, object_)):\n        x = x.astype(float)\n\n    # Immediately handle some default, simple, fast, and common cases.\n    if axis is None:\n        ndim = x.ndim\n        if (\n            (ord is None) or\n            (ord in ('f', 'fro') and ndim == 2) or\n            (ord == 2 and ndim == 1)\n        ):\n            x = x.ravel(order='K')\n            if isComplexType(x.dtype.type):\n                x_real = x.real\n                x_imag = x.imag\n                sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n            else:\n                sqnorm = x.dot(x)\n            ret = sqrt(sqnorm)\n            if keepdims:\n                ret = ret.reshape(ndim*[1])\n            return ret\n\n    # Normalize the `axis` argument to a tuple.\n    nd = x.ndim\n    if axis is None:\n        axis = tuple(range(nd))\n    elif not isinstance(axis, tuple):\n        try:\n            axis = int(axis)\n        except Exception as e:\n            raise TypeError(\n                \"'axis' must be None, an integer or a tuple of integers\"\n            ) from e\n        axis = (axis,)\n\n    if len(axis) == 1:\n        if ord == inf:\n            return abs(x).max(axis=axis, keepdims=keepdims)\n        elif ord == -inf:\n            return abs(x).min(axis=axis, keepdims=keepdims)\n        elif ord == 0:\n            # Zero norm\n            return (\n                (x != 0)\n                .astype(x.real.dtype)\n                .sum(axis=axis, keepdims=keepdims)\n            )\n        elif ord == 1:\n            # special case for speedup\n            return add.reduce(abs(x), axis=axis, keepdims=keepdims)\n        elif ord is None or ord == 2:\n            # special case for speedup\n            s = (x.conj() * x).real\n            return sqrt(add.reduce(s, axis=axis, keepdims=keepdims))\n        # None of the str-type keywords for ord ('fro', 'nuc')\n        # are valid for vectors\n        elif isinstance(ord, str):\n            raise ValueError(f\"Invalid norm order '{ord}' for vectors\")\n        else:\n            absx = abs(x)\n            absx **= ord\n            ret = add.reduce(absx, axis=axis, keepdims=keepdims)\n            ret **= reciprocal(ord, dtype=ret.dtype)\n            return ret\n    elif len(axis) == 2:\n        row_axis, col_axis = axis\n        row_axis = normalize_axis_index(row_axis, nd)\n        col_axis = normalize_axis_index(col_axis, nd)\n        if row_axis == col_axis:\n            raise ValueError('Duplicate axes given.')\n        if ord == 2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amax)\n        elif ord == -2:\n            ret = _multi_svd_norm(x, row_axis, col_axis, amin)\n        elif ord == 1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).max(axis=col_axis)\n        elif ord == inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).max(axis=row_axis)\n        elif ord == -1:\n            if col_axis > row_axis:\n                col_axis -= 1\n            ret = add.reduce(abs(x), axis=row_axis).min(axis=col_axis)\n        elif ord == -inf:\n            if row_axis > col_axis:\n                row_axis -= 1\n            ret = add.reduce(abs(x), axis=col_axis).min(axis=row_axis)\n        elif ord in [None, 'fro', 'f']:\n            ret = sqrt(add.reduce((x.conj() * x).real, axis=axis))\n        elif ord == 'nuc':\n            ret = _multi_svd_norm(x, row_axis, col_axis, sum)\n        else:\n            raise ValueError(\"Invalid norm order for matrices.\")\n        if keepdims:\n            ret_shape = list(x.shape)\n            ret_shape[axis[0]] = 1\n            ret_shape[axis[1]] = 1\n            ret = ret.reshape(ret_shape)\n        return ret\n    else:\n        raise ValueError(\"Improper number of dimensions to norm.\")"}
{"function_name": "decimal.Decimal.adjusted", "api_doc": "adjusted()\nReturn the adjusted exponent after shifting out the coefficient\u2019s rightmost digits until only the lead digit remains: Decimal('321e+5').adjusted() returns seven. Used for determining the position of the most significant digit with respect to the decimal point.", "api_code": "def adjusted(self):\n    try:\n        return self._exp + len(self._int) - 1\n    # If NaN or Infinity, self._exp is string\n    except TypeError:\n        return 0"}
{"function_name": "html.escape", "api_doc": "html.escape(s, quote=True)\nConvert the characters &, < and > in string s to HTML-safe sequences. Use this if you need to display text that might contain such characters in HTML. If the optional flag quote is true, the characters (\") and (') are also translated; this helps for inclusion in an HTML attribute value delimited by quotes, as in <a href=\"...\">.\n\nAdded in version 3.2.", "api_code": "def escape(s, quote=True):\n    s = s.replace(\"&\", \"&amp;\") # Must be done first!\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace(\">\", \"&gt;\")\n    if quote:\n        s = s.replace('\"', \"&quot;\")\n        s = s.replace('\\'', \"&#x27;\")\n    return s"}
{"function_name": "networkx.algorithms.bipartite.basic.color", "api_doc": "color\ncolor(G)[source]\nReturns a two-coloring of the graph.\n\nRaises an exception if the graph is not bipartite.\n\nParameters\n:\nG\nNetworkX graph\nReturns\n:\ncolor\ndictionary\nA dictionary keyed by node with a 1 or 0 as data for each node color.\n\nRaises\n:\nNetworkXError\nIf the graph is not two-colorable.\n\nExamples\n\nfrom networkx.algorithms import bipartite\nG = nx.path_graph(4)\nc = bipartite.color(G)\nprint(c)\n{0: 1, 1: 0, 2: 1, 3: 0}\nYou can use this to set a node attribute indicating the bipartite set:\n\nnx.set_node_attributes(G, c, \"bipartite\")\nprint(G.nodes[0][\"bipartite\"])\n1\nprint(G.nodes[1][\"bipartite\"])\n0", "api_code": "@nx._dispatchable\ndef color(G):\n    if G.is_directed():\n        import itertools\n\n        def neighbors(v):\n            return itertools.chain.from_iterable([G.predecessors(v), G.successors(v)])\n\n    else:\n        neighbors = G.neighbors\n\n    color = {}\n    for n in G:  # handle disconnected graphs\n        if n in color or len(G[n]) == 0:  # skip isolates\n            continue\n        queue = [n]\n        color[n] = 1  # nodes seen with color (1 or 0)\n        while queue:\n            v = queue.pop()\n            c = 1 - color[v]  # opposite color of node v\n            for w in neighbors(v):\n                if w in color:\n                    if color[w] == color[v]:\n                        raise nx.NetworkXError(\"Graph is not bipartite.\")\n                else:\n                    color[w] = c\n                    queue.append(w)\n    # color isolates with 0\n    color.update(dict.fromkeys(nx.isolates(G), 0))\n    return color"}
